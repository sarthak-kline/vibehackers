{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25af167f-654b-4668-aa26-a206e5296a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8a3233c-ccc5-4844-88c5-fd56ec37a766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MappingModeName</th>\n",
       "      <th>ServiceAndProductMappingId</th>\n",
       "      <th>SourceMasterBrand</th>\n",
       "      <th>SourceBrand</th>\n",
       "      <th>SourceSubBrand</th>\n",
       "      <th>SourceCategory</th>\n",
       "      <th>SourceSubcategory</th>\n",
       "      <th>SourceDescription</th>\n",
       "      <th>SourcePackagingTypeName</th>\n",
       "      <th>SourceSize</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "      <th>Unnamed: 20</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "      <th>Unnamed: 24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Map</td>\n",
       "      <td>6242009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L'OREAL</td>\n",
       "      <td>Styling Products</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Curl Expression Mousse</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Map</td>\n",
       "      <td>6324365</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Loreal Curl Expression Mousse</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>8.2 oz</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Map</td>\n",
       "      <td>6326141</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Styling</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Curls 10 In 1 Mousse</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>8.5 Oz</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Map</td>\n",
       "      <td>6182020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L'OREAL PROFESSIONNEL</td>\n",
       "      <td>CURL EXPRESSION</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>SOIN MULTI BENEFICES CREME-MOUSSE</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Map</td>\n",
       "      <td>6493784</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Styling</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L'Oreal Curl Expression Mousse</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  MappingModeName  ServiceAndProductMappingId SourceMasterBrand  \\\n",
       "0             Map                     6242009               NaN   \n",
       "1             Map                     6324365               NaN   \n",
       "2             Map                     6326141               NaN   \n",
       "3             Map                     6182020               NaN   \n",
       "4             Map                     6493784               NaN   \n",
       "\n",
       "             SourceBrand    SourceSubBrand SourceCategory SourceSubcategory  \\\n",
       "0                L'OREAL  Styling Products    Unspecified       Unspecified   \n",
       "1                    NaN               NaN          Other               NaN   \n",
       "2                    NaN               NaN        Styling               NaN   \n",
       "3  L'OREAL PROFESSIONNEL   CURL EXPRESSION    Unspecified       Unspecified   \n",
       "4                    NaN               NaN        Styling               NaN   \n",
       "\n",
       "                   SourceDescription SourcePackagingTypeName SourceSize  ...  \\\n",
       "0             Curl Expression Mousse           Not Specified        NaN  ...   \n",
       "1      Loreal Curl Expression Mousse           Not Specified     8.2 oz  ...   \n",
       "2               Curls 10 In 1 Mousse           Not Specified     8.5 Oz  ...   \n",
       "3  SOIN MULTI BENEFICES CREME-MOUSSE           Not Specified        NaN  ...   \n",
       "4     L'Oreal Curl Expression Mousse           Not Specified        NaN  ...   \n",
       "\n",
       "  Unnamed: 15 Unnamed: 16  Unnamed: 17  Unnamed: 18  Unnamed: 19  Unnamed: 20  \\\n",
       "0         NaN         NaN          NaN          NaN          NaN          NaN   \n",
       "1         NaN         NaN          NaN          NaN          NaN          NaN   \n",
       "2         NaN         NaN          NaN          NaN          NaN          NaN   \n",
       "3         NaN         NaN          NaN          NaN          NaN          NaN   \n",
       "4         NaN         NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "   Unnamed: 21  Unnamed: 22  Unnamed: 23  Unnamed: 24  \n",
       "0          NaN          NaN          NaN          NaN  \n",
       "1          NaN          NaN          NaN          NaN  \n",
       "2          NaN          NaN          NaN          NaN  \n",
       "3          NaN          NaN          NaN          NaN  \n",
       "4          NaN          NaN          NaN          NaN  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace 'products.csv' with your actual file name\n",
    "df = pd.read_csv(\"SampleData.csv\")\n",
    "\n",
    "# Check first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da13b0f1-a50c-469c-b685-8ebf55309a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns to compare\n",
    "cols = ['SourceDescription', 'SourceMasterBrand', 'SourceBrand']\n",
    "\n",
    "# Fill missing values with empty strings to avoid errors\n",
    "df_selected = df[cols].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad29eb68-d2e9-40a4-94e1-eed9bd377207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all columns into one string per row\n",
    "combined_text = df_selected.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b12591e7-2e44-438d-9af4-65539cf78524",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(combined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5178657d-6584-4b57-a375-343d3d6f9598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps and min_samples can be tuned\n",
    "model = DBSCAN(eps=0.5, min_samples=3, metric='cosine')\n",
    "labels = model.fit_predict(X)\n",
    "\n",
    "# Add cluster labels to the original dataframe\n",
    "df['Cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7b78697-13bc-4b8b-a2c3-e3fb306ce4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters found: 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MappingModeName</th>\n",
       "      <th>ServiceAndProductMappingId</th>\n",
       "      <th>SourceMasterBrand</th>\n",
       "      <th>SourceBrand</th>\n",
       "      <th>SourceSubBrand</th>\n",
       "      <th>SourceCategory</th>\n",
       "      <th>SourceSubcategory</th>\n",
       "      <th>SourceDescription</th>\n",
       "      <th>SourcePackagingTypeName</th>\n",
       "      <th>SourceSize</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "      <th>Unnamed: 20</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "      <th>Unnamed: 24</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Map</td>\n",
       "      <td>6242009</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L'OREAL</td>\n",
       "      <td>Styling Products</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Curl Expression Mousse</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Map</td>\n",
       "      <td>6324365</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Loreal Curl Expression Mousse</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>8.2 oz</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Map</td>\n",
       "      <td>6326141</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Styling</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Curls 10 In 1 Mousse</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>8.5 Oz</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Map</td>\n",
       "      <td>6182020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L'OREAL PROFESSIONNEL</td>\n",
       "      <td>CURL EXPRESSION</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>SOIN MULTI BENEFICES CREME-MOUSSE</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Map</td>\n",
       "      <td>7581323</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L'Oréal Professionnel revente</td>\n",
       "      <td>Curl expression</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Curl Expression Crème-en-Mousse 10-en-1 250 ml</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Map</td>\n",
       "      <td>6182015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>L'Oreal</td>\n",
       "      <td>CURL EXPRESSION</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>CREME EN MOUSSE</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Map</td>\n",
       "      <td>1431510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Conditioner</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stimulate me rinse</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Map</td>\n",
       "      <td>9258707</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a7.Products Hair</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KM 44 stimulate me wash 8.4 oz</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>8.4 oz</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Map</td>\n",
       "      <td>1700616</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wash</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kevin Murphy Stimulate Me Wash</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>8.4oz.</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Map</td>\n",
       "      <td>5064219</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shampoo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>km stimulte wash</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>Map</td>\n",
       "      <td>2355153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Other</td>\n",
       "      <td>NaN</td>\n",
       "      <td>StimulateMe.Rinse</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>8.4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>Map</td>\n",
       "      <td>4173799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shampoo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>StimulateMe.Wash</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>Map</td>\n",
       "      <td>859734</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hair Care</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KMStimulate-Me.Wash</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>Map</td>\n",
       "      <td>6362111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hair Care</td>\n",
       "      <td>Conditioners</td>\n",
       "      <td>KMStimulate-Me.Rinse</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Map</td>\n",
       "      <td>8961516</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shampoo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kev Stimulate Wash</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>Map</td>\n",
       "      <td>6763343</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shampoo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STIMULATE-ME.WASH (MINI)</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>40 Ml</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>Map</td>\n",
       "      <td>1204342</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Unassigned</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Stimulate Me Wash Mini</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>Map</td>\n",
       "      <td>4930382</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shampoo</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KEV STIMULATE.ME WASH</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>250.00 ml</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>Map</td>\n",
       "      <td>6204637</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hair Care</td>\n",
       "      <td>Shampoo</td>\n",
       "      <td>KMStimulate-Me.Wash</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>753</th>\n",
       "      <td>Map</td>\n",
       "      <td>5205084</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KEVINMURPHY</td>\n",
       "      <td>SHAMPOOS</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>KM StimulateMe Wash 250ml</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>Map</td>\n",
       "      <td>4931790</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Conditioner</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KEV STIMULATE ME RINSE</td>\n",
       "      <td>Not Specified</td>\n",
       "      <td>250.00 ml</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    MappingModeName  ServiceAndProductMappingId SourceMasterBrand  \\\n",
       "0               Map                     6242009               NaN   \n",
       "1               Map                     6324365               NaN   \n",
       "2               Map                     6326141               NaN   \n",
       "3               Map                     6182020               NaN   \n",
       "26              Map                     7581323               NaN   \n",
       "28              Map                     6182015               NaN   \n",
       "41              Map                     1431510               NaN   \n",
       "42              Map                     9258707               NaN   \n",
       "43              Map                     1700616               NaN   \n",
       "130             Map                     5064219               NaN   \n",
       "149             Map                     2355153               NaN   \n",
       "159             Map                     4173799               NaN   \n",
       "161             Map                      859734               NaN   \n",
       "162             Map                     6362111               NaN   \n",
       "199             Map                     8961516               NaN   \n",
       "256             Map                     6763343               NaN   \n",
       "280             Map                     1204342               NaN   \n",
       "282             Map                     4930382               NaN   \n",
       "620             Map                     6204637               NaN   \n",
       "753             Map                     5205084               NaN   \n",
       "766             Map                     4931790               NaN   \n",
       "\n",
       "                       SourceBrand    SourceSubBrand    SourceCategory  \\\n",
       "0                          L'OREAL  Styling Products       Unspecified   \n",
       "1                              NaN               NaN             Other   \n",
       "2                              NaN               NaN           Styling   \n",
       "3            L'OREAL PROFESSIONNEL   CURL EXPRESSION       Unspecified   \n",
       "26   L'Oréal Professionnel revente   Curl expression       Unspecified   \n",
       "28                         L'Oreal   CURL EXPRESSION       Unspecified   \n",
       "41                             NaN               NaN       Conditioner   \n",
       "42                             NaN               NaN  a7.Products Hair   \n",
       "43                             NaN               NaN              Wash   \n",
       "130                            NaN               NaN           Shampoo   \n",
       "149                            NaN               NaN             Other   \n",
       "159                            NaN               NaN           Shampoo   \n",
       "161                            NaN               NaN         Hair Care   \n",
       "162                            NaN               NaN         Hair Care   \n",
       "199                            NaN               NaN           Shampoo   \n",
       "256                            NaN               NaN           Shampoo   \n",
       "280                            NaN               NaN        Unassigned   \n",
       "282                            NaN               NaN           Shampoo   \n",
       "620                            NaN               NaN         Hair Care   \n",
       "753                    KEVINMURPHY          SHAMPOOS       Unspecified   \n",
       "766                            NaN               NaN       Conditioner   \n",
       "\n",
       "    SourceSubcategory                               SourceDescription  \\\n",
       "0         Unspecified                          Curl Expression Mousse   \n",
       "1                 NaN                   Loreal Curl Expression Mousse   \n",
       "2                 NaN                            Curls 10 In 1 Mousse   \n",
       "3         Unspecified               SOIN MULTI BENEFICES CREME-MOUSSE   \n",
       "26        Unspecified  Curl Expression Crème-en-Mousse 10-en-1 250 ml   \n",
       "28        Unspecified                                 CREME EN MOUSSE   \n",
       "41                NaN                              Stimulate me rinse   \n",
       "42                NaN                  KM 44 stimulate me wash 8.4 oz   \n",
       "43                NaN                  Kevin Murphy Stimulate Me Wash   \n",
       "130               NaN                                km stimulte wash   \n",
       "149               NaN                               StimulateMe.Rinse   \n",
       "159               NaN                                StimulateMe.Wash   \n",
       "161               NaN                             KMStimulate-Me.Wash   \n",
       "162      Conditioners                            KMStimulate-Me.Rinse   \n",
       "199               NaN                              Kev Stimulate Wash   \n",
       "256               NaN                        STIMULATE-ME.WASH (MINI)   \n",
       "280               NaN                          Stimulate Me Wash Mini   \n",
       "282               NaN                           KEV STIMULATE.ME WASH   \n",
       "620           Shampoo                             KMStimulate-Me.Wash   \n",
       "753       Unspecified                       KM StimulateMe Wash 250ml   \n",
       "766               NaN                          KEV STIMULATE ME RINSE   \n",
       "\n",
       "    SourcePackagingTypeName SourceSize  ... Unnamed: 16 Unnamed: 17  \\\n",
       "0             Not Specified        NaN  ...         NaN         NaN   \n",
       "1             Not Specified     8.2 oz  ...         NaN         NaN   \n",
       "2             Not Specified     8.5 Oz  ...         NaN         NaN   \n",
       "3             Not Specified        NaN  ...         NaN         NaN   \n",
       "26            Not Specified        NaN  ...         NaN         NaN   \n",
       "28            Not Specified        NaN  ...         NaN         NaN   \n",
       "41            Not Specified        NaN  ...         NaN         NaN   \n",
       "42            Not Specified     8.4 oz  ...         NaN         NaN   \n",
       "43            Not Specified     8.4oz.  ...         NaN         NaN   \n",
       "130           Not Specified        NaN  ...         NaN         NaN   \n",
       "149           Not Specified        8.4  ...         NaN         NaN   \n",
       "159           Not Specified        NaN  ...         NaN         NaN   \n",
       "161           Not Specified        NaN  ...         NaN         NaN   \n",
       "162           Not Specified        NaN  ...         NaN         NaN   \n",
       "199           Not Specified        NaN  ...         NaN         NaN   \n",
       "256           Not Specified      40 Ml  ...         NaN         NaN   \n",
       "280           Not Specified        NaN  ...         NaN         NaN   \n",
       "282           Not Specified  250.00 ml  ...         NaN         NaN   \n",
       "620           Not Specified        NaN  ...         NaN         NaN   \n",
       "753           Not Specified        NaN  ...         NaN         NaN   \n",
       "766           Not Specified  250.00 ml  ...         NaN         NaN   \n",
       "\n",
       "     Unnamed: 18  Unnamed: 19  Unnamed: 20  Unnamed: 21  Unnamed: 22  \\\n",
       "0            NaN          NaN          NaN          NaN          NaN   \n",
       "1            NaN          NaN          NaN          NaN          NaN   \n",
       "2            NaN          NaN          NaN          NaN          NaN   \n",
       "3            NaN          NaN          NaN          NaN          NaN   \n",
       "26           NaN          NaN          NaN          NaN          NaN   \n",
       "28           NaN          NaN          NaN          NaN          NaN   \n",
       "41           NaN          NaN          NaN          NaN          NaN   \n",
       "42           NaN          NaN          NaN          NaN          NaN   \n",
       "43           NaN          NaN          NaN          NaN          NaN   \n",
       "130          NaN          NaN          NaN          NaN          NaN   \n",
       "149          NaN          NaN          NaN          NaN          NaN   \n",
       "159          NaN          NaN          NaN          NaN          NaN   \n",
       "161          NaN          NaN          NaN          NaN          NaN   \n",
       "162          NaN          NaN          NaN          NaN          NaN   \n",
       "199          NaN          NaN          NaN          NaN          NaN   \n",
       "256          NaN          NaN          NaN          NaN          NaN   \n",
       "280          NaN          NaN          NaN          NaN          NaN   \n",
       "282          NaN          NaN          NaN          NaN          NaN   \n",
       "620          NaN          NaN          NaN          NaN          NaN   \n",
       "753          NaN          NaN          NaN          NaN          NaN   \n",
       "766          NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "     Unnamed: 23  Unnamed: 24  Cluster  \n",
       "0            NaN          NaN        0  \n",
       "1            NaN          NaN        0  \n",
       "2            NaN          NaN        0  \n",
       "3            NaN          NaN       -1  \n",
       "26           NaN          NaN       -1  \n",
       "28           NaN          NaN       -1  \n",
       "41           NaN          NaN        1  \n",
       "42           NaN          NaN        1  \n",
       "43           NaN          NaN        1  \n",
       "130          NaN          NaN        5  \n",
       "149          NaN          NaN        2  \n",
       "159          NaN          NaN        2  \n",
       "161          NaN          NaN        3  \n",
       "162          NaN          NaN        3  \n",
       "199          NaN          NaN        4  \n",
       "256          NaN          NaN        5  \n",
       "280          NaN          NaN        5  \n",
       "282          NaN          NaN        4  \n",
       "620          NaN          NaN        3  \n",
       "753          NaN          NaN        2  \n",
       "766          NaN          NaN        4  \n",
       "\n",
       "[21 rows x 26 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View how many clusters were formed (excluding noise)\n",
    "import numpy as np\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(f\"Number of clusters found: {n_clusters}\")\n",
    "\n",
    "# Show some examples from each cluster\n",
    "df.groupby('Cluster').head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecfbb3d1-ba1e-4aa0-a5c7-ed774acfa720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering complete. Output saved to 'clustered_products.csv'\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"clustered_productsnew.csv\", index=False)\n",
    "print(\"Clustering complete. Output saved to 'clustered_products.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22c9cb8b-372c-4747-b79c-d19aec318e9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'products.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 📦 STEP 1: Install Required Libraries (Run this in a separate cell if needed)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# !pip install pandas sentence-transformers hdbscan scikit-learn\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 📥 STEP 2: Load and Prepare Data\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproducts.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Replace with your actual filename\u001b[39;00m\n\u001b[0;32m      8\u001b[0m cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSourceDescription\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSourceMasterBrand\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSourceBrand\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# excluding barcode for better clustering\u001b[39;00m\n\u001b[0;32m      9\u001b[0m df \u001b[38;5;241m=\u001b[39m df[cols]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'products.csv'"
     ]
    }
   ],
   "source": [
    "# 📦 STEP 1: Install Required Libraries (Run this in a separate cell if needed)\n",
    "# !pip install pandas sentence-transformers hdbscan scikit-learn\n",
    "\n",
    "# 📥 STEP 2: Load and Prepare Data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"products.csv\")  # Replace with your actual filename\n",
    "cols = ['SourceDescription', 'SourceMasterBrand', 'SourceBrand']  # excluding barcode for better clustering\n",
    "df = df[cols].fillna(\"\")\n",
    "\n",
    "# Combine selected columns into one text column\n",
    "df['text'] = df.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "# 🧼 STEP 3: Clean the Text\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # normalize spaces\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# 🤖 STEP 4: Generate Sentence Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Small & fast, 384-dim\n",
    "embeddings = model.encode(df['text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# 🧠 STEP 5: Run HDBSCAN Clustering\n",
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=20, min_samples=5, metric='euclidean')\n",
    "labels = clusterer.fit_predict(embeddings)\n",
    "\n",
    "df['Cluster'] = labels\n",
    "\n",
    "# 🧪 STEP 6: Evaluate Clustering Quality\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Filter out noise (-1 labels) for scoring\n",
    "if len(set(labels)) > 1 and (labels != -1).sum() > 1:\n",
    "    score = silhouette_score(embeddings[labels != -1], labels[labels != -1])\n",
    "    print(f\"Silhouette Score (excluding noise): {score:.4f}\")\n",
    "else:\n",
    "    print(\"Not enough clusters to compute silhouette score.\")\n",
    "\n",
    "# 📤 STEP 7: Export Clustered Data\n",
    "df.to_csv(\"clustered_products_new.csv\", index=False)\n",
    "print(\"✅ Clustering complete. Results saved to 'clustered_products.csv'.\")\n",
    "\n",
    "# 👀 STEP 8: Preview a Few Rows from Each Cluster\n",
    "df.groupby('Cluster').head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92728133-3157-4504-83ea-c8e2563fe0db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean_text)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# 🤖 STEP 4: Generate Sentence Embeddings\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m     28\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Small & fast, 384-dim\u001b[39;00m\n\u001b[0;32m     29\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "# 📦 STEP 1: Install Required Libraries (Run this in a separate cell if needed)\n",
    "# !pip install pandas sentence-transformers hdbscan scikit-learn\n",
    "\n",
    "# 📥 STEP 2: Load and Prepare Data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"SampleData.csv\")  # Replace with your actual filename\n",
    "cols = ['SourceDescription', 'SourceMasterBrand', 'SourceBrand']  # excluding barcode for better clustering\n",
    "df = df[cols].fillna(\"\")\n",
    "\n",
    "# Combine selected columns into one text column\n",
    "df['text'] = df.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "# 🧼 STEP 3: Clean the Text\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # normalize spaces\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# 🤖 STEP 4: Generate Sentence Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Small & fast, 384-dim\n",
    "embeddings = model.encode(df['text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# 🧠 STEP 5: Run HDBSCAN Clustering\n",
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=20, min_samples=5, metric='euclidean')\n",
    "labels = clusterer.fit_predict(embeddings)\n",
    "\n",
    "df['Cluster'] = labels\n",
    "\n",
    "# 🧪 STEP 6: Evaluate Clustering Quality\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Filter out noise (-1 labels) for scoring\n",
    "if len(set(labels)) > 1 and (labels != -1).sum() > 1:\n",
    "    score = silhouette_score(embeddings[labels != -1], labels[labels != -1])\n",
    "    print(f\"Silhouette Score (excluding noise): {score:.4f}\")\n",
    "else:\n",
    "    print(\"Not enough clusters to compute silhouette score.\")\n",
    "\n",
    "# 📤 STEP 7: Export Clustered Data\n",
    "df.to_csv(\"clustered_products.csv\", index=False)\n",
    "print(\"✅ Clustering complete. Results saved to 'clustered_products.csv'.\")\n",
    "\n",
    "# 👀 STEP 8: Preview a Few Rows from Each Cluster\n",
    "df.groupby('Cluster').head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "764d18ed-d13c-4918-a663-716ce50ea5f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1250447445.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    pip install pandas sentence-transformers hdbscan scikit-learn\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# 📦 STEP 1: Install Required Libraries (Run this in a separate cell if needed)\n",
    "pip install pandas sentence-transformers hdbscan scikit-learn\n",
    "\n",
    "# 📥 STEP 2: Load and Prepare Data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"products.csv\")  # Replace with your actual filename\n",
    "cols = ['SourceDescription', 'SourceMasterBrand', 'SourceBrand']  # excluding barcode for better clustering\n",
    "df = df[cols].fillna(\"\")\n",
    "\n",
    "# Combine selected columns into one text column\n",
    "df['text'] = df.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "# 🧼 STEP 3: Clean the Text\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # normalize spaces\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# 🤖 STEP 4: Generate Sentence Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Small & fast, 384-dim\n",
    "embeddings = model.encode(df['text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# 🧠 STEP 5: Run HDBSCAN Clustering\n",
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=20, min_samples=5, metric='euclidean')\n",
    "labels = clusterer.fit_predict(embeddings)\n",
    "\n",
    "df['Cluster'] = labels\n",
    "\n",
    "# 🧪 STEP 6: Evaluate Clustering Quality\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Filter out noise (-1 labels) for scoring\n",
    "if len(set(labels)) > 1 and (labels != -1).sum() > 1:\n",
    "    score = silhouette_score(embeddings[labels != -1], labels[labels != -1])\n",
    "    print(f\"Silhouette Score (excluding noise): {score:.4f}\")\n",
    "else:\n",
    "    print(\"Not enough clusters to compute silhouette score.\")\n",
    "\n",
    "# 📤 STEP 7: Export Clustered Data\n",
    "df.to_csv(\"clustered_products_new.csv\", index=False)\n",
    "print(\"✅ Clustering complete. Results saved to 'clustered_products.csv'.\")\n",
    "\n",
    "# 👀 STEP 8: Preview a Few Rows from Each Cluster\n",
    "df.groupby('Cluster').head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7c88c5-8c9e-4516-b9bd-7b7c7926468e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7194d961-ae49-416d-a39c-bf9e69ca1b12",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'products.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 📥 STEP 2: Load and Prepare Data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproducts.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Replace with your actual filename\u001b[39;00m\n\u001b[0;32m      5\u001b[0m cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSourceDescription\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSourceMasterBrand\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSourceBrand\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# excluding barcode for better clustering\u001b[39;00m\n\u001b[0;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m df[cols]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'products.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "# 📥 STEP 2: Load and Prepare Data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"products.csv\")  # Replace with your actual filename\n",
    "cols = ['SourceDescription', 'SourceMasterBrand', 'SourceBrand']  # excluding barcode for better clustering\n",
    "df = df[cols].fillna(\"\")\n",
    "\n",
    "# Combine selected columns into one text column\n",
    "df['text'] = df.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "# 🧼 STEP 3: Clean the Text\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # normalize spaces\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# 🤖 STEP 4: Generate Sentence Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Small & fast, 384-dim\n",
    "embeddings = model.encode(df['text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# 🧠 STEP 5: Run HDBSCAN Clustering\n",
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=20, min_samples=5, metric='euclidean')\n",
    "labels = clusterer.fit_predict(embeddings)\n",
    "\n",
    "df['Cluster'] = labels\n",
    "\n",
    "# 🧪 STEP 6: Evaluate Clustering Quality\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Filter out noise (-1 labels) for scoring\n",
    "if len(set(labels)) > 1 and (labels != -1).sum() > 1:\n",
    "    score = silhouette_score(embeddings[labels != -1], labels[labels != -1])\n",
    "    print(f\"Silhouette Score (excluding noise): {score:.4f}\")\n",
    "else:\n",
    "    print(\"Not enough clusters to compute silhouette score.\")\n",
    "\n",
    "# 📤 STEP 7: Export Clustered Data\n",
    "df.to_csv(\"clustered_products_new.csv\", index=False)\n",
    "print(\"✅ Clustering complete. Results saved to 'clustered_products.csv'.\")\n",
    "\n",
    "# 👀 STEP 8: Preview a Few Rows from Each Cluster\n",
    "df.groupby('Cluster').head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ff29e00-16fa-4978-9c81-6d389ec76d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting hdbscan\n",
      "  Downloading hdbscan-0.8.40.tar.gz (6.9 MB)\n",
      "     ---------------------------------------- 0.0/6.9 MB ? eta -:--:--\n",
      "     ---------- ----------------------------- 1.8/6.9 MB 9.9 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 3.9/6.9 MB 9.7 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 6.0/6.9 MB 9.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 6.9/6.9 MB 8.9 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.54.1-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Using cached huggingface_hub-0.34.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Using cached tokenizers-0.21.4-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from hdbscan) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sarthak.prakash\\appdata\\local\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.4.26)\n",
      "Using cached sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
      "Using cached transformers-4.54.1-py3-none-any.whl (11.2 MB)\n",
      "Using cached huggingface_hub-0.34.3-py3-none-any.whl (558 kB)\n",
      "Using cached tokenizers-0.21.4-cp39-abi3-win_amd64.whl (2.5 MB)\n",
      "Building wheels for collected packages: hdbscan\n",
      "  Building wheel for hdbscan (pyproject.toml): started\n",
      "  Building wheel for hdbscan (pyproject.toml): finished with status 'error'\n",
      "Failed to build hdbscan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for hdbscan (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [39 lines of output]\n",
      "  C:\\Users\\Sarthak.Prakash\\AppData\\Local\\Temp\\pip-build-env-3um73yq1\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py:289: UserWarning: Unknown distribution option: 'test_suite'\n",
      "    warnings.warn(msg)\n",
      "  C:\\Users\\Sarthak.Prakash\\AppData\\Local\\Temp\\pip-build-env-3um73yq1\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py:289: UserWarning: Unknown distribution option: 'tests_require'\n",
      "    warnings.warn(msg)\n",
      "  C:\\Users\\Sarthak.Prakash\\AppData\\Local\\Temp\\pip-build-env-3um73yq1\\overlay\\Lib\\site-packages\\setuptools\\dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \n",
      "          License :: OSI Approved\n",
      "  \n",
      "          See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    self._finalize_license_expression()\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\\lib.win-amd64-cpython-313\\hdbscan\n",
      "  copying hdbscan\\branches.py -> build\\lib.win-amd64-cpython-313\\hdbscan\n",
      "  copying hdbscan\\flat.py -> build\\lib.win-amd64-cpython-313\\hdbscan\n",
      "  copying hdbscan\\hdbscan_.py -> build\\lib.win-amd64-cpython-313\\hdbscan\n",
      "  copying hdbscan\\plots.py -> build\\lib.win-amd64-cpython-313\\hdbscan\n",
      "  copying hdbscan\\prediction.py -> build\\lib.win-amd64-cpython-313\\hdbscan\n",
      "  copying hdbscan\\robust_single_linkage_.py -> build\\lib.win-amd64-cpython-313\\hdbscan\n",
      "  copying hdbscan\\validity.py -> build\\lib.win-amd64-cpython-313\\hdbscan\n",
      "  copying hdbscan\\__init__.py -> build\\lib.win-amd64-cpython-313\\hdbscan\n",
      "  creating build\\lib.win-amd64-cpython-313\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_branches.py -> build\\lib.win-amd64-cpython-313\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_flat.py -> build\\lib.win-amd64-cpython-313\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_hdbscan.py -> build\\lib.win-amd64-cpython-313\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_prediction_utils.py -> build\\lib.win-amd64-cpython-313\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\test_rsl.py -> build\\lib.win-amd64-cpython-313\\hdbscan\\tests\n",
      "  copying hdbscan\\tests\\__init__.py -> build\\lib.win-amd64-cpython-313\\hdbscan\\tests\n",
      "  running build_ext\n",
      "  building 'hdbscan._hdbscan_tree' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for hdbscan\n",
      "ERROR: Failed to build installable wheels for some pyproject.toml based projects (hdbscan)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean_text)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 🤖 STEP 4: Generate Sentence Embeddings\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m     30\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Small & fast, 384-dim\u001b[39;00m\n\u001b[0;32m     31\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "# 📦 STEP 1: Install Required Libraries (Run this in a separate cell if needed)\n",
    "!pip install pandas sentence-transformers hdbscan scikit-learn\n",
    "\n",
    "\n",
    "\n",
    "# 📥 STEP 2: Load and Prepare Data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"SampleData.csv\")  # Replace with your actual filename\n",
    "cols = ['SourceDescription', 'SourceMasterBrand', 'SourceBrand']  # excluding barcode for better clustering\n",
    "df = df[cols].fillna(\"\")\n",
    "\n",
    "# Combine selected columns into one text column\n",
    "df['text'] = df.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "# 🧼 STEP 3: Clean the Text\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # normalize spaces\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# 🤖 STEP 4: Generate Sentence Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Small & fast, 384-dim\n",
    "embeddings = model.encode(df['text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# 🧠 STEP 5: Run HDBSCAN Clustering\n",
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=20, min_samples=5, metric='euclidean')\n",
    "labels = clusterer.fit_predict(embeddings)\n",
    "\n",
    "df['Cluster'] = labels\n",
    "\n",
    "# 🧪 STEP 6: Evaluate Clustering Quality\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Filter out noise (-1 labels) for scoring\n",
    "if len(set(labels)) > 1 and (labels != -1).sum() > 1:\n",
    "    score = silhouette_score(embeddings[labels != -1], labels[labels != -1])\n",
    "    print(f\"Silhouette Score (excluding noise): {score:.4f}\")\n",
    "else:\n",
    "    print(\"Not enough clusters to compute silhouette score.\")\n",
    "\n",
    "# 📤 STEP 7: Export Clustered Data\n",
    "df.to_csv(\"clustered_products_new.csv\", index=False)\n",
    "print(\"✅ Clustering complete. Results saved to 'clustered_products.csv'.\")\n",
    "\n",
    "# 👀 STEP 8: Preview a Few Rows from Each Cluster\n",
    "df.groupby('Cluster').head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4002c434-7370-478c-88d5-cb656d587b93",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean_text)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# 🤖 STEP 4: Generate Sentence Embeddings\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m     25\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Small & fast, 384-dim\u001b[39;00m\n\u001b[0;32m     26\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "# 📥 STEP 2: Load and Prepare Data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"SampleData.csv\")  # Replace with your actual filename\n",
    "cols = ['SourceDescription', 'SourceMasterBrand', 'SourceBrand']  # excluding barcode for better clustering\n",
    "df = df[cols].fillna(\"\")\n",
    "\n",
    "# Combine selected columns into one text column\n",
    "df['text'] = df.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "# 🧼 STEP 3: Clean the Text\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)  # remove punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # normalize spaces\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# 🤖 STEP 4: Generate Sentence Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Small & fast, 384-dim\n",
    "embeddings = model.encode(df['text'].tolist(), show_progress_bar=True)\n",
    "\n",
    "# 🧠 STEP 5: Run HDBSCAN Clustering\n",
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=20, min_samples=5, metric='euclidean')\n",
    "labels = clusterer.fit_predict(embeddings)\n",
    "\n",
    "df['Cluster'] = labels\n",
    "\n",
    "# 🧪 STEP 6: Evaluate Clustering Quality\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Filter out noise (-1 labels) for scoring\n",
    "if len(set(labels)) > 1 and (labels != -1).sum() > 1:\n",
    "    score = silhouette_score(embeddings[labels != -1], labels[labels != -1])\n",
    "    print(f\"Silhouette Score (excluding noise): {score:.4f}\")\n",
    "else:\n",
    "    print(\"Not enough clusters to compute silhouette score.\")\n",
    "\n",
    "# 📤 STEP 7: Export Clustered Data\n",
    "df.to_csv(\"clustered_products_new.csv\", index=False)\n",
    "print(\"✅ Clustering complete. Results saved to 'clustered_products.csv'.\")\n",
    "\n",
    "# 👀 STEP 8: Preview a Few Rows from Each Cluster\n",
    "df.groupby('Cluster').head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0498ae1c-b488-43f1-9ef5-62cdff0230bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer, util\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load your data\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "data = [\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Loreal Curl Expression Mousse\",\n",
    "    \"Curls 10 In 1 Mousse\",\n",
    "    \"SOIN MULTI BENEFICES CREME-MOUSSE\",\n",
    "    \"L'Oreal Curl Expression Mousse\",\n",
    "    \"Curl Expression 10-In-1 Cream-In-Mousse\",\n",
    "    \"Curl Expression - Mousse\",\n",
    "    \"Curl epxpression 10 in 1 cream in mousse\",\n",
    "    \"Curl Expression 10-IN-1\",\n",
    "    \"Curl Expression 10 in 1 Mousse\",\n",
    "    \"10 -in- 1 Professional Cream in Mouse\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"oreal curl expression\",\n",
    "    \"Curl Express Mousse\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Curl Expression 10-in-1 Cream-in-Mousse\",\n",
    "    \"Curl Expression Cream Mousse\",\n",
    "    \"L'Oreal Curl Expression Mousse\",\n",
    "    \"Curl Exp 10-1 mousse\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Curl Express Mousse\",\n",
    "    \"Curl Expression Cream In Mouse\",\n",
    "    \"L'Oreal Curl Expression - Curl Mousse\",\n",
    "    \"Curl Expression Crème-en-Mousse 10-en-1 250 ml\",\n",
    "    \"SERIE - Curl Expression - Mousse\",\n",
    "    \"CREME EN MOUSSE\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Curl Expression 10-in-1 Cream-Mousse\",\n",
    "    \"CURL EXPRESS MOUSSE\",\n",
    "    \"Curl Expression Mousse 10-in-1\",\n",
    "    \"Curl Expression 10 - 1 Mousse 8.2oz\",\n",
    "    \"LP Curls 10 in 1 Mousse\",\n",
    "    \"curl expression mousse\",\n",
    "    \"loreal curl expressions mousse\",\n",
    "    \"Curl Expressions 10-in-1 Cream Mousse\",\n",
    "    \"Curl Expressions 10-IN-1 Cream-In-Mousse\",\n",
    "    \"Loreal Creme Mouse\",\n",
    "    \"Mousse 10 en 1 soin multi-bénéfiste\"\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=['SourceDescription'])\n",
    "\n",
    "# Canonical master product\n",
    "master_product = \"10 IN 1 CREAM IN MOUSSE\"\n",
    "\n",
    "# Load SBERT model (small and fast variant)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode all descriptions and the master product\n",
    "embeddings = model.encode(df['SourceDescription'].tolist(), convert_to_tensor=True)\n",
    "master_embedding = model.encode(master_product, convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarities = util.cos_sim(master_embedding, embeddings)[0]\n",
    "\n",
    "# Add similarity scores to DataFrame\n",
    "df['SimilarityScore'] = similarities.cpu().numpy()\n",
    "\n",
    "# Set a threshold to confirm match (optional)\n",
    "threshold = 0.6\n",
    "df['MatchedToMaster'] = df['SimilarityScore'] >= threshold\n",
    "df['MappedProduct'] = df['MatchedToMaster'].apply(lambda x: master_product if x else \"Check manually\")\n",
    "\n",
    "# View result\n",
    "import caas_jupyter_tools as cjtools\n",
    "cjtools.display_dataframe_to_user(name=\"Matched Products\", dataframe=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7ae7816-6bbd-4741-bbf9-bcc2359b6c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71c8f691-816f-4e16-82c1-1e8eec6c79d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ddddca77e294bc6ad56d98fa8023abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Sarthak.Prakash\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12ef8a5d4ed7471f8ee8c1d920cdd5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67294dfcec1b4556abf238b9232d08dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05d36066e61e4e1488f04ef7cea0f28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423d43097dc445c2b6c8f7832bc9fe88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91388c18e77b4ea2b4881544fbab0735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2fa3c569c94ca0b1ee223174fd4e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8933eb40de0644c0bfd1af772f780bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e3cabdf3df4d958e7e719388a800ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134a257fd5584aa6b30374a97681027f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c19d40632975418eacf10abf488abe94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'caas_jupyter_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 72\u001b[0m\n\u001b[0;32m     69\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMappedProduct\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatchedToMaster\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: master_product \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheck manually\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# View result\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcaas_jupyter_tools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcjtools\u001b[39;00m\n\u001b[0;32m     73\u001b[0m cjtools\u001b[38;5;241m.\u001b[39mdisplay_dataframe_to_user(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMatched Products\u001b[39m\u001b[38;5;124m\"\u001b[39m, dataframe\u001b[38;5;241m=\u001b[39mdf)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'caas_jupyter_tools'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "data = [\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Loreal Curl Expression Mousse\",\n",
    "    \"Curls 10 In 1 Mousse\",\n",
    "    \"SOIN MULTI BENEFICES CREME-MOUSSE\",\n",
    "    \"L'Oreal Curl Expression Mousse\",\n",
    "    \"Curl Expression 10-In-1 Cream-In-Mousse\",\n",
    "    \"Curl Expression - Mousse\",\n",
    "    \"Curl epxpression 10 in 1 cream in mousse\",\n",
    "    \"Curl Expression 10-IN-1\",\n",
    "    \"Curl Expression 10 in 1 Mousse\",\n",
    "    \"10 -in- 1 Professional Cream in Mouse\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"oreal curl expression\",\n",
    "    \"Curl Express Mousse\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Curl Expression 10-in-1 Cream-in-Mousse\",\n",
    "    \"Curl Expression Cream Mousse\",\n",
    "    \"L'Oreal Curl Expression Mousse\",\n",
    "    \"Curl Exp 10-1 mousse\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Curl Express Mousse\",\n",
    "    \"Curl Expression Cream In Mouse\",\n",
    "    \"L'Oreal Curl Expression - Curl Mousse\",\n",
    "    \"Curl Expression Crème-en-Mousse 10-en-1 250 ml\",\n",
    "    \"SERIE - Curl Expression - Mousse\",\n",
    "    \"CREME EN MOUSSE\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Curl Expression 10-in-1 Cream-Mousse\",\n",
    "    \"CURL EXPRESS MOUSSE\",\n",
    "    \"Curl Expression Mousse 10-in-1\",\n",
    "    \"Curl Expression 10 - 1 Mousse 8.2oz\",\n",
    "    \"LP Curls 10 in 1 Mousse\",\n",
    "    \"curl expression mousse\",\n",
    "    \"loreal curl expressions mousse\",\n",
    "    \"Curl Expressions 10-in-1 Cream Mousse\",\n",
    "    \"Curl Expressions 10-IN-1 Cream-In-Mousse\",\n",
    "    \"Loreal Creme Mouse\",\n",
    "    \"Mousse 10 en 1 soin multi-bénéfiste\"\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=['SourceDescription'])\n",
    "\n",
    "# Canonical master product\n",
    "master_product = \"10 IN 1 CREAM IN MOUSSE\"\n",
    "\n",
    "# Load SBERT model (small and fast variant)\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode all descriptions and the master product\n",
    "embeddings = model.encode(df['SourceDescription'].tolist(), convert_to_tensor=True)\n",
    "master_embedding = model.encode(master_product, convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarities = util.cos_sim(master_embedding, embeddings)[0]\n",
    "\n",
    "# Add similarity scores to DataFrame\n",
    "df['SimilarityScore'] = similarities.cpu().numpy()\n",
    "\n",
    "# Set a threshold to confirm match (optional)\n",
    "threshold = 0.6\n",
    "df['MatchedToMaster'] = df['SimilarityScore'] >= threshold\n",
    "df['MappedProduct'] = df['MatchedToMaster'].apply(lambda x: master_product if x else \"Check manually\")\n",
    "\n",
    "# View result\n",
    "import caas_jupyter_tools as cjtools\n",
    "cjtools.display_dataframe_to_user(name=\"Matched Products\", dataframe=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5a96bb5-d0e0-4eb1-93b9-44b5d08727b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 SourceDescription  SimilarityScore  \\\n",
      "0                           Curl Expression Mousse         0.819024   \n",
      "1                    Loreal Curl Expression Mousse         0.792863   \n",
      "2                             Curls 10 In 1 Mousse         0.722310   \n",
      "3                SOIN MULTI BENEFICES CREME-MOUSSE         0.457562   \n",
      "4                   L'Oreal Curl Expression Mousse         0.769618   \n",
      "5          Curl Expression 10-In-1 Cream-In-Mousse         1.000000   \n",
      "6                         Curl Expression - Mousse         0.810031   \n",
      "7         Curl epxpression 10 in 1 cream in mousse         0.844786   \n",
      "8                          Curl Expression 10-IN-1         0.718177   \n",
      "9                   Curl Expression 10 in 1 Mousse         0.850729   \n",
      "10           10 -in- 1 Professional Cream in Mouse         0.345365   \n",
      "11                          Curl Expression Mousse         0.819024   \n",
      "12                           oreal curl expression         0.635643   \n",
      "13                             Curl Express Mousse         0.756142   \n",
      "14                          Curl Expression Mousse         0.819024   \n",
      "15                          Curl Expression Mousse         0.819024   \n",
      "16         Curl Expression 10-in-1 Cream-in-Mousse         1.000000   \n",
      "17                    Curl Expression Cream Mousse         0.903365   \n",
      "18                  L'Oreal Curl Expression Mousse         0.769618   \n",
      "19                            Curl Exp 10-1 mousse         0.790622   \n",
      "20                          Curl Expression Mousse         0.819024   \n",
      "21                          Curl Expression Mousse         0.819024   \n",
      "22                             Curl Express Mousse         0.756142   \n",
      "23                  Curl Expression Cream In Mouse         0.565389   \n",
      "24           L'Oreal Curl Expression - Curl Mousse         0.771177   \n",
      "25  Curl Expression Crème-en-Mousse 10-en-1 250 ml         0.827270   \n",
      "26                SERIE - Curl Expression - Mousse         0.745539   \n",
      "27                                 CREME EN MOUSSE         0.498395   \n",
      "28                          Curl Expression Mousse         0.819024   \n",
      "29            Curl Expression 10-in-1 Cream-Mousse         0.993220   \n",
      "30                             CURL EXPRESS MOUSSE         0.756142   \n",
      "31                  Curl Expression Mousse 10-in-1         0.869359   \n",
      "32             Curl Expression 10 - 1 Mousse 8.2oz         0.820101   \n",
      "33                         LP Curls 10 in 1 Mousse         0.640638   \n",
      "34                          curl expression mousse         0.819024   \n",
      "35                  loreal curl expressions mousse         0.778460   \n",
      "36           Curl Expressions 10-in-1 Cream Mousse         0.974696   \n",
      "37        Curl Expressions 10-IN-1 Cream-In-Mousse         0.983201   \n",
      "38                              Loreal Creme Mouse         0.170433   \n",
      "39             Mousse 10 en 1 soin multi-bénéfiste         0.370772   \n",
      "\n",
      "                              MappedProduct  \n",
      "0   Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "1   Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "2   Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "3                            Check manually  \n",
      "4   Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "5   Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "6   Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "7   Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "8   Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "9   Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "10                           Check manually  \n",
      "11  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "12  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "13  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "14  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "15  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "16  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "17  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "18  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "19  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "20  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "21  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "22  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "23                           Check manually  \n",
      "24  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "25  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "26  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "27                           Check manually  \n",
      "28  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "29  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "30  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "31  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "32  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "33  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "34  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "35  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "36  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "37  Curl Expression 10-in-1 Cream-in-Mousse  \n",
      "38                           Check manually  \n",
      "39                           Check manually  \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "data = [\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Loreal Curl Expression Mousse\",\n",
    "    \"Curls 10 In 1 Mousse\",\n",
    "    \"SOIN MULTI BENEFICES CREME-MOUSSE\",\n",
    "    \"L'Oreal Curl Expression Mousse\",\n",
    "    \"Curl Expression 10-In-1 Cream-In-Mousse\",\n",
    "    \"Curl Expression - Mousse\",\n",
    "    \"Curl epxpression 10 in 1 cream in mousse\",\n",
    "    \"Curl Expression 10-IN-1\",\n",
    "    \"Curl Expression 10 in 1 Mousse\",\n",
    "    \"10 -in- 1 Professional Cream in Mouse\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"oreal curl expression\",\n",
    "    \"Curl Express Mousse\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Curl Expression 10-in-1 Cream-in-Mousse\",\n",
    "    \"Curl Expression Cream Mousse\",\n",
    "    \"L'Oreal Curl Expression Mousse\",\n",
    "    \"Curl Exp 10-1 mousse\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Curl Express Mousse\",\n",
    "    \"Curl Expression Cream In Mouse\",\n",
    "    \"L'Oreal Curl Expression - Curl Mousse\",\n",
    "    \"Curl Expression Crème-en-Mousse 10-en-1 250 ml\",\n",
    "    \"SERIE - Curl Expression - Mousse\",\n",
    "    \"CREME EN MOUSSE\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Curl Expression 10-in-1 Cream-Mousse\",\n",
    "    \"CURL EXPRESS MOUSSE\",\n",
    "    \"Curl Expression Mousse 10-in-1\",\n",
    "    \"Curl Expression 10 - 1 Mousse 8.2oz\",\n",
    "    \"LP Curls 10 in 1 Mousse\",\n",
    "    \"curl expression mousse\",\n",
    "    \"loreal curl expressions mousse\",\n",
    "    \"Curl Expressions 10-in-1 Cream Mousse\",\n",
    "    \"Curl Expressions 10-IN-1 Cream-In-Mousse\",\n",
    "    \"Loreal Creme Mouse\",\n",
    "    \"Mousse 10 en 1 soin multi-bénéfiste\"\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=['SourceDescription'])\n",
    "\n",
    "# Canonical master product\n",
    "master_product = \"Curl Expression 10-in-1 Cream-in-Mousse\"\n",
    "\n",
    "# Load SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode\n",
    "embeddings = model.encode(df['SourceDescription'].tolist(), convert_to_tensor=True)\n",
    "master_embedding = model.encode(master_product, convert_to_tensor=True)\n",
    "\n",
    "# Similarity\n",
    "similarities = util.cos_sim(master_embedding, embeddings)[0]\n",
    "df['SimilarityScore'] = similarities.cpu().numpy()\n",
    "\n",
    "# Mapping logic\n",
    "threshold = 0.6\n",
    "df['MatchedToMaster'] = df['SimilarityScore'] >= threshold\n",
    "df['MappedProduct'] = df['MatchedToMaster'].apply(lambda x: master_product if x else \"Check manually\")\n",
    "\n",
    "# Show result\n",
    "print(df[['SourceDescription', 'SimilarityScore', 'MappedProduct']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6314f0e8-6ad2-495e-b620-46a0acf1245a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 SourceDescription  SimilarityScore  \\\n",
      "0                           Curl Expression Mousse         0.292035   \n",
      "1                    Loreal Curl Expression Mousse         0.310874   \n",
      "2                             Curls 10 In 1 Mousse         0.625827   \n",
      "3                SOIN MULTI BENEFICES CREME-MOUSSE         0.542094   \n",
      "4                   L'Oreal Curl Expression Mousse         0.342276   \n",
      "5          Curl Expression 10-In-1 Cream-In-Mousse         0.622831   \n",
      "6                         Curl Expression - Mousse         0.280588   \n",
      "7         Curl epxpression 10 in 1 cream in mousse         0.686028   \n",
      "8                          Curl Expression 10-IN-1         0.217655   \n",
      "9                   Curl Expression 10 in 1 Mousse         0.445685   \n",
      "10           10 -in- 1 Professional Cream in Mouse         0.576613   \n",
      "11                          Curl Expression Mousse         0.292035   \n",
      "12                           oreal curl expression         0.127491   \n",
      "13                             Curl Express Mousse         0.346293   \n",
      "14                          Curl Expression Mousse         0.292035   \n",
      "15                          Curl Expression Mousse         0.292035   \n",
      "16         Curl Expression 10-in-1 Cream-in-Mousse         0.622831   \n",
      "17                    Curl Expression Cream Mousse         0.539479   \n",
      "18                  L'Oreal Curl Expression Mousse         0.342276   \n",
      "19                            Curl Exp 10-1 mousse         0.441094   \n",
      "20                          Curl Expression Mousse         0.292035   \n",
      "21                          Curl Expression Mousse         0.292035   \n",
      "22                             Curl Express Mousse         0.346293   \n",
      "23                  Curl Expression Cream In Mouse         0.210160   \n",
      "24           L'Oreal Curl Expression - Curl Mousse         0.292983   \n",
      "25  Curl Expression Crème-en-Mousse 10-en-1 250 ml         0.500047   \n",
      "26                SERIE - Curl Expression - Mousse         0.300096   \n",
      "27                                 CREME EN MOUSSE         0.579401   \n",
      "28                          Curl Expression Mousse         0.292035   \n",
      "29            Curl Expression 10-in-1 Cream-Mousse         0.611203   \n",
      "30                             CURL EXPRESS MOUSSE         0.346293   \n",
      "31                  Curl Expression Mousse 10-in-1         0.412130   \n",
      "32             Curl Expression 10 - 1 Mousse 8.2oz         0.485684   \n",
      "33                         LP Curls 10 in 1 Mousse         0.513755   \n",
      "34                          curl expression mousse         0.292035   \n",
      "35                  loreal curl expressions mousse         0.297446   \n",
      "36           Curl Expressions 10-in-1 Cream Mousse         0.619260   \n",
      "37        Curl Expressions 10-IN-1 Cream-In-Mousse         0.616465   \n",
      "38                              Loreal Creme Mouse         0.217467   \n",
      "39             Mousse 10 en 1 soin multi-bénéfiste         0.455709   \n",
      "\n",
      "              MappedProduct  \n",
      "0            Check manually  \n",
      "1            Check manually  \n",
      "2   10 IN 1 CREAM IN MOUSSE  \n",
      "3            Check manually  \n",
      "4            Check manually  \n",
      "5   10 IN 1 CREAM IN MOUSSE  \n",
      "6            Check manually  \n",
      "7   10 IN 1 CREAM IN MOUSSE  \n",
      "8            Check manually  \n",
      "9            Check manually  \n",
      "10           Check manually  \n",
      "11           Check manually  \n",
      "12           Check manually  \n",
      "13           Check manually  \n",
      "14           Check manually  \n",
      "15           Check manually  \n",
      "16  10 IN 1 CREAM IN MOUSSE  \n",
      "17           Check manually  \n",
      "18           Check manually  \n",
      "19           Check manually  \n",
      "20           Check manually  \n",
      "21           Check manually  \n",
      "22           Check manually  \n",
      "23           Check manually  \n",
      "24           Check manually  \n",
      "25           Check manually  \n",
      "26           Check manually  \n",
      "27           Check manually  \n",
      "28           Check manually  \n",
      "29  10 IN 1 CREAM IN MOUSSE  \n",
      "30           Check manually  \n",
      "31           Check manually  \n",
      "32           Check manually  \n",
      "33           Check manually  \n",
      "34           Check manually  \n",
      "35           Check manually  \n",
      "36  10 IN 1 CREAM IN MOUSSE  \n",
      "37  10 IN 1 CREAM IN MOUSSE  \n",
      "38           Check manually  \n",
      "39           Check manually  \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "data = [\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Loreal Curl Expression Mousse\",\n",
    "    \"Curls 10 In 1 Mousse\",\n",
    "    \"SOIN MULTI BENEFICES CREME-MOUSSE\",\n",
    "    \"L'Oreal Curl Expression Mousse\",\n",
    "    \"Curl Expression 10-In-1 Cream-In-Mousse\",\n",
    "    \"Curl Expression - Mousse\",\n",
    "    \"Curl epxpression 10 in 1 cream in mousse\",\n",
    "    \"Curl Expression 10-IN-1\",\n",
    "    \"Curl Expression 10 in 1 Mousse\",\n",
    "    \"10 -in- 1 Professional Cream in Mouse\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"oreal curl expression\",\n",
    "    \"Curl Express Mousse\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Curl Expression 10-in-1 Cream-in-Mousse\",\n",
    "    \"Curl Expression Cream Mousse\",\n",
    "    \"L'Oreal Curl Expression Mousse\",\n",
    "    \"Curl Exp 10-1 mousse\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Curl Express Mousse\",\n",
    "    \"Curl Expression Cream In Mouse\",\n",
    "    \"L'Oreal Curl Expression - Curl Mousse\",\n",
    "    \"Curl Expression Crème-en-Mousse 10-en-1 250 ml\",\n",
    "    \"SERIE - Curl Expression - Mousse\",\n",
    "    \"CREME EN MOUSSE\",\n",
    "    \"Curl Expression Mousse\",\n",
    "    \"Curl Expression 10-in-1 Cream-Mousse\",\n",
    "    \"CURL EXPRESS MOUSSE\",\n",
    "    \"Curl Expression Mousse 10-in-1\",\n",
    "    \"Curl Expression 10 - 1 Mousse 8.2oz\",\n",
    "    \"LP Curls 10 in 1 Mousse\",\n",
    "    \"curl expression mousse\",\n",
    "    \"loreal curl expressions mousse\",\n",
    "    \"Curl Expressions 10-in-1 Cream Mousse\",\n",
    "    \"Curl Expressions 10-IN-1 Cream-In-Mousse\",\n",
    "    \"Loreal Creme Mouse\",\n",
    "    \"Mousse 10 en 1 soin multi-bénéfiste\"\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=['SourceDescription'])\n",
    "\n",
    "# Canonical master product\n",
    "master_product = \"10 IN 1 CREAM IN MOUSSE\"\n",
    "\n",
    "# Load SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode\n",
    "embeddings = model.encode(df['SourceDescription'].tolist(), convert_to_tensor=True)\n",
    "master_embedding = model.encode(master_product, convert_to_tensor=True)\n",
    "\n",
    "# Similarity\n",
    "similarities = util.cos_sim(master_embedding, embeddings)[0]\n",
    "df['SimilarityScore'] = similarities.cpu().numpy()\n",
    "\n",
    "# Mapping logic\n",
    "threshold = 0.6\n",
    "df['MatchedToMaster'] = df['SimilarityScore'] >= threshold\n",
    "df['MappedProduct'] = df['MatchedToMaster'].apply(lambda x: master_product if x else \"Check manually\")\n",
    "\n",
    "# Show result\n",
    "print(df[['SourceDescription', 'SimilarityScore', 'MappedProduct']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3bcbce6-a42c-43f6-bbdf-aa41a7e8f255",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          SourceDescription     BestMatchProduct  SimilarityScore Confidence\n",
      "554         Zero Yellow Kit      ZERO YELLOW KIT         1.000000       High\n",
      "428     RICH REPAIR SHAMPOO  RICH REPAIR SHAMPOO         1.000000       High\n",
      "81        STIMULATE ME WASH    STIMULATE ME WASH         1.000000       High\n",
      "83        stimulate me wash    STIMULATE ME WASH         1.000000       High\n",
      "468     Rich repair shampoo  RICH REPAIR SHAMPOO         1.000000       High\n",
      "..                      ...                  ...              ...        ...\n",
      "475              Final Sale    HAIR DRESSERS SET         0.214029        Low\n",
      "440     Wishes Do Come True   STIMULATE ME RINSE         0.203026        Low\n",
      "550  Bb Wishes do come true    REVOLVE WOMEN DUO         0.186687        Low\n",
      "254  Stim rins9339341003991   STIMULATE ME RINSE         0.168589        Low\n",
      "482                  Bumble    STIMULATE ME WASH         0.142000        Low\n",
      "\n",
      "[568 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV file (update filename as needed)\n",
    "df = pd.read_csv('SampleFullData.csv')  # Make sure it has 'SourceDescription' and 'ProductName' columns\n",
    "\n",
    "# Drop NA and get unique lists\n",
    "source_descriptions = df['SourceDescription'].dropna().unique().tolist()\n",
    "master_products = df['ProductName'].dropna().unique().tolist()\n",
    "\n",
    "# Load SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode source descriptions and master product names\n",
    "source_embeddings = model.encode(source_descriptions, convert_to_tensor=True)\n",
    "master_embeddings = model.encode(master_products, convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarities between each source and each master\n",
    "cosine_scores = util.cos_sim(source_embeddings, master_embeddings)\n",
    "\n",
    "# For each source description, find best matching master\n",
    "best_match_idx = cosine_scores.argmax(dim=1).cpu().numpy()\n",
    "best_scores = cosine_scores.max(dim=1).values.cpu().numpy()\n",
    "\n",
    "# Build result dataframe\n",
    "matched_df = pd.DataFrame({\n",
    "    'SourceDescription': source_descriptions,\n",
    "    'BestMatchProduct': [master_products[i] for i in best_match_idx],\n",
    "    'SimilarityScore': best_scores\n",
    "})\n",
    "\n",
    "# Optional: Filter low-confidence matches\n",
    "threshold = 0.6\n",
    "matched_df['Confidence'] = matched_df['SimilarityScore'].apply(lambda x: 'High' if x >= threshold else 'Low')\n",
    "\n",
    "# Show results\n",
    "print(matched_df.sort_values('SimilarityScore', ascending=False))\n",
    "matched_df.to_csv('outputofmodel.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c1d018e-a48e-4d6c-b2e4-613c106de792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\Temp\\ipykernel_3288\\2082870063.py:5: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('SampleFullData.csv')  # Must contain 'SourceDescription' and 'ProductName'\n",
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matching complete. Output saved to 'outputofmodel.csv'\n",
      "                   SourceDescription              ProductName  \\\n",
      "0             Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "1      Loreal Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "2               Curls 10 In 1 Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "3  SOIN MULTI BENEFICES CREME-MOUSSE  10 IN 1 CREAM IN MOUSSE   \n",
      "4     L'Oreal Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "\n",
      "            BestMatchProduct  SimilarityScore Confidence  \n",
      "0       CURL DEFINING MOUSSE         0.913944       High  \n",
      "1       CURL DEFINING MOUSSE         0.809130       High  \n",
      "2  FRESH CURLS SPRING MOUSSE         0.802155       High  \n",
      "3      MULTICROISSANCE CREME         0.684465       High  \n",
      "4                CURL MOUSSE         0.805791       High  \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv('SampleFullData.csv')  # Must contain 'SourceDescription' and 'ProductName'\n",
    "\n",
    "# Drop rows with missing data\n",
    "df = df.dropna(subset=['SourceDescription', 'ProductName'])\n",
    "\n",
    "# Get master product list (unique ProductName values)\n",
    "master_products = df['ProductName'].dropna().unique().tolist()\n",
    "\n",
    "# Load SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode master products\n",
    "master_embeddings = model.encode(master_products, convert_to_tensor=True)\n",
    "\n",
    "# Encode source descriptions (row-wise)\n",
    "source_embeddings = model.encode(df['SourceDescription'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarity for each row with all master products\n",
    "cosine_scores = util.cos_sim(source_embeddings, master_embeddings)\n",
    "\n",
    "# Get best match index and score for each source description\n",
    "best_match_idx = cosine_scores.argmax(dim=1).cpu().numpy()\n",
    "best_scores = cosine_scores.max(dim=1).values.cpu().numpy()\n",
    "\n",
    "# Assign results back to dataframe\n",
    "df['BestMatchProduct'] = [master_products[i] for i in best_match_idx]\n",
    "df['SimilarityScore'] = best_scores\n",
    "\n",
    "# Confidence label\n",
    "threshold = 0.6\n",
    "df['Confidence'] = df['SimilarityScore'].apply(lambda x: 'High' if x >= threshold else 'Low')\n",
    "\n",
    "# Save output\n",
    "df.to_csv('outputofmodel.csv', index=False)\n",
    "\n",
    "# Preview\n",
    "print(\"✅ Matching complete. Output saved to 'outputofmodel.csv'\")\n",
    "print(df[['SourceDescription', 'ProductName', 'BestMatchProduct', 'SimilarityScore', 'Confidence']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31576e10-9c33-4b3f-8ca3-02f4fbf6a97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\Temp\\ipykernel_3288\\3031593436.py:5: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('SampleFullData.csv')\n",
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matching complete. Output saved to 'outputofmodel1.csv'\n",
      "                   SourceDescription              ProductName  \\\n",
      "0             Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "1      Loreal Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "2               Curls 10 In 1 Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "3  SOIN MULTI BENEFICES CREME-MOUSSE  10 IN 1 CREAM IN MOUSSE   \n",
      "4     L'Oreal Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "\n",
      "                BestMatchProduct  SimilarityScore Confidence  \n",
      "0                    CURL MOUSSE         0.857571       High  \n",
      "1                    CURL MOUSSE         0.828772       High  \n",
      "2      CURLS FIRM STYLING MOUSSE         0.821535       High  \n",
      "3        10 IN 1 CREAM IN MOUSSE         0.766348       High  \n",
      "4  NATURAL CURL ENHANCING MOUSSE         0.820982       High  \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# Drop rows with missing SourceDescription or ProductName\n",
    "df = df.dropna(subset=['SourceDescription', 'ProductName'])\n",
    "\n",
    "# Replace NaNs in helper columns with empty strings\n",
    "for col in ['SourceBrand', 'SourceSubBrand', 'SourceMasterBrand']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# Create enriched text for raw data\n",
    "df['CombinedSourceText'] = (\n",
    "    'Description: ' + df['SourceDescription'].astype(str) + ' | ' +\n",
    "    'Brand: ' + df['SourceBrand'] + ' | ' +\n",
    "    'SubBrand: ' + df['SourceSubBrand'] + ' | ' +\n",
    "    'MasterBrand: ' + df['SourceMasterBrand']\n",
    ")\n",
    "\n",
    "# Create master list with additional brand info\n",
    "master_df = df[['ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates().fillna('')\n",
    "\n",
    "# Create enriched master product text\n",
    "master_df['CombinedMasterText'] = (\n",
    "    'Description: ' + master_df['ProductName'].astype(str) + ' | ' +\n",
    "    'Brand: ' + master_df['BrandName'] + ' | ' +\n",
    "    'SubBrand: ' + master_df['SubBrandName'] + ' | ' +\n",
    "    'MasterBrand: ' + master_df['MasterBrandName']\n",
    ")\n",
    "\n",
    "# Encode with SBERT\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "source_embeddings = model.encode(df['CombinedSourceText'].tolist(), convert_to_tensor=True)\n",
    "master_embeddings = model.encode(master_df['CombinedMasterText'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarities\n",
    "cosine_scores = util.cos_sim(source_embeddings, master_embeddings)\n",
    "\n",
    "# Get best matches\n",
    "best_match_idx = cosine_scores.argmax(dim=1).cpu().numpy()\n",
    "best_scores = cosine_scores.max(dim=1).values.cpu().numpy()\n",
    "\n",
    "# Add matched info to df\n",
    "df['BestMatchProduct'] = master_df.iloc[best_match_idx]['ProductName'].values\n",
    "df['MatchedBrand'] = master_df.iloc[best_match_idx]['BrandName'].values\n",
    "df['MatchedSubBrand'] = master_df.iloc[best_match_idx]['SubBrandName'].values\n",
    "df['MatchedMasterBrand'] = master_df.iloc[best_match_idx]['MasterBrandName'].values\n",
    "df['SimilarityScore'] = best_scores\n",
    "\n",
    "# Confidence tagging\n",
    "threshold = 0.6\n",
    "df['Confidence'] = df['SimilarityScore'].apply(lambda x: 'High' if x >= threshold else 'Low')\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('outputofmodel1.csv', index=False)\n",
    "\n",
    "# Preview\n",
    "print(\"✅ Matching complete. Output saved to 'outputofmodel1.csv'\")\n",
    "print(df[['SourceDescription', 'ProductName', 'BestMatchProduct', 'SimilarityScore', 'Confidence']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "285f4fd6-dac4-41b0-903e-aba86fd72f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\Temp\\ipykernel_3288\\1683330020.py:6: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('SampleFullData.csv')\n",
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 66\u001b[0m\n\u001b[0;32m     59\u001b[0m filtered_master_df \u001b[38;5;241m=\u001b[39m master_df[\n\u001b[0;32m     60\u001b[0m     (master_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBrandName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m source_brand) \u001b[38;5;241m|\u001b[39m\n\u001b[0;32m     61\u001b[0m     (master_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMasterBrandName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m source_masterbrand)\n\u001b[0;32m     62\u001b[0m ]\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered_master_df\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# Encode only filtered master entries\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     filtered_embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(filtered_master_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombinedMasterText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     67\u001b[0m     source_embedding \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(source_text, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     68\u001b[0m     similarities \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mcos_sim(source_embedding, filtered_embeddings)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1052\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m-> 1052\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1054\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1133\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[1;34m(self, input, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m             module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[0;32m   1128\u001b[0m         module_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1129\u001b[0m             key: value\n\u001b[0;32m   1130\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1131\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mforward_kwargs)\n\u001b[0;32m   1132\u001b[0m         }\n\u001b[1;32m-> 1133\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs)\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:437\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features, **kwargs)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[0;32m    431\u001b[0m trans_features \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    432\u001b[0m     key: value\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    435\u001b[0m }\n\u001b[1;32m--> 437\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    438\u001b[0m token_embeddings \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    439\u001b[0m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token_embeddings\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1028\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1028\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1029\u001b[0m     embedding_output,\n\u001b[0;32m   1030\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   1031\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1032\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1033\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1034\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1035\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1036\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1037\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1038\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1039\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m   1040\u001b[0m )\n\u001b[0;32m   1041\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1042\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:675\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    671\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m    673\u001b[0m layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    676\u001b[0m     hidden_states,\n\u001b[0;32m    677\u001b[0m     attention_mask,\n\u001b[0;32m    678\u001b[0m     layer_head_mask,\n\u001b[0;32m    679\u001b[0m     encoder_hidden_states,  \u001b[38;5;66;03m# as a positional argument for gradient checkpointing\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    681\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    682\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    683\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    684\u001b[0m )\n\u001b[0;32m    686\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:614\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    611\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    612\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add cross attentions if we output attention weights\u001b[39;00m\n\u001b[1;32m--> 614\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_chunk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim, attention_output\n\u001b[0;32m    616\u001b[0m )\n\u001b[0;32m    617\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[0;32m    619\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\pytorch_utils.py:251\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m forward_fn(\u001b[38;5;241m*\u001b[39minput_tensors)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:622\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m--> 622\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m    623\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[0;32m    624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:539\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 539\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m    540\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# Drop incomplete rows\n",
    "df = df.dropna(subset=['SourceDescription', 'ProductName'])\n",
    "\n",
    "# Fill missing helper columns\n",
    "for col in ['SourceBrand', 'SourceSubBrand', 'SourceMasterBrand']:\n",
    "    df[col] = df[col].fillna('')\n",
    "for col in ['BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# Create combined source text\n",
    "df['CombinedSourceText'] = (\n",
    "    'Description: ' + df['SourceDescription'].astype(str) + ' | ' +\n",
    "    'Brand: ' + df['SourceBrand'] + ' | ' +\n",
    "    'SubBrand: ' + df['SourceSubBrand'] + ' | ' +\n",
    "    'MasterBrand: ' + df['SourceMasterBrand']\n",
    ")\n",
    "\n",
    "# Get master product list with unique entries\n",
    "master_df = df[['ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates()\n",
    "\n",
    "# Create combined master text\n",
    "master_df['CombinedMasterText'] = (\n",
    "    'Description: ' + master_df['ProductName'].astype(str) + ' | ' +\n",
    "    'Brand: ' + master_df['BrandName'] + ' | ' +\n",
    "    'SubBrand: ' + master_df['SubBrandName'] + ' | ' +\n",
    "    'MasterBrand: ' + master_df['MasterBrandName']\n",
    ")\n",
    "\n",
    "# Load SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode master product texts\n",
    "master_embeddings_all = model.encode(master_df['CombinedMasterText'].tolist(), convert_to_tensor=True)\n",
    "master_texts_all = master_df['CombinedMasterText'].tolist()\n",
    "master_products_all = master_df['ProductName'].tolist()\n",
    "\n",
    "# Store final matched values\n",
    "best_matches = []\n",
    "match_types = []\n",
    "similarity_scores = []\n",
    "matched_brands = []\n",
    "matched_subbrands = []\n",
    "matched_masterbrands = []\n",
    "\n",
    "# Main loop: match each row individually\n",
    "for idx, row in df.iterrows():\n",
    "    source_text = row['CombinedSourceText']\n",
    "    source_brand = row['SourceBrand'].strip().lower()\n",
    "    source_masterbrand = row['SourceMasterBrand'].strip().lower()\n",
    "\n",
    "    # Try to filter master products by brand or masterbrand\n",
    "    filtered_master_df = master_df[\n",
    "        (master_df['BrandName'].str.strip().str.lower() == source_brand) |\n",
    "        (master_df['MasterBrandName'].str.strip().str.lower() == source_masterbrand)\n",
    "    ]\n",
    "\n",
    "    if not filtered_master_df.empty:\n",
    "        # Encode only filtered master entries\n",
    "        filtered_embeddings = model.encode(filtered_master_df['CombinedMasterText'].tolist(), convert_to_tensor=True)\n",
    "        source_embedding = model.encode(source_text, convert_to_tensor=True)\n",
    "        similarities = util.cos_sim(source_embedding, filtered_embeddings)[0]\n",
    "        best_idx = int(similarities.argmax())\n",
    "        score = float(similarities[best_idx])\n",
    "\n",
    "        best_matches.append(filtered_master_df.iloc[best_idx]['ProductName'])\n",
    "        matched_brands.append(filtered_master_df.iloc[best_idx]['BrandName'])\n",
    "        matched_subbrands.append(filtered_master_df.iloc[best_idx]['SubBrandName'])\n",
    "        matched_masterbrands.append(filtered_master_df.iloc[best_idx]['MasterBrandName'])\n",
    "        match_types.append(\"BrandFiltered\")\n",
    "        similarity_scores.append(score)\n",
    "    else:\n",
    "        # Fallback to full master list\n",
    "        source_embedding = model.encode(source_text, convert_to_tensor=True)\n",
    "        similarities = util.cos_sim(source_embedding, master_embeddings_all)[0]\n",
    "        best_idx = int(similarities.argmax())\n",
    "        score = float(similarities[best_idx])\n",
    "\n",
    "        best_matches.append(master_products_all[best_idx])\n",
    "        matched_brands.append(master_df.iloc[best_idx]['BrandName'])\n",
    "        matched_subbrands.append(master_df.iloc[best_idx]['SubBrandName'])\n",
    "        matched_masterbrands.append(master_df.iloc[best_idx]['MasterBrandName'])\n",
    "        match_types.append(\"FallbackGlobal\")\n",
    "        similarity_scores.append(score)\n",
    "\n",
    "# Add results to original DataFrame\n",
    "df['BestMatchProduct'] = best_matches\n",
    "df['MatchedBrand'] = matched_brands\n",
    "df['MatchedSubBrand'] = matched_subbrands\n",
    "df['MatchedMasterBrand'] = matched_masterbrands\n",
    "df['MatchType'] = match_types\n",
    "df['SimilarityScore'] = similarity_scores\n",
    "df['Confidence'] = df['SimilarityScore'].apply(lambda x: 'High' if x >= 0.6 else 'Low')\n",
    "df['IsMismatch'] = df['ProductName'] != df['BestMatchProduct']\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('outputofmodel3.csv', index=False)\n",
    "\n",
    "# Summary\n",
    "print(\"✅ Smart matching complete. Output saved to 'outputofmodel3.csv'\")\n",
    "print(df[['SourceDescription', 'ProductName', 'BestMatchProduct', 'MatchType', 'SimilarityScore', 'Confidence', 'IsMismatch']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "146c2a16-c227-4021-acad-42efb13b96f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\Temp\\ipykernel_3288\\1255716761.py:6: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('SampleFullData.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding master texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1ba847cd024f3980c03a77df130791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding source texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbab652b3ee4425b87571e395f03d812",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity matrix...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "DataFrame index must be unique for orient='index'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIsMismatch\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProductName\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBestMatchProduct\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Optional: Join additional info from master_df\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m master_info \u001b[38;5;241m=\u001b[39m master_df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProductName\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto_dict(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     58\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatchedBrand\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [master_info\u001b[38;5;241m.\u001b[39mget(p, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBrandName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBestMatchProduct\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     59\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatchedSubBrand\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [master_info\u001b[38;5;241m.\u001b[39mget(p, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubBrandName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBestMatchProduct\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:2178\u001b[0m, in \u001b[0;36mDataFrame.to_dict\u001b[1;34m(self, orient, into, index)\u001b[0m\n\u001b[0;32m   2075\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2076\u001b[0m \u001b[38;5;124;03mConvert the DataFrame to a dictionary.\u001b[39;00m\n\u001b[0;32m   2077\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2174\u001b[0m \u001b[38;5;124;03m defaultdict(<class 'list'>, {'col1': 2, 'col2': 0.75})]\u001b[39;00m\n\u001b[0;32m   2175\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2176\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmethods\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mto_dict\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_dict\n\u001b[1;32m-> 2178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_dict(\u001b[38;5;28mself\u001b[39m, orient, into\u001b[38;5;241m=\u001b[39minto, index\u001b[38;5;241m=\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\methods\\to_dict.py:242\u001b[0m, in \u001b[0;36mto_dict\u001b[1;34m(df, orient, into, index)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[1;32m--> 242\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame index must be unique for orient=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    243\u001b[0m     columns \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m are_all_object_dtype_cols:\n",
      "\u001b[1;31mValueError\u001b[0m: DataFrame index must be unique for orient='index'."
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# Fill NA to empty string for concatenation\n",
    "for col in ['SourceDescription', 'SourceBrand', 'SourceSubBrand', 'SourceMasterBrand',\n",
    "            'ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# Create combined source and master texts\n",
    "df['CombinedSourceText'] = (\n",
    "    'Description: ' + df['SourceDescription'] + ' | Brand: ' + df['SourceBrand'] +\n",
    "    ' | SubBrand: ' + df['SourceSubBrand'] + ' | MasterBrand: ' + df['SourceMasterBrand']\n",
    ")\n",
    "\n",
    "df['CombinedMasterText'] = (\n",
    "    'Description: ' + df['ProductName'] + ' | Brand: ' + df['BrandName'] +\n",
    "    ' | SubBrand: ' + df['SubBrandName'] + ' | MasterBrand: ' + df['MasterBrandName']\n",
    ")\n",
    "\n",
    "# Get unique master products\n",
    "master_df = df[['ProductName', 'CombinedMasterText', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates()\n",
    "master_texts = master_df['CombinedMasterText'].tolist()\n",
    "master_products = master_df['ProductName'].tolist()\n",
    "\n",
    "# Get all source texts\n",
    "source_texts = df['CombinedSourceText'].tolist()\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Batch encode\n",
    "print(\"Encoding master texts...\")\n",
    "master_embeddings = model.encode(master_texts, batch_size=2048, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "print(\"Encoding source texts...\")\n",
    "source_embeddings = model.encode(source_texts, batch_size=2048, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# Compute cosine similarity matrix (100k x N)\n",
    "print(\"Computing cosine similarity matrix...\")\n",
    "cosine_scores = util.cos_sim(source_embeddings, master_embeddings)  # Shape: [num_sources, num_masters]\n",
    "\n",
    "# Get best match indices and scores\n",
    "best_match_idx = cosine_scores.argmax(dim=1).cpu().numpy()\n",
    "best_scores = cosine_scores.max(dim=1).values.cpu().numpy()\n",
    "\n",
    "# Get matched master info\n",
    "df['BestMatchProduct'] = [master_products[i] for i in best_match_idx]\n",
    "df['SimilarityScore'] = best_scores\n",
    "df['Confidence'] = df['SimilarityScore'].apply(lambda x: 'High' if x >= 0.6 else 'Low')\n",
    "df['IsMismatch'] = df['ProductName'] != df['BestMatchProduct']\n",
    "\n",
    "# Optional: Join additional info from master_df\n",
    "master_info = master_df.set_index('ProductName').to_dict(orient='index')\n",
    "df['MatchedBrand'] = [master_info.get(p, {}).get('BrandName', '') for p in df['BestMatchProduct']]\n",
    "df['MatchedSubBrand'] = [master_info.get(p, {}).get('SubBrandName', '') for p in df['BestMatchProduct']]\n",
    "df['MatchedMasterBrand'] = [master_info.get(p, {}).get('MasterBrandName', '') for p in df['BestMatchProduct']]\n",
    "\n",
    "# Save output\n",
    "df.to_csv('outputofmodel_fast.csv', index=False)\n",
    "print(\"✅ Done. Saved to 'outputofmodel_fast.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "957903be-c81d-448f-80bd-5c94f661c27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\Temp\\ipykernel_3288\\1796916511.py:7: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('SampleFullData.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding master texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "841e58d9e8ab47d7b3b8bbcb2ab00d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding source texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f033bf9e14e45039357c5cff0db30a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity matrix in batches...\n",
      "✅ Done. Saved to 'outputofmodel_fast.csv'\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# Fill NA to empty string\n",
    "for col in ['SourceDescription', 'SourceBrand', 'SourceSubBrand', 'SourceMasterBrand',\n",
    "            'ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# Create combined source and master texts\n",
    "df['CombinedSourceText'] = (\n",
    "    'Description: ' + df['SourceDescription'] + ' | Brand: ' + df['SourceBrand'] +\n",
    "    ' | SubBrand: ' + df['SourceSubBrand'] + ' | MasterBrand: ' + df['SourceMasterBrand']\n",
    ")\n",
    "\n",
    "df['CombinedMasterText'] = (\n",
    "    'Description: ' + df['ProductName'] + ' | Brand: ' + df['BrandName'] +\n",
    "    ' | SubBrand: ' + df['SubBrandName'] + ' | MasterBrand: ' + df['MasterBrandName']\n",
    ")\n",
    "\n",
    "# Get unique master products\n",
    "master_df = df[['ProductName', 'CombinedMasterText', 'BrandName', 'SubBrandName', 'MasterBrandName']] \\\n",
    "    .drop_duplicates(subset='ProductName')\n",
    "\n",
    "master_texts = master_df['CombinedMasterText'].tolist()\n",
    "master_products = master_df['ProductName'].tolist()\n",
    "\n",
    "# Get source texts\n",
    "source_texts = df['CombinedSourceText'].tolist()\n",
    "\n",
    "# Load sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode master texts\n",
    "print(\"Encoding master texts...\")\n",
    "master_embeddings = model.encode(master_texts, batch_size=512, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# Encode source texts\n",
    "print(\"Encoding source texts...\")\n",
    "source_embeddings = model.encode(source_texts, batch_size=512, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# Compute cosine similarity in batches\n",
    "print(\"Computing cosine similarity matrix in batches...\")\n",
    "batch_size = 512\n",
    "all_best_idx = []\n",
    "all_best_scores = []\n",
    "\n",
    "for i in range(0, len(source_embeddings), batch_size):\n",
    "    batch = source_embeddings[i:i + batch_size]\n",
    "    cosine_scores = util.cos_sim(batch, master_embeddings)  # [batch_size, num_masters]\n",
    "    best_idx = cosine_scores.argmax(dim=1).cpu().numpy()\n",
    "    best_scores = cosine_scores.max(dim=1).values.cpu().numpy()\n",
    "\n",
    "    all_best_idx.extend(best_idx)\n",
    "    all_best_scores.extend(best_scores)\n",
    "\n",
    "# Assign match results\n",
    "df['BestMatchProduct'] = [master_products[i] for i in all_best_idx]\n",
    "df['SimilarityScore'] = all_best_scores\n",
    "df['Confidence'] = df['SimilarityScore'].apply(lambda x: 'High' if x >= 0.6 else 'Low')\n",
    "df['IsMismatch'] = df['ProductName'] != df['BestMatchProduct']\n",
    "\n",
    "# Prepare a clean master info dictionary (safe indexing)\n",
    "master_info = master_df.drop_duplicates(subset='ProductName') \\\n",
    "    .set_index('ProductName')[['BrandName', 'SubBrandName', 'MasterBrandName']] \\\n",
    "    .to_dict(orient='index')\n",
    "\n",
    "# Match additional attributes\n",
    "df['MatchedBrand'] = df['BestMatchProduct'].apply(lambda p: master_info.get(p, {}).get('BrandName', ''))\n",
    "df['MatchedSubBrand'] = df['BestMatchProduct'].apply(lambda p: master_info.get(p, {}).get('SubBrandName', ''))\n",
    "df['MatchedMasterBrand'] = df['BestMatchProduct'].apply(lambda p: master_info.get(p, {}).get('MasterBrandName', ''))\n",
    "\n",
    "# Save output\n",
    "df.to_csv('outputofmodel_fast.csv', index=False)\n",
    "print(\"✅ Done. Saved to 'outputofmodel_fast.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15ad3262-ea6f-4573-b4f4-38d8e010626f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding master texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "533099a8046d4f2382d35cac21a58c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding source texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a1c0d3464274dcb801a845859b10b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity matrix...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "DataFrame index must be unique for orient='index'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIsMismatch\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProductName\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m!=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBestMatchProduct\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Optional: Join additional info from master_df\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m master_info \u001b[38;5;241m=\u001b[39m master_df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProductName\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto_dict(orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     60\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatchedBrand\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [master_info\u001b[38;5;241m.\u001b[39mget(p, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBrandName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBestMatchProduct\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     61\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatchedSubBrand\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m [master_info\u001b[38;5;241m.\u001b[39mget(p, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubBrandName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBestMatchProduct\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:333\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    328\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    329\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    330\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    332\u001b[0m     )\n\u001b[1;32m--> 333\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:2178\u001b[0m, in \u001b[0;36mDataFrame.to_dict\u001b[1;34m(self, orient, into, index)\u001b[0m\n\u001b[0;32m   2075\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2076\u001b[0m \u001b[38;5;124;03mConvert the DataFrame to a dictionary.\u001b[39;00m\n\u001b[0;32m   2077\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2174\u001b[0m \u001b[38;5;124;03m defaultdict(<class 'list'>, {'col1': 2, 'col2': 0.75})]\u001b[39;00m\n\u001b[0;32m   2175\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2176\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmethods\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mto_dict\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m to_dict\n\u001b[1;32m-> 2178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m to_dict(\u001b[38;5;28mself\u001b[39m, orient, into\u001b[38;5;241m=\u001b[39minto, index\u001b[38;5;241m=\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\methods\\to_dict.py:242\u001b[0m, in \u001b[0;36mto_dict\u001b[1;34m(df, orient, into, index)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mis_unique:\n\u001b[1;32m--> 242\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame index must be unique for orient=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    243\u001b[0m     columns \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m are_all_object_dtype_cols:\n",
      "\u001b[1;31mValueError\u001b[0m: DataFrame index must be unique for orient='index'."
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# Fill NA to empty string for concatenation\n",
    "for col in ['SourceDescription', 'SourceBrand', 'SourceSubBrand', 'SourceMasterBrand',\n",
    "            'ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# ✅ Fix 1: Emphasize brand and subbrand more semantically\n",
    "df['CombinedSourceText'] = (\n",
    "    'This product is from brand ' + df['SourceBrand'] +\n",
    "    ' and sub-brand ' + df['SourceSubBrand'] +\n",
    "    '. Description: ' + df['SourceDescription']\n",
    ")\n",
    "\n",
    "df['CombinedMasterText'] = (\n",
    "    'This product is from brand ' + df['BrandName'] +\n",
    "    ' and sub-brand ' + df['SubBrandName'] +\n",
    "    '. Description: ' + df['ProductName']\n",
    ")\n",
    "\n",
    "# Get unique master products\n",
    "master_df = df[['ProductName', 'CombinedMasterText', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates()\n",
    "master_texts = master_df['CombinedMasterText'].tolist()\n",
    "master_products = master_df['ProductName'].tolist()\n",
    "\n",
    "# Get all source texts\n",
    "source_texts = df['CombinedSourceText'].tolist()\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Batch encode\n",
    "print(\"Encoding master texts...\")\n",
    "master_embeddings = model.encode(master_texts, batch_size=2048, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "print(\"Encoding source texts...\")\n",
    "source_embeddings = model.encode(source_texts, batch_size=2048, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# Compute cosine similarity matrix (100k x N)\n",
    "print(\"Computing cosine similarity matrix...\")\n",
    "cosine_scores = util.cos_sim(source_embeddings, master_embeddings)  # Shape: [num_sources, num_masters]\n",
    "\n",
    "# Get best match indices and scores\n",
    "best_match_idx = cosine_scores.argmax(dim=1).cpu().numpy()\n",
    "best_scores = cosine_scores.max(dim=1).values.cpu().numpy()\n",
    "\n",
    "# Get matched master info\n",
    "df['BestMatchProduct'] = [master_products[i] for i in best_match_idx]\n",
    "df['SimilarityScore'] = best_scores\n",
    "df['Confidence'] = df['SimilarityScore'].apply(lambda x: 'High' if x >= 0.6 else 'Low')\n",
    "df['IsMismatch'] = df['ProductName'] != df['BestMatchProduct']\n",
    "\n",
    "# Optional: Join additional info from master_df\n",
    "master_info = master_df.set_index('ProductName').to_dict(orient='index')\n",
    "df['MatchedBrand'] = [master_info.get(p, {}).get('BrandName', '') for p in df['BestMatchProduct']]\n",
    "df['MatchedSubBrand'] = [master_info.get(p, {}).get('SubBrandName', '') for p in df['BestMatchProduct']]\n",
    "df['MatchedMasterBrand'] = [master_info.get(p, {}).get('MasterBrandName', '') for p in df['BestMatchProduct']]\n",
    "\n",
    "# ✅ Fix 2: Downgrade confidence if brand does not match\n",
    "df['IsBrandMatch'] = df['SourceBrand'].str.lower() == df['MatchedBrand'].str.lower()\n",
    "df['Confidence'] = df.apply(\n",
    "    lambda row: 'Low' if not row['IsBrandMatch'] else row['Confidence'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save output\n",
    "df.to_csv('outputofmodel_fast1.csv', index=False)\n",
    "print(\"✅ Done. Saved to 'outputofmodel_fast1.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d24c5953-3bc0-4efe-9a1b-c9cee13371b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding master texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8ec5ecbd284ad193bf31ea653ec521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding source texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd619a18af74eeca5db08dbdc985aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity matrix...\n",
      "✅ Done. Saved to 'outputofmodel_brand_emphasized1.csv'\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# Fill NA to empty string for consistency\n",
    "for col in ['SourceDescription', 'SourceBrand', 'SourceSubBrand', 'SourceMasterBrand',\n",
    "            'ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# ✅ Emphasize brand fields in Combined Source and Master texts\n",
    "df['CombinedSourceText'] = (\n",
    "    '[BRAND] ' + df['SourceBrand'] + ' [SUBBRAND] ' + df['SourceSubBrand'] +\n",
    "    ' [MASTERBRAND] ' + df['SourceMasterBrand'] +\n",
    "    ' [DESC] ' + df['SourceDescription']\n",
    ")\n",
    "\n",
    "df['CombinedMasterText'] = (\n",
    "    '[BRAND] ' + df['BrandName'] + ' [SUBBRAND] ' + df['SubBrandName'] +\n",
    "    ' [MASTERBRAND] ' + df['MasterBrandName'] +\n",
    "    ' [DESC] ' + df['ProductName']\n",
    ")\n",
    "\n",
    "# Get unique master products\n",
    "master_df = df[['ProductName', 'CombinedMasterText', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates()\n",
    "master_texts = master_df['CombinedMasterText'].tolist()\n",
    "master_products = master_df['ProductName'].tolist()\n",
    "\n",
    "# Get all source texts\n",
    "source_texts = df['CombinedSourceText'].tolist()\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Batch encode master and source texts\n",
    "print(\"Encoding master texts...\")\n",
    "master_embeddings = model.encode(master_texts, batch_size=512, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "print(\"Encoding source texts...\")\n",
    "source_embeddings = model.encode(source_texts, batch_size=512, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# Compute cosine similarity matrix\n",
    "print(\"Computing cosine similarity matrix...\")\n",
    "cosine_scores = util.cos_sim(source_embeddings, master_embeddings)  # Shape: [num_sources, num_masters]\n",
    "\n",
    "# Get best match indices and scores\n",
    "best_match_idx = cosine_scores.argmax(dim=1).cpu().numpy()\n",
    "best_scores = cosine_scores.max(dim=1).values.cpu().numpy()\n",
    "\n",
    "# Assign best match and score to main DataFrame\n",
    "df['BestMatchProduct'] = [master_products[i] for i in best_match_idx]\n",
    "df['SimilarityScore'] = best_scores\n",
    "df['Confidence'] = df['SimilarityScore'].apply(lambda x: 'High' if x >= 0.6 else 'Low')\n",
    "df['IsMismatch'] = df['ProductName'] != df['BestMatchProduct']\n",
    "\n",
    "# Safely handle non-unique ProductName entries in master_df for merging\n",
    "master_df = master_df.drop_duplicates(subset=['ProductName'])\n",
    "\n",
    "# Map additional matched info\n",
    "master_info = master_df.set_index('ProductName').to_dict(orient='index')\n",
    "df['MatchedBrand'] = [master_info.get(p, {}).get('BrandName', '') for p in df['BestMatchProduct']]\n",
    "df['MatchedSubBrand'] = [master_info.get(p, {}).get('SubBrandName', '') for p in df['BestMatchProduct']]\n",
    "df['MatchedMasterBrand'] = [master_info.get(p, {}).get('MasterBrandName', '') for p in df['BestMatchProduct']]\n",
    "\n",
    "# Save output\n",
    "df.to_csv('outputofmodel_brand_emphasized1.csv', index=False)\n",
    "print(\"✅ Done. Saved to 'outputofmodel_brand_emphasized1.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "371f0355-0a81-4cfc-a950-d9cadadb8c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting brand-restricted matching...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "  0%|          | 4/26000 [09:39<1046:01:16, 144.86s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Encode\u001b[39;00m\n\u001b[0;32m     54\u001b[0m source_embedding \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(source_text, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 55\u001b[0m master_embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(candidate_masters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombinedMasterText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Similarity\u001b[39;00m\n\u001b[0;32m     58\u001b[0m cosine_scores \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mcos_sim(source_embedding, master_embeddings)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1052\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m-> 1052\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1054\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1133\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[1;34m(self, input, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m             module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[0;32m   1128\u001b[0m         module_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1129\u001b[0m             key: value\n\u001b[0;32m   1130\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1131\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mforward_kwargs)\n\u001b[0;32m   1132\u001b[0m         }\n\u001b[1;32m-> 1133\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs)\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:437\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features, **kwargs)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[0;32m    431\u001b[0m trans_features \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    432\u001b[0m     key: value\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    435\u001b[0m }\n\u001b[1;32m--> 437\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    438\u001b[0m token_embeddings \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    439\u001b[0m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token_embeddings\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1028\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1028\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1029\u001b[0m     embedding_output,\n\u001b[0;32m   1030\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   1031\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1032\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1033\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1034\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1035\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1036\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1037\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1038\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1039\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m   1040\u001b[0m )\n\u001b[0;32m   1041\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1042\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:675\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    671\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m    673\u001b[0m layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    676\u001b[0m     hidden_states,\n\u001b[0;32m    677\u001b[0m     attention_mask,\n\u001b[0;32m    678\u001b[0m     layer_head_mask,\n\u001b[0;32m    679\u001b[0m     encoder_hidden_states,  \u001b[38;5;66;03m# as a positional argument for gradient checkpointing\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    681\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    682\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    683\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    684\u001b[0m )\n\u001b[0;32m    686\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:584\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m     cache_position: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    583\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 584\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m    585\u001b[0m         hidden_states,\n\u001b[0;32m    586\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    587\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    588\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    589\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    590\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    591\u001b[0m     )\n\u001b[0;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    593\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:514\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.55.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    512\u001b[0m     cache_position: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 514\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    515\u001b[0m         hidden_states,\n\u001b[0;32m    516\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    517\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    518\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    519\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    520\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    521\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    522\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    523\u001b[0m     )\n\u001b[0;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:374\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(\n\u001b[0;32m    361\u001b[0m         hidden_states,\n\u001b[0;32m    362\u001b[0m         attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m         cache_position,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[0;32m    371\u001b[0m bsz, tgt_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m    373\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(hidden_states)\u001b[38;5;241m.\u001b[39mview(bsz, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_head_size)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    375\u001b[0m )\n\u001b[0;32m    377\u001b[0m \u001b[38;5;66;03m# If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;66;03m# mask needs to be such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[0;32m    379\u001b[0m is_cross_attention \u001b[38;5;241m=\u001b[39m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# Fill NA\n",
    "for col in ['SourceDescription', 'SourceBrand', 'SourceSubBrand', 'SourceMasterBrand',\n",
    "            'ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# Create enhanced inputs\n",
    "df['CombinedSourceText'] = (\n",
    "    '[BRAND] ' + df['SourceBrand'] + ' [SUBBRAND] ' + df['SourceSubBrand'] +\n",
    "    ' [MASTERBRAND] ' + df['SourceMasterBrand'] +\n",
    "    ' [DESC] ' + df['SourceDescription']\n",
    ")\n",
    "\n",
    "df['CombinedMasterText'] = (\n",
    "    '[BRAND] ' + df['BrandName'] + ' [SUBBRAND] ' + df['SubBrandName'] +\n",
    "    ' [MASTERBRAND] ' + df['MasterBrandName'] +\n",
    "    ' [DESC] ' + df['ProductName']\n",
    ")\n",
    "\n",
    "# Unique master products\n",
    "master_df = df[['ProductName', 'CombinedMasterText', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates()\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Output columns\n",
    "best_matches = []\n",
    "scores = []\n",
    "matched_brands = []\n",
    "matched_subbrands = []\n",
    "matched_masterbrands = []\n",
    "\n",
    "print(\"Starting brand-restricted matching...\")\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    source_text = row['CombinedSourceText']\n",
    "    source_master_brand = row['SourceMasterBrand']\n",
    "\n",
    "    # Filter master_df by MasterBrandName\n",
    "    candidate_masters = master_df[master_df['MasterBrandName'].str.lower() == source_master_brand.lower()]\n",
    "\n",
    "    # Fallback: if no brand match, compare against all (optional)\n",
    "    if candidate_masters.empty:\n",
    "        candidate_masters = master_df\n",
    "\n",
    "    # Encode\n",
    "    source_embedding = model.encode(source_text, convert_to_tensor=True)\n",
    "    master_embeddings = model.encode(candidate_masters['CombinedMasterText'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "    # Similarity\n",
    "    cosine_scores = util.cos_sim(source_embedding, master_embeddings)[0]\n",
    "    best_idx = cosine_scores.argmax().item()\n",
    "    best_score = cosine_scores[best_idx].item()\n",
    "\n",
    "    # Save result\n",
    "    best_product = candidate_masters.iloc[best_idx]['ProductName']\n",
    "    best_matches.append(best_product)\n",
    "    scores.append(best_score)\n",
    "\n",
    "    matched_brands.append(candidate_masters.iloc[best_idx]['BrandName'])\n",
    "    matched_subbrands.append(candidate_masters.iloc[best_idx]['SubBrandName'])\n",
    "    matched_masterbrands.append(candidate_masters.iloc[best_idx]['MasterBrandName'])\n",
    "\n",
    "# Assign results\n",
    "df['BestMatchProduct'] = best_matches\n",
    "df['SimilarityScore'] = scores\n",
    "df['Confidence'] = df['SimilarityScore'].apply(lambda x: 'High' if x >= 0.6 else 'Low')\n",
    "df['IsMismatch'] = df['ProductName'] != df['BestMatchProduct']\n",
    "\n",
    "df['MatchedBrand'] = matched_brands\n",
    "df['MatchedSubBrand'] = matched_subbrands\n",
    "df['MatchedMasterBrand'] = matched_masterbrands\n",
    "\n",
    "# Save final output\n",
    "df.to_csv('output_brand_scoped_matching1.csv', index=False)\n",
    "print(\"✅ Matching complete. Results saved to output_brand_scoped_matching1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "674f3b30-6810-4d3f-84fd-48e8380989fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding master products (just once)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "100%|██████████| 656/656 [01:30<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running matching per row...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/26000 [02:54<1258:02:19, 174.20s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 64\u001b[0m\n\u001b[0;32m     60\u001b[0m     group_data \u001b[38;5;241m=\u001b[39m master_brand_groups[source_brand]\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     62\u001b[0m     group_data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf\u001b[39m\u001b[38;5;124m'\u001b[39m: master_df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m---> 64\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mencode(master_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombinedMasterText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     65\u001b[0m     }\n\u001b[0;32m     67\u001b[0m source_embedding \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(source_text, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     69\u001b[0m cosine_scores \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mcos_sim(source_embedding, group_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m'\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1052\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m-> 1052\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1054\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1133\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[1;34m(self, input, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m             module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[0;32m   1128\u001b[0m         module_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1129\u001b[0m             key: value\n\u001b[0;32m   1130\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1131\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mforward_kwargs)\n\u001b[0;32m   1132\u001b[0m         }\n\u001b[1;32m-> 1133\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs)\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:437\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features, **kwargs)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[0;32m    431\u001b[0m trans_features \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    432\u001b[0m     key: value\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    435\u001b[0m }\n\u001b[1;32m--> 437\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    438\u001b[0m token_embeddings \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    439\u001b[0m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token_embeddings\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1028\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1028\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1029\u001b[0m     embedding_output,\n\u001b[0;32m   1030\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   1031\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1032\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1033\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1034\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1035\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1036\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1037\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1038\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1039\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m   1040\u001b[0m )\n\u001b[0;32m   1041\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1042\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:675\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    671\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m    673\u001b[0m layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    676\u001b[0m     hidden_states,\n\u001b[0;32m    677\u001b[0m     attention_mask,\n\u001b[0;32m    678\u001b[0m     layer_head_mask,\n\u001b[0;32m    679\u001b[0m     encoder_hidden_states,  \u001b[38;5;66;03m# as a positional argument for gradient checkpointing\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    681\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    682\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    683\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    684\u001b[0m )\n\u001b[0;32m    686\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:584\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m     cache_position: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    583\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 584\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m    585\u001b[0m         hidden_states,\n\u001b[0;32m    586\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    587\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    588\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    589\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    590\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    591\u001b[0m     )\n\u001b[0;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    593\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:514\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.55.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    512\u001b[0m     cache_position: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 514\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    515\u001b[0m         hidden_states,\n\u001b[0;32m    516\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    517\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    518\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    519\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    520\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    521\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    522\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    523\u001b[0m     )\n\u001b[0;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:407\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    401\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(current_states)\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;241m.\u001b[39mview(bsz, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_head_size)\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    405\u001b[0m     )\n\u001b[0;32m    406\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 407\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(current_states)\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;241m.\u001b[39mview(bsz, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_head_size)\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    410\u001b[0m     )\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    413\u001b[0m         \u001b[38;5;66;03m# save all key/value_layer to cache to be re-used for fast auto-regressive generation\u001b[39;00m\n\u001b[0;32m    414\u001b[0m         cache_position \u001b[38;5;241m=\u001b[39m cache_position \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# Fill missing values\n",
    "for col in ['SourceDescription', 'SourceBrand', 'SourceSubBrand', 'SourceMasterBrand',\n",
    "            'ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# Create combined text columns\n",
    "df['CombinedSourceText'] = (\n",
    "    '[BRAND] ' + df['SourceBrand'] +\n",
    "    ' [SUBBRAND] ' + df['SourceSubBrand'] +\n",
    "    ' [MASTERBRAND] ' + df['SourceMasterBrand'] +\n",
    "    ' [DESC] ' + df['SourceDescription']\n",
    ")\n",
    "\n",
    "df['CombinedMasterText'] = (\n",
    "    '[BRAND] ' + df['BrandName'] +\n",
    "    ' [SUBBRAND] ' + df['SubBrandName'] +\n",
    "    ' [MASTERBRAND] ' + df['MasterBrandName'] +\n",
    "    ' [DESC] ' + df['ProductName']\n",
    ")\n",
    "\n",
    "# Unique master products\n",
    "master_df = df[['ProductName', 'CombinedMasterText', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates()\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Step 1: Group and encode master_df by MasterBrand\n",
    "master_brand_groups = {}\n",
    "\n",
    "print(\"Encoding master products (just once)...\")\n",
    "for brand in tqdm(master_df['MasterBrandName'].unique()):\n",
    "    group = master_df[master_df['MasterBrandName'] == brand]\n",
    "    texts = group['CombinedMasterText'].tolist()\n",
    "    embeddings = model.encode(texts, convert_to_tensor=True, batch_size=64)\n",
    "    master_brand_groups[brand.lower()] = {\n",
    "        'df': group.reset_index(drop=True),\n",
    "        'embeddings': embeddings\n",
    "    }\n",
    "\n",
    "# Step 2: Match each source row to correct brand group\n",
    "print(\"Running matching per row...\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    source_text = row['CombinedSourceText']\n",
    "    source_brand = row['SourceMasterBrand'].lower()\n",
    "\n",
    "    # Use matching group if exists, otherwise fallback to full master_df\n",
    "    if source_brand in master_brand_groups:\n",
    "        group_data = master_brand_groups[source_brand]\n",
    "    else:\n",
    "        group_data = {\n",
    "            'df': master_df.reset_index(drop=True),\n",
    "            'embeddings': model.encode(master_df['CombinedMasterText'].tolist(), convert_to_tensor=True)\n",
    "        }\n",
    "\n",
    "    source_embedding = model.encode(source_text, convert_to_tensor=True)\n",
    "\n",
    "    cosine_scores = util.cos_sim(source_embedding, group_data['embeddings'])[0]\n",
    "    best_idx = torch.argmax(cosine_scores).item()\n",
    "    best_score = cosine_scores[best_idx].item()\n",
    "\n",
    "    match_row = group_data['df'].iloc[best_idx]\n",
    "\n",
    "    results.append({\n",
    "        'BestMatchProduct': match_row['ProductName'],\n",
    "        'SimilarityScore': best_score,\n",
    "        'Confidence': 'High' if best_score >= 0.6 else 'Low',\n",
    "        'IsMismatch': row['ProductName'] != match_row['ProductName'],\n",
    "        'MatchedBrand': match_row['BrandName'],\n",
    "        'MatchedSubBrand': match_row['SubBrandName'],\n",
    "        'MatchedMasterBrand': match_row['MasterBrandName']\n",
    "    })\n",
    "\n",
    "# Step 3: Merge results into df\n",
    "results_df = pd.DataFrame(results)\n",
    "df = pd.concat([df.reset_index(drop=True), results_df], axis=1)\n",
    "\n",
    "# Save output\n",
    "df.to_csv('fast_brand_scoped_output1.csv', index=False)\n",
    "print(\"✅ Matching completed and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "430e59d3-8d1a-47fc-8c65-71060c1ab149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fast brand-scoped batched matching...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "  1%|          | 3/370 [06:28<13:11:09, 129.34s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m master_texts \u001b[38;5;241m=\u001b[39m master_group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombinedMasterText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     53\u001b[0m source_embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(source_texts, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m---> 54\u001b[0m master_embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(master_texts, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Compute cosine similarity matrix\u001b[39;00m\n\u001b[0;32m     57\u001b[0m cosine_scores \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mcos_sim(source_embeddings, master_embeddings)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1052\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m-> 1052\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1054\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1133\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[1;34m(self, input, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m             module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[0;32m   1128\u001b[0m         module_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1129\u001b[0m             key: value\n\u001b[0;32m   1130\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1131\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mforward_kwargs)\n\u001b[0;32m   1132\u001b[0m         }\n\u001b[1;32m-> 1133\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs)\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:437\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features, **kwargs)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[0;32m    431\u001b[0m trans_features \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    432\u001b[0m     key: value\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    435\u001b[0m }\n\u001b[1;32m--> 437\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    438\u001b[0m token_embeddings \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    439\u001b[0m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token_embeddings\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1028\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1028\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1029\u001b[0m     embedding_output,\n\u001b[0;32m   1030\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   1031\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1032\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1033\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1034\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1035\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1036\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1037\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1038\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1039\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m   1040\u001b[0m )\n\u001b[0;32m   1041\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1042\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:675\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    671\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m    673\u001b[0m layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    676\u001b[0m     hidden_states,\n\u001b[0;32m    677\u001b[0m     attention_mask,\n\u001b[0;32m    678\u001b[0m     layer_head_mask,\n\u001b[0;32m    679\u001b[0m     encoder_hidden_states,  \u001b[38;5;66;03m# as a positional argument for gradient checkpointing\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    681\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    682\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    683\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    684\u001b[0m )\n\u001b[0;32m    686\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:584\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m     cache_position: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    583\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 584\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m    585\u001b[0m         hidden_states,\n\u001b[0;32m    586\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    587\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    588\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    589\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    590\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    591\u001b[0m     )\n\u001b[0;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    593\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:514\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.55.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    512\u001b[0m     cache_position: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 514\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    515\u001b[0m         hidden_states,\n\u001b[0;32m    516\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    517\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    518\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    519\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    520\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    521\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    522\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    523\u001b[0m     )\n\u001b[0;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:374\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(\n\u001b[0;32m    361\u001b[0m         hidden_states,\n\u001b[0;32m    362\u001b[0m         attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m         cache_position,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[0;32m    371\u001b[0m bsz, tgt_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39msize()\n\u001b[0;32m    373\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(hidden_states)\u001b[38;5;241m.\u001b[39mview(bsz, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_head_size)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    375\u001b[0m )\n\u001b[0;32m    377\u001b[0m \u001b[38;5;66;03m# If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\u001b[39;00m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;66;03m# mask needs to be such that the encoder's padding tokens are not attended to.\u001b[39;00m\n\u001b[0;32m    379\u001b[0m is_cross_attention \u001b[38;5;241m=\u001b[39m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# Fill NA\n",
    "for col in ['SourceDescription', 'SourceBrand', 'SourceSubBrand', 'SourceMasterBrand',\n",
    "            'ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# Combine source + master text\n",
    "df['CombinedSourceText'] = (\n",
    "    '[BRAND] ' + df['SourceBrand'] +\n",
    "    ' [SUBBRAND] ' + df['SourceSubBrand'] +\n",
    "    ' [MASTERBRAND] ' + df['SourceMasterBrand'] +\n",
    "    ' [DESC] ' + df['SourceDescription']\n",
    ")\n",
    "\n",
    "df['CombinedMasterText'] = (\n",
    "    '[BRAND] ' + df['BrandName'] +\n",
    "    ' [SUBBRAND] ' + df['SubBrandName'] +\n",
    "    ' [MASTERBRAND] ' + df['MasterBrandName'] +\n",
    "    ' [DESC] ' + df['ProductName']\n",
    ")\n",
    "\n",
    "# Get unique master products\n",
    "master_df = df[['ProductName', 'CombinedMasterText', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates()\n",
    "\n",
    "# Prepare output\n",
    "final_results = []\n",
    "\n",
    "# Group both source and master by MasterBrand\n",
    "grouped_source = df.groupby(df['SourceMasterBrand'].str.lower())\n",
    "grouped_master = master_df.groupby(master_df['MasterBrandName'].str.lower())\n",
    "\n",
    "print(\"Running fast brand-scoped batched matching...\\n\")\n",
    "\n",
    "# Process each MasterBrand group\n",
    "for brand, source_group in tqdm(grouped_source, total=len(grouped_source)):\n",
    "    master_group = grouped_master.get_group(brand) if brand in grouped_master.groups else master_df\n",
    "\n",
    "    # Encode both in batch\n",
    "    source_texts = source_group['CombinedSourceText'].tolist()\n",
    "    master_texts = master_group['CombinedMasterText'].tolist()\n",
    "\n",
    "    source_embeddings = model.encode(source_texts, convert_to_tensor=True, batch_size=128)\n",
    "    master_embeddings = model.encode(master_texts, convert_to_tensor=True, batch_size=128)\n",
    "\n",
    "    # Compute cosine similarity matrix\n",
    "    cosine_scores = util.cos_sim(source_embeddings, master_embeddings)\n",
    "\n",
    "    # Get best match indices\n",
    "    best_scores, best_indices = torch.max(cosine_scores, dim=1)\n",
    "\n",
    "    # Prepare result rows\n",
    "    for pos, (df_index, row) in enumerate(source_group.iterrows()):\n",
    "        best_idx = best_indices[pos].item()\n",
    "        best_score = best_scores[pos].item()\n",
    "        match_row = master_group.iloc[best_idx]\n",
    "\n",
    "        final_results.append({\n",
    "            'Index': df_index,\n",
    "            'OriginalProductName': row['ProductName'],\n",
    "            'BestMatchProduct': match_row['ProductName'],\n",
    "            'SimilarityScore': round(best_score, 4),\n",
    "            'Confidence': 'High' if best_score >= 0.6 else 'Low',\n",
    "            'IsMismatch': row['ProductName'] != match_row['ProductName'],\n",
    "            'MatchedBrand': match_row['BrandName'],\n",
    "            'MatchedSubBrand': match_row['SubBrandName'],\n",
    "            'MatchedMasterBrand': match_row['MasterBrandName']\n",
    "        })\n",
    "\n",
    "\n",
    "# Merge back to df\n",
    "results_df = pd.DataFrame(final_results).set_index('Index')\n",
    "df.update(results_df)\n",
    "\n",
    "# Save\n",
    "df.to_csv(\"final_fast_output1.csv\", index=False)\n",
    "print(\"✅ Done. File saved as 'final_fast_output1.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "366cda86-8334-4267-92d9-ef51f9e119fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running chunked brand-scoped matching with progress bars...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "\n",
      "\u001b[AMatching chunks:   0%|          | 0/94 [00:00<?, ?it/s]\n",
      "\u001b[AMatching chunks:   1%|          | 1/94 [00:02<03:32,  2.28s/it]\n",
      "\u001b[AMatching chunks:   2%|▏         | 2/94 [00:03<02:57,  1.93s/it]\n",
      "\u001b[AMatching chunks:   3%|▎         | 3/94 [00:05<02:49,  1.87s/it]\n",
      "\u001b[AMatching chunks:   4%|▍         | 4/94 [00:08<03:04,  2.06s/it]\n",
      "\u001b[AMatching chunks:   5%|▌         | 5/94 [00:10<03:09,  2.12s/it]\n",
      "\u001b[AMatching chunks:   6%|▋         | 6/94 [00:12<03:08,  2.14s/it]\n",
      "\u001b[AMatching chunks:   7%|▋         | 7/94 [00:14<03:14,  2.23s/it]\n",
      "\u001b[AMatching chunks:   9%|▊         | 8/94 [00:16<03:04,  2.15s/it]\n",
      "\u001b[AMatching chunks:  10%|▉         | 9/94 [00:19<03:04,  2.17s/it]\n",
      "\u001b[AMatching chunks:  11%|█         | 10/94 [00:21<02:55,  2.08s/it]\n",
      "\u001b[AMatching chunks:  12%|█▏        | 11/94 [00:23<03:04,  2.22s/it]\n",
      "\u001b[AMatching chunks:  13%|█▎        | 12/94 [00:26<03:11,  2.34s/it]\n",
      "\u001b[AMatching chunks:  14%|█▍        | 13/94 [00:28<03:07,  2.32s/it]\n",
      "\u001b[AMatching chunks:  15%|█▍        | 14/94 [00:30<02:59,  2.25s/it]\n",
      "\u001b[AMatching chunks:  16%|█▌        | 15/94 [00:32<02:45,  2.10s/it]\n",
      "\u001b[AMatching chunks:  17%|█▋        | 16/94 [00:34<02:42,  2.08s/it]\n",
      "\u001b[AMatching chunks:  18%|█▊        | 17/94 [00:36<02:38,  2.05s/it]\n",
      "\u001b[AMatching chunks:  19%|█▉        | 18/94 [00:38<02:31,  1.99s/it]\n",
      "\u001b[AMatching chunks:  20%|██        | 19/94 [00:40<02:38,  2.11s/it]\n",
      "\u001b[AMatching chunks:  21%|██▏       | 20/94 [00:42<02:40,  2.17s/it]\n",
      "\u001b[AMatching chunks:  22%|██▏       | 21/94 [00:45<02:44,  2.26s/it]\n",
      "\u001b[AMatching chunks:  23%|██▎       | 22/94 [00:47<02:44,  2.28s/it]\n",
      "\u001b[AMatching chunks:  24%|██▍       | 23/94 [00:49<02:35,  2.18s/it]\n",
      "\u001b[AMatching chunks:  26%|██▌       | 24/94 [00:51<02:25,  2.08s/it]\n",
      "\u001b[AMatching chunks:  27%|██▋       | 25/94 [00:53<02:25,  2.11s/it]\n",
      "\u001b[AMatching chunks:  28%|██▊       | 26/94 [00:56<02:32,  2.25s/it]\n",
      "\u001b[AMatching chunks:  29%|██▊       | 27/94 [00:58<02:32,  2.27s/it]\n",
      "\u001b[AMatching chunks:  30%|██▉       | 28/94 [01:01<02:35,  2.35s/it]\n",
      "\u001b[AMatching chunks:  31%|███       | 29/94 [01:02<02:23,  2.21s/it]\n",
      "\u001b[AMatching chunks:  32%|███▏      | 30/94 [01:04<02:14,  2.10s/it]\n",
      "\u001b[AMatching chunks:  33%|███▎      | 31/94 [01:06<02:09,  2.06s/it]\n",
      "\u001b[AMatching chunks:  34%|███▍      | 32/94 [01:08<02:05,  2.02s/it]\n",
      "\u001b[AMatching chunks:  35%|███▌      | 33/94 [01:10<02:05,  2.06s/it]\n",
      "\u001b[AMatching chunks:  36%|███▌      | 34/94 [01:13<02:10,  2.18s/it]\n",
      "\u001b[AMatching chunks:  37%|███▋      | 35/94 [01:15<02:15,  2.29s/it]\n",
      "\u001b[AMatching chunks:  38%|███▊      | 36/94 [01:17<02:06,  2.18s/it]\n",
      "\u001b[AMatching chunks:  39%|███▉      | 37/94 [01:20<02:05,  2.20s/it]\n",
      "\u001b[AMatching chunks:  40%|████      | 38/94 [01:21<01:59,  2.14s/it]\n",
      "\u001b[AMatching chunks:  41%|████▏     | 39/94 [01:24<01:56,  2.12s/it]\n",
      "\u001b[AMatching chunks:  43%|████▎     | 40/94 [01:26<01:52,  2.08s/it]\n",
      "\u001b[AMatching chunks:  44%|████▎     | 41/94 [01:28<01:57,  2.21s/it]\n",
      "\u001b[AMatching chunks:  45%|████▍     | 42/94 [01:31<01:59,  2.29s/it]\n",
      "\u001b[AMatching chunks:  46%|████▌     | 43/94 [01:33<01:56,  2.28s/it]\n",
      "\u001b[AMatching chunks:  47%|████▋     | 44/94 [01:35<01:51,  2.24s/it]\n",
      "\u001b[AMatching chunks:  48%|████▊     | 45/94 [01:37<01:45,  2.15s/it]\n",
      "\u001b[AMatching chunks:  49%|████▉     | 46/94 [01:39<01:42,  2.13s/it]\n",
      "\u001b[AMatching chunks:  50%|█████     | 47/94 [01:41<01:38,  2.09s/it]\n",
      "\u001b[AMatching chunks:  51%|█████     | 48/94 [01:43<01:38,  2.14s/it]\n",
      "\u001b[AMatching chunks:  52%|█████▏    | 49/94 [01:46<01:45,  2.34s/it]\n",
      "\u001b[AMatching chunks:  53%|█████▎    | 50/94 [01:49<01:45,  2.39s/it]\n",
      "\u001b[AMatching chunks:  54%|█████▍    | 51/94 [01:51<01:37,  2.28s/it]\n",
      "\u001b[AMatching chunks:  55%|█████▌    | 52/94 [01:53<01:32,  2.21s/it]\n",
      "\u001b[AMatching chunks:  56%|█████▋    | 53/94 [01:55<01:30,  2.21s/it]\n",
      "\u001b[AMatching chunks:  57%|█████▋    | 54/94 [01:57<01:27,  2.19s/it]\n",
      "\u001b[AMatching chunks:  59%|█████▊    | 55/94 [01:59<01:26,  2.22s/it]\n",
      "\u001b[AMatching chunks:  60%|█████▉    | 56/94 [02:02<01:29,  2.35s/it]\n",
      "\u001b[AMatching chunks:  61%|██████    | 57/94 [02:04<01:25,  2.30s/it]\n",
      "\u001b[AMatching chunks:  62%|██████▏   | 58/94 [02:06<01:18,  2.18s/it]\n",
      "\u001b[AMatching chunks:  63%|██████▎   | 59/94 [02:08<01:14,  2.14s/it]\n",
      "\u001b[AMatching chunks:  64%|██████▍   | 60/94 [02:10<01:14,  2.18s/it]\n",
      "\u001b[AMatching chunks:  65%|██████▍   | 61/94 [02:12<01:11,  2.16s/it]\n",
      "\u001b[AMatching chunks:  66%|██████▌   | 62/94 [02:15<01:11,  2.23s/it]\n",
      "\u001b[AMatching chunks:  67%|██████▋   | 63/94 [02:17<01:11,  2.32s/it]\n",
      "\u001b[AMatching chunks:  68%|██████▊   | 64/94 [02:19<01:07,  2.26s/it]\n",
      "\u001b[AMatching chunks:  69%|██████▉   | 65/94 [02:21<01:03,  2.19s/it]\n",
      "\u001b[AMatching chunks:  70%|███████   | 66/94 [02:23<00:59,  2.14s/it]\n",
      "\u001b[AMatching chunks:  71%|███████▏  | 67/94 [02:26<00:57,  2.12s/it]\n",
      "\u001b[AMatching chunks:  72%|███████▏  | 68/94 [02:28<00:56,  2.17s/it]\n",
      "\u001b[AMatching chunks:  73%|███████▎  | 69/94 [02:30<00:54,  2.20s/it]\n",
      "\u001b[AMatching chunks:  74%|███████▍  | 70/94 [02:33<00:54,  2.27s/it]\n",
      "\u001b[AMatching chunks:  76%|███████▌  | 71/94 [02:35<00:54,  2.35s/it]\n",
      "\u001b[AMatching chunks:  77%|███████▋  | 72/94 [02:37<00:50,  2.29s/it]\n",
      "\u001b[AMatching chunks:  78%|███████▊  | 73/94 [02:39<00:46,  2.22s/it]\n",
      "\u001b[AMatching chunks:  79%|███████▊  | 74/94 [02:41<00:43,  2.18s/it]\n",
      "\u001b[AMatching chunks:  80%|███████▉  | 75/94 [02:43<00:39,  2.10s/it]\n",
      "\u001b[AMatching chunks:  81%|████████  | 76/94 [02:45<00:37,  2.10s/it]\n",
      "\u001b[AMatching chunks:  82%|████████▏ | 77/94 [02:48<00:37,  2.18s/it]\n",
      "\u001b[AMatching chunks:  83%|████████▎ | 78/94 [02:50<00:36,  2.29s/it]\n",
      "\u001b[AMatching chunks:  84%|████████▍ | 79/94 [02:52<00:33,  2.24s/it]\n",
      "\u001b[AMatching chunks:  85%|████████▌ | 80/94 [02:54<00:29,  2.13s/it]\n",
      "\u001b[AMatching chunks:  86%|████████▌ | 81/94 [02:56<00:26,  2.06s/it]\n",
      "\u001b[AMatching chunks:  87%|████████▋ | 82/94 [02:58<00:23,  1.96s/it]\n",
      "\u001b[AMatching chunks:  88%|████████▊ | 83/94 [03:00<00:21,  1.96s/it]\n",
      "\u001b[AMatching chunks:  89%|████████▉ | 84/94 [03:02<00:19,  1.98s/it]\n",
      "\u001b[AMatching chunks:  90%|█████████ | 85/94 [03:04<00:18,  2.05s/it]\n",
      "\u001b[AMatching chunks:  91%|█████████▏| 86/94 [03:06<00:17,  2.14s/it]\n",
      "\u001b[AMatching chunks:  93%|█████████▎| 87/94 [03:09<00:15,  2.17s/it]\n",
      "\u001b[AMatching chunks:  94%|█████████▎| 88/94 [03:11<00:12,  2.12s/it]\n",
      "\u001b[AMatching chunks:  95%|█████████▍| 89/94 [03:13<00:10,  2.08s/it]\n",
      "\u001b[AMatching chunks:  96%|█████████▌| 90/94 [03:15<00:08,  2.06s/it]\n",
      "\u001b[AMatching chunks:  97%|█████████▋| 91/94 [03:17<00:06,  2.08s/it]\n",
      "\u001b[AMatching chunks:  98%|█████████▊| 92/94 [03:19<00:04,  2.15s/it]\n",
      "\u001b[AMatching chunks:  99%|█████████▉| 93/94 [03:22<00:02,  2.21s/it]\n",
      "\u001b[AMatching chunks: 100%|██████████| 94/94 [03:22<00:00,  1.71s/it]\n",
      "MasterBrand groups:   0%|          | 1/370 [04:50<29:48:07, 290.75s/it]\n",
      "\u001b[AMatching chunks:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "MasterBrand groups:   1%|          | 2/370 [06:20<17:39:13, 172.70s/it]\n",
      "\u001b[AMatching chunks:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A                                                      \n",
      "\u001b[AMatching chunks:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "MasterBrand groups:   1%|          | 4/370 [08:06<9:55:02, 97.55s/it]  \n",
      "\u001b[AMatching chunks:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "MasterBrand groups:   1%|▏         | 5/370 [09:44<9:54:34, 97.74s/it]\n",
      "\u001b[AMatching chunks:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "\u001b[A                                                      \n",
      "\u001b[AMatching chunks:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "MasterBrand groups:   2%|▏         | 7/370 [12:18<10:38:02, 105.46s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 78\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Precompute master embeddings\u001b[39;00m\n\u001b[0;32m     77\u001b[0m master_texts \u001b[38;5;241m=\u001b[39m master_group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombinedMasterText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m---> 78\u001b[0m master_embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(master_texts, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Source texts and row indices\u001b[39;00m\n\u001b[0;32m     81\u001b[0m source_texts \u001b[38;5;241m=\u001b[39m source_group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombinedSourceText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1052\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m-> 1052\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1054\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1133\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[1;34m(self, input, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m             module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[0;32m   1128\u001b[0m         module_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1129\u001b[0m             key: value\n\u001b[0;32m   1130\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1131\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mforward_kwargs)\n\u001b[0;32m   1132\u001b[0m         }\n\u001b[1;32m-> 1133\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs)\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:437\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features, **kwargs)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[0;32m    431\u001b[0m trans_features \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    432\u001b[0m     key: value\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    435\u001b[0m }\n\u001b[1;32m--> 437\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    438\u001b[0m token_embeddings \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    439\u001b[0m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token_embeddings\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1028\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1028\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1029\u001b[0m     embedding_output,\n\u001b[0;32m   1030\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   1031\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1032\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1033\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1034\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1035\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1036\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1037\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1038\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1039\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m   1040\u001b[0m )\n\u001b[0;32m   1041\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1042\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:675\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    671\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m    673\u001b[0m layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    676\u001b[0m     hidden_states,\n\u001b[0;32m    677\u001b[0m     attention_mask,\n\u001b[0;32m    678\u001b[0m     layer_head_mask,\n\u001b[0;32m    679\u001b[0m     encoder_hidden_states,  \u001b[38;5;66;03m# as a positional argument for gradient checkpointing\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    681\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    682\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    683\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    684\u001b[0m )\n\u001b[0;32m    686\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:584\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m     cache_position: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    583\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 584\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m    585\u001b[0m         hidden_states,\n\u001b[0;32m    586\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    587\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    588\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    589\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    590\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    591\u001b[0m     )\n\u001b[0;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    593\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:514\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.55.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    512\u001b[0m     cache_position: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 514\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    515\u001b[0m         hidden_states,\n\u001b[0;32m    516\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    517\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    518\u001b[0m         encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m    519\u001b[0m         encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    520\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    521\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    522\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    523\u001b[0m     )\n\u001b[0;32m    524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:438\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[0;32m    434\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    435\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    436\u001b[0m )\n\u001b[1;32m--> 438\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[0;32m    439\u001b[0m     query_layer,\n\u001b[0;32m    440\u001b[0m     key_layer,\n\u001b[0;32m    441\u001b[0m     value_layer,\n\u001b[0;32m    442\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    443\u001b[0m     dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_prob \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m    444\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m    445\u001b[0m )\n\u001b[0;32m    447\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    448\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# Fill NA\n",
    "for col in ['SourceDescription', 'SourceBrand', 'SourceSubBrand', 'SourceMasterBrand',\n",
    "            'ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# Combine source + master text\n",
    "df['CombinedSourceText'] = (\n",
    "    '[BRAND] ' + df['SourceBrand'] +\n",
    "    ' [SUBBRAND] ' + df['SourceSubBrand'] +\n",
    "    ' [MASTERBRAND] ' + df['SourceMasterBrand'] +\n",
    "    ' [DESC] ' + df['SourceDescription']\n",
    ")\n",
    "\n",
    "df['CombinedMasterText'] = (\n",
    "    '[BRAND] ' + df['BrandName'] +\n",
    "    ' [SUBBRAND] ' + df['SubBrandName'] +\n",
    "    ' [MASTERBRAND] ' + df['MasterBrandName'] +\n",
    "    ' [DESC] ' + df['ProductName']\n",
    ")\n",
    "\n",
    "# Get unique master products\n",
    "master_df = df[['ProductName', 'CombinedMasterText', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates()\n",
    "\n",
    "# Prepare output\n",
    "final_results = []\n",
    "\n",
    "# Group both source and master by MasterBrand\n",
    "grouped_source = df.groupby(df['SourceMasterBrand'].str.lower())\n",
    "grouped_master = master_df.groupby(master_df['MasterBrandName'].str.lower())\n",
    "\n",
    "print(\"Running chunked brand-scoped matching with progress bars...\\n\")\n",
    "\n",
    "# Function to process chunks of source embeddings\n",
    "def match_in_chunks(source_texts, master_embeddings, master_group, original_indices, batch_size=256):\n",
    "    chunk_results = []\n",
    "    for i in tqdm(range(0, len(source_texts), batch_size), desc=\" ↳ Matching chunks\", leave=False):\n",
    "        chunk = source_texts[i:i+batch_size]\n",
    "        chunk_embeddings = model.encode(chunk, convert_to_tensor=True, batch_size=128)\n",
    "        cosine_scores = util.cos_sim(chunk_embeddings, master_embeddings)\n",
    "        best_scores, best_indices = torch.max(cosine_scores, dim=1)\n",
    "\n",
    "        for j in range(len(chunk)):\n",
    "            df_index = original_indices[i + j]\n",
    "            best_idx = best_indices[j].item()\n",
    "            best_score = best_scores[j].item()\n",
    "            match_row = master_group.iloc[best_idx]\n",
    "\n",
    "            chunk_results.append({\n",
    "                'Index': df_index,\n",
    "                'BestMatchProduct': match_row['ProductName'],\n",
    "                'SimilarityScore': round(best_score, 4),\n",
    "                'Confidence': 'High' if best_score >= 0.6 else 'Low',\n",
    "                'IsMismatch': df.loc[df_index]['ProductName'] != match_row['ProductName'],\n",
    "                'MatchedBrand': match_row['BrandName'],\n",
    "                'MatchedSubBrand': match_row['SubBrandName'],\n",
    "                'MatchedMasterBrand': match_row['MasterBrandName']\n",
    "            })\n",
    "    return chunk_results\n",
    "\n",
    "# Process each MasterBrand group\n",
    "for brand, source_group in tqdm(grouped_source, desc=\"MasterBrand groups\"):\n",
    "    master_group = grouped_master.get_group(brand) if brand in grouped_master.groups else master_df\n",
    "\n",
    "    # Precompute master embeddings\n",
    "    master_texts = master_group['CombinedMasterText'].tolist()\n",
    "    master_embeddings = model.encode(master_texts, convert_to_tensor=True, batch_size=128)\n",
    "\n",
    "    # Source texts and row indices\n",
    "    source_texts = source_group['CombinedSourceText'].tolist()\n",
    "    source_indices = source_group.index.tolist()\n",
    "\n",
    "    # Chunked matching\n",
    "    results = match_in_chunks(source_texts, master_embeddings, master_group, source_indices)\n",
    "    final_results.extend(results)\n",
    "\n",
    "# Merge back\n",
    "results_df = pd.DataFrame(final_results).set_index('Index')\n",
    "df.update(results_df)\n",
    "\n",
    "# Save\n",
    "df.to_csv(\"final_fast_output_chunked.csv\", index=False)\n",
    "print(\"✅ Done. Output saved as 'final_fast_output_chunked.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fd20ee4-53e2-4870-923b-857973fd3558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matching complete. Output saved to 'outputofmodel2.csv'\n",
      "                   SourceDescription              ProductName  \\\n",
      "0             Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "1      Loreal Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "2               Curls 10 In 1 Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "3  SOIN MULTI BENEFICES CREME-MOUSSE  10 IN 1 CREAM IN MOUSSE   \n",
      "4     L'Oreal Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "\n",
      "                BestMatchProduct  SimilarityScore Confidence  \n",
      "0                    CURL MOUSSE         0.857571       High  \n",
      "1                    CURL MOUSSE         0.828772       High  \n",
      "2      CURLS FIRM STYLING MOUSSE         0.821535       High  \n",
      "3        10 IN 1 CREAM IN MOUSSE         0.766348       High  \n",
      "4  NATURAL CURL ENHANCING MOUSSE         0.820982       High  \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# Drop rows with missing SourceDescription or ProductName\n",
    "df = df.dropna(subset=['SourceDescription', 'ProductName'])\n",
    "\n",
    "# Replace NaNs in helper columns with empty strings\n",
    "for col in ['SourceBrand', 'SourceSubBrand', 'SourceMasterBrand']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# Create enriched text for raw data\n",
    "df['CombinedSourceText'] = (\n",
    "    'Description: ' + df['SourceDescription'].astype(str) + ' | ' +\n",
    "    'Brand: ' + df['SourceBrand'] + ' | ' +\n",
    "    'SubBrand: ' + df['SourceSubBrand'] + ' | ' +\n",
    "    'MasterBrand: ' + df['SourceMasterBrand']\n",
    ")\n",
    "\n",
    "# Create master list with additional brand info\n",
    "master_df = df[['ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates().fillna('')\n",
    "\n",
    "# Create enriched master product text\n",
    "master_df['CombinedMasterText'] = (\n",
    "    'Description: ' + master_df['ProductName'].astype(str) + ' | ' +\n",
    "    'Brand: ' + master_df['BrandName'] + ' | ' +\n",
    "    'SubBrand: ' + master_df['SubBrandName'] + ' | ' +\n",
    "    'MasterBrand: ' + master_df['MasterBrandName']\n",
    ")\n",
    "\n",
    "# Encode with SBERT\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "source_embeddings = model.encode(df['CombinedSourceText'].tolist(), convert_to_tensor=True)\n",
    "master_embeddings = model.encode(master_df['CombinedMasterText'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarities\n",
    "cosine_scores = util.cos_sim(source_embeddings, master_embeddings)\n",
    "\n",
    "# Get best matches\n",
    "best_match_idx = cosine_scores.argmax(dim=1).cpu().numpy()\n",
    "best_scores = cosine_scores.max(dim=1).values.cpu().numpy()\n",
    "\n",
    "# Add matched info to df\n",
    "df['BestMatchProduct'] = master_df.iloc[best_match_idx]['ProductName'].values\n",
    "df['MatchedBrand'] = master_df.iloc[best_match_idx]['BrandName'].values\n",
    "df['MatchedSubBrand'] = master_df.iloc[best_match_idx]['SubBrandName'].values\n",
    "df['MatchedMasterBrand'] = master_df.iloc[best_match_idx]['MasterBrandName'].values\n",
    "df['SimilarityScore'] = best_scores\n",
    "\n",
    "# Confidence tagging\n",
    "threshold = 0.6\n",
    "df['Confidence'] = df['SimilarityScore'].apply(lambda x: 'High' if x >= threshold else 'Low')\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('outputofmodel2.csv', index=False)\n",
    "\n",
    "# Preview\n",
    "print(\"✅ Matching complete. Output saved to 'outputofmodel2.csv'\")\n",
    "print(df[['SourceDescription', 'ProductName', 'BestMatchProduct', 'SimilarityScore', 'Confidence']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f71a7390-2757-4a72-bc05-7b2eac7cb358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Loading SBERT model...\n",
      "🔄 Encoding source texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332f34bc6022414c95f08703ea340d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Encoding master texts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d79406ab68e491aa8ed75cdac8df41c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Computing cosine similarity matrix...\n",
      "🔄 Extracting best matches...\n",
      "✅ Matching complete. Output saved to 'outputofmodel3.csv'\n",
      "                   SourceDescription              ProductName  \\\n",
      "0             Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "1      Loreal Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "2               Curls 10 In 1 Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "3  SOIN MULTI BENEFICES CREME-MOUSSE  10 IN 1 CREAM IN MOUSSE   \n",
      "4     L'Oreal Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "\n",
      "            BestMatchProduct  SimilarityScore Confidence  \n",
      "0                CURL MOUSSE         0.804365       High  \n",
      "1                CURL MOUSSE         0.820288       High  \n",
      "2  CURLS FIRM STYLING MOUSSE         0.807076       High  \n",
      "3    10 IN 1 CREAM IN MOUSSE         0.756025       High  \n",
      "4                CURL MOUSSE         0.802275       High  \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# Drop rows with missing SourceDescription or ProductName\n",
    "df = df.dropna(subset=['SourceDescription', 'ProductName'])\n",
    "\n",
    "# Replace NaNs in brand-related and product columns with empty strings\n",
    "brand_cols_source = ['SourceBrand', 'SourceSubBrand', 'SourceMasterBrand']\n",
    "brand_cols_master = ['BrandName', 'SubBrandName', 'MasterBrandName']\n",
    "for col in brand_cols_source + brand_cols_master:\n",
    "    df[col] = df[col].fillna('').astype(str)\n",
    "\n",
    "# Create BrandContext for source and master\n",
    "df['BrandContext'] = (\n",
    "    df['SourceBrand'].str.strip() + ' ' +\n",
    "    df['SourceSubBrand'].str.strip() + ' ' +\n",
    "    df['SourceMasterBrand'].str.strip()\n",
    ").str.strip()\n",
    "\n",
    "# Create enriched text for raw data\n",
    "df['CombinedSourceText'] = (\n",
    "    'Description: ' + df['SourceDescription'].astype(str).str.strip() + ' | ' +\n",
    "    'BrandContext: ' + df['BrandContext']\n",
    ")\n",
    "\n",
    "# Create master product list with deduplication\n",
    "master_df = df[['ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates().copy()\n",
    "\n",
    "# Fill missing brand data in master\n",
    "for col in ['BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    master_df[col] = master_df[col].fillna('').astype(str)\n",
    "\n",
    "# Create BrandContext in master\n",
    "master_df['BrandContext'] = (\n",
    "    master_df['BrandName'].str.strip() + ' ' +\n",
    "    master_df['SubBrandName'].str.strip() + ' ' +\n",
    "    master_df['MasterBrandName'].str.strip()\n",
    ").str.strip()\n",
    "\n",
    "# Create enriched master product text\n",
    "master_df['CombinedMasterText'] = (\n",
    "    'Description: ' + master_df['ProductName'].astype(str).str.strip() + ' | ' +\n",
    "    'BrandContext: ' + master_df['BrandContext']\n",
    ")\n",
    "\n",
    "# Load model\n",
    "print(\"🔄 Loading SBERT model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode with progress bar\n",
    "print(\"🔄 Encoding source texts...\")\n",
    "source_texts = df['CombinedSourceText'].tolist()\n",
    "source_embeddings = model.encode(source_texts, convert_to_tensor=True, batch_size=128, show_progress_bar=True)\n",
    "\n",
    "print(\"🔄 Encoding master texts...\")\n",
    "master_texts = master_df['CombinedMasterText'].tolist()\n",
    "master_embeddings = model.encode(master_texts, convert_to_tensor=True, batch_size=128, show_progress_bar=True)\n",
    "\n",
    "# Compute cosine similarities\n",
    "print(\"🔄 Computing cosine similarity matrix...\")\n",
    "cosine_scores = util.cos_sim(source_embeddings, master_embeddings)\n",
    "\n",
    "# Get best matches\n",
    "print(\"🔄 Extracting best matches...\")\n",
    "best_match_idx = cosine_scores.argmax(dim=1).cpu().numpy()\n",
    "best_scores = cosine_scores.max(dim=1).values.cpu().numpy()\n",
    "\n",
    "# Add matched info to df\n",
    "df['BestMatchProduct'] = master_df.iloc[best_match_idx]['ProductName'].values\n",
    "df['MatchedBrand'] = master_df.iloc[best_match_idx]['BrandName'].values\n",
    "df['MatchedSubBrand'] = master_df.iloc[best_match_idx]['SubBrandName'].values\n",
    "df['MatchedMasterBrand'] = master_df.iloc[best_match_idx]['MasterBrandName'].values\n",
    "df['SimilarityScore'] = best_scores\n",
    "\n",
    "# Confidence tagging\n",
    "threshold = 0.6\n",
    "df['Confidence'] = df['SimilarityScore'].apply(lambda x: 'High' if x >= threshold else 'Low')\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('outputofmodel3.csv', index=False)\n",
    "\n",
    "# Preview\n",
    "print(\"✅ Matching complete. Output saved to 'outputofmodel3.csv'\")\n",
    "print(df[['SourceDescription', 'ProductName', 'BestMatchProduct', 'SimilarityScore', 'Confidence']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b9323c6-77b0-457d-8bdc-5050aaf188d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Encoding source records...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c5fdf70cd0b4ec392fee5c75bb1d525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Encoding master products...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94745a20e47e44db90d3fe6dfda5e9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Computing cosine similarities...\n",
      "🔍 Checking brand consistency...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25999/25999 [00:02<00:00, 9364.54it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matching complete. Output saved to 'outputofmodel4.csv'\n",
      "                   SourceDescription              ProductName  \\\n",
      "0             Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "1      Loreal Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "2               Curls 10 In 1 Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "3  SOIN MULTI BENEFICES CREME-MOUSSE  10 IN 1 CREAM IN MOUSSE   \n",
      "4     L'Oreal Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "\n",
      "            BestMatchProduct  SimilarityScore Confidence  Accuracy  \n",
      "0    10 IN 1 CREAM IN MOUSSE         0.793987       High     False  \n",
      "1    10 IN 1 CREAM IN MOUSSE         0.864063       High     False  \n",
      "2  CURLS FIRM STYLING MOUSSE         0.781853       High     False  \n",
      "3    10 IN 1 CREAM IN MOUSSE         0.848228       High      True  \n",
      "4    10 IN 1 CREAM IN MOUSSE         0.817913       High     False  \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# Drop rows with missing required text fields\n",
    "df = df.dropna(subset=['SourceDescription', 'ProductName'])\n",
    "\n",
    "# Replace NaNs in brand-related columns with empty strings\n",
    "for col in ['SourceBrand', 'SourceSubBrand', 'SourceMasterBrand']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# Create BrandContext\n",
    "df['BrandContext'] = df['SourceMasterBrand'].astype(str) + \" \" + df['SourceBrand'] + \" \" + df['SourceSubBrand']\n",
    "df['BrandContext'] = df['BrandContext'].str.strip()\n",
    "\n",
    "# Create CombinedSourceText with brand emphasis\n",
    "df['CombinedSourceText'] = (\n",
    "    'BrandContext: ' + df['BrandContext'] + ' | ' +\n",
    "    'Description: ' + df['SourceDescription'].astype(str) + ' | ' +\n",
    "    'BrandContext: ' + df['BrandContext']  # repeated to give more weight\n",
    ")\n",
    "\n",
    "# Create master list with unique product records\n",
    "master_df = df[['ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates().fillna('')\n",
    "\n",
    "# Create master BrandContext\n",
    "master_df['BrandContext'] = master_df['MasterBrandName'].astype(str) + \" \" + master_df['BrandName'] + \" \" + master_df['SubBrandName']\n",
    "master_df['BrandContext'] = master_df['BrandContext'].str.strip()\n",
    "\n",
    "# Create CombinedMasterText\n",
    "master_df['CombinedMasterText'] = (\n",
    "    'BrandContext: ' + master_df['BrandContext'] + ' | ' +\n",
    "    'Description: ' + master_df['ProductName'].astype(str) + ' | ' +\n",
    "    'BrandContext: ' + master_df['BrandContext']\n",
    ")\n",
    "\n",
    "# Load SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode with progress bars\n",
    "print(\"🔍 Encoding source records...\")\n",
    "source_embeddings = model.encode(df['CombinedSourceText'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "print(\"📦 Encoding master products...\")\n",
    "master_embeddings = model.encode(master_df['CombinedMasterText'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# Compute cosine similarity\n",
    "print(\"📊 Computing cosine similarities...\")\n",
    "cosine_scores = util.cos_sim(source_embeddings, master_embeddings)\n",
    "\n",
    "# Get best match index and score\n",
    "best_match_idx = cosine_scores.argmax(dim=1).cpu().numpy()\n",
    "best_scores = cosine_scores.max(dim=1).values.cpu().numpy()\n",
    "\n",
    "# Attach matched info\n",
    "df['BestMatchProduct'] = master_df.iloc[best_match_idx]['ProductName'].values\n",
    "df['MatchedBrand'] = master_df.iloc[best_match_idx]['BrandName'].values\n",
    "df['MatchedSubBrand'] = master_df.iloc[best_match_idx]['SubBrandName'].values\n",
    "df['MatchedMasterBrand'] = master_df.iloc[best_match_idx]['MasterBrandName'].values\n",
    "df['SimilarityScore'] = best_scores\n",
    "\n",
    "# Set confidence flag\n",
    "threshold = 0.6\n",
    "df['Confidence'] = df['SimilarityScore'].apply(lambda x: 'High' if x >= threshold else 'Low')\n",
    "\n",
    "# Brand consistency logic\n",
    "def is_brand_consistent(source_row, match_row):\n",
    "    return (\n",
    "        source_row['SourceBrand'].strip().lower() == match_row['BrandName'].strip().lower()\n",
    "        or source_row['SourceSubBrand'].strip().lower() == match_row['SubBrandName'].strip().lower()\n",
    "        or source_row['SourceMasterBrand'].strip().lower() == match_row['MasterBrandName'].strip().lower()\n",
    "    )\n",
    "\n",
    "# Check brand consistency row-wise\n",
    "print(\"🔍 Checking brand consistency...\")\n",
    "df['BrandConsistent'] = [\n",
    "    is_brand_consistent(source_row, match_row)\n",
    "    for (_, source_row), (_, match_row) in tqdm(zip(df.iterrows(), master_df.iloc[best_match_idx].iterrows()), total=len(df))\n",
    "]\n",
    "\n",
    "# Final accuracy flag\n",
    "df['Accuracy'] = df['BrandConsistent']\n",
    "\n",
    "# Save output\n",
    "df.to_csv('outputofmodel4', index=False)\n",
    "print(\"✅ Matching complete. Output saved to 'outputofmodel4.csv'\")\n",
    "\n",
    "# Preview a few records\n",
    "print(df[['SourceDescription', 'ProductName', 'BestMatchProduct', 'SimilarityScore', 'Confidence', 'Accuracy']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81feb9fd-a3c5-49c7-9710-e7604a8e2bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Encoding source records...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d533978311a41539ea4cb8d3d1f209c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📦 Encoding master products...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0b2a0c9fc149af9cb1dbef479cddc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Computing cosine similarities...\n",
      "🔍 Checking brand consistency...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25999/25999 [00:02<00:00, 8702.32it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matching complete. Output saved to 'outputofmodel5.csv'\n",
      "                   SourceDescription              ProductName  \\\n",
      "0             Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "1      Loreal Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "2               Curls 10 In 1 Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "3  SOIN MULTI BENEFICES CREME-MOUSSE  10 IN 1 CREAM IN MOUSSE   \n",
      "4     L'Oreal Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "\n",
      "            BestMatchProduct  SimilarityScore Confidence  Accuracy  \n",
      "0                CURL MOUSSE         0.804365       High     False  \n",
      "1                CURL MOUSSE         0.820288       High     False  \n",
      "2  CURLS FIRM STYLING MOUSSE         0.807811       High     False  \n",
      "3    10 IN 1 CREAM IN MOUSSE         0.769096       High      True  \n",
      "4                CURL MOUSSE         0.802275       High     False  \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# Drop rows with missing required text fields\n",
    "df = df.dropna(subset=['SourceDescription', 'ProductName'])\n",
    "\n",
    "# Replace NaNs in brand-related columns with empty strings\n",
    "for col in ['SourceBrand', 'SourceSubBrand', 'SourceMasterBrand']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# Create BrandContext\n",
    "df['BrandContext'] = df['SourceMasterBrand'].astype(str) + \" \" + df['SourceBrand'] + \" \" + df['SourceSubBrand']\n",
    "df['BrandContext'] = df['BrandContext'].str.strip()\n",
    "\n",
    "# Create CombinedSourceText with brand emphasis\n",
    "df['CombinedSourceText'] = (\n",
    "    'Description: ' + df['SourceDescription'].astype(str) + ' | ' +\n",
    "    'BrandContext: ' + df['BrandContext']\n",
    ")\n",
    "\n",
    "# Create master list with unique product records\n",
    "master_df = df[['ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates().fillna('')\n",
    "\n",
    "# Create master BrandContext\n",
    "master_df['BrandContext'] = master_df['MasterBrandName'].astype(str) + \" \" + master_df['BrandName'] + \" \" + master_df['SubBrandName']\n",
    "master_df['BrandContext'] = master_df['BrandContext'].str.strip()\n",
    "\n",
    "# Create CombinedMasterText\n",
    "master_df['CombinedMasterText'] = (\n",
    "    'Description: ' + master_df['ProductName'].astype(str) + ' | ' +\n",
    "    'BrandContext: ' + master_df['BrandContext']\n",
    ")\n",
    "\n",
    "# Load SBERT model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode with progress bars\n",
    "print(\"🔍 Encoding source records...\")\n",
    "source_embeddings = model.encode(df['CombinedSourceText'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "print(\"📦 Encoding master products...\")\n",
    "master_embeddings = model.encode(master_df['CombinedMasterText'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# Compute cosine similarity\n",
    "print(\"📊 Computing cosine similarities...\")\n",
    "cosine_scores = util.cos_sim(source_embeddings, master_embeddings)\n",
    "\n",
    "# Get best match index and score\n",
    "best_match_idx = cosine_scores.argmax(dim=1).cpu().numpy()\n",
    "best_scores = cosine_scores.max(dim=1).values.cpu().numpy()\n",
    "\n",
    "# Attach matched info\n",
    "df['BestMatchProduct'] = master_df.iloc[best_match_idx]['ProductName'].values\n",
    "df['MatchedBrand'] = master_df.iloc[best_match_idx]['BrandName'].values\n",
    "df['MatchedSubBrand'] = master_df.iloc[best_match_idx]['SubBrandName'].values\n",
    "df['MatchedMasterBrand'] = master_df.iloc[best_match_idx]['MasterBrandName'].values\n",
    "df['SimilarityScore'] = best_scores\n",
    "\n",
    "# Set confidence flag\n",
    "threshold = 0.6\n",
    "df['Confidence'] = df['SimilarityScore'].apply(lambda x: 'High' if x >= threshold else 'Low')\n",
    "\n",
    "# Brand consistency logic\n",
    "def is_brand_consistent(source_row, match_row):\n",
    "    return (\n",
    "        source_row['SourceBrand'].strip().lower() == match_row['BrandName'].strip().lower()\n",
    "        or source_row['SourceSubBrand'].strip().lower() == match_row['SubBrandName'].strip().lower()\n",
    "        or source_row['SourceMasterBrand'].strip().lower() == match_row['MasterBrandName'].strip().lower()\n",
    "    )\n",
    "\n",
    "# Check brand consistency row-wise\n",
    "print(\"🔍 Checking brand consistency...\")\n",
    "df['BrandConsistent'] = [\n",
    "    is_brand_consistent(source_row, match_row)\n",
    "    for (_, source_row), (_, match_row) in tqdm(zip(df.iterrows(), master_df.iloc[best_match_idx].iterrows()), total=len(df))\n",
    "]\n",
    "\n",
    "# Final accuracy flag\n",
    "df['Accuracy'] = df['BrandConsistent']\n",
    "\n",
    "# Save output\n",
    "df.to_csv('outputofmodel5', index=False)\n",
    "print(\"✅ Matching complete. Output saved to 'outputofmodel5.csv'\")\n",
    "\n",
    "# Preview a few records\n",
    "print(df[['SourceDescription', 'ProductName', 'BestMatchProduct', 'SimilarityScore', 'Confidence', 'Accuracy']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4309edbc-8e73-47c8-b137-9690497e6a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matching complete. Output saved to 'outputofmodel6.csv'\n",
      "                   SourceDescription              ProductName  \\\n",
      "0             Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "1      Loreal Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "2               Curls 10 In 1 Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "3  SOIN MULTI BENEFICES CREME-MOUSSE  10 IN 1 CREAM IN MOUSSE   \n",
      "4     L'Oreal Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "\n",
      "            BestMatchProduct  SimilarityScore Confidence Accuracy  \n",
      "0                CURL MOUSSE         0.804365       High    FALSE  \n",
      "1                CURL MOUSSE         0.820288       High    FALSE  \n",
      "2  CURLS FIRM STYLING MOUSSE         0.807076       High    FALSE  \n",
      "3    10 IN 1 CREAM IN MOUSSE         0.756025       High    FALSE  \n",
      "4                CURL MOUSSE         0.802275       High    FALSE  \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# Drop rows with missing SourceDescription or ProductName\n",
    "df = df.dropna(subset=['SourceDescription', 'ProductName'])\n",
    "\n",
    "# Replace NaNs in brand-related columns with empty strings\n",
    "for col in ['SourceBrand', 'SourceSubBrand', 'SourceMasterBrand']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# Create CombinedSourceText (no repeated brand terms)\n",
    "df['CombinedSourceText'] = (\n",
    "    'Description: ' + df['SourceDescription'].astype(str) + ' | ' +\n",
    "    'BrandContext: ' + df['SourceBrand'] + ' ' + df['SourceSubBrand'] + ' ' + df['SourceMasterBrand']\n",
    ")\n",
    "\n",
    "# Prepare Master Data\n",
    "master_df = df[['ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates().fillna('')\n",
    "master_df['CombinedMasterText'] = (\n",
    "    'Description: ' + master_df['ProductName'].astype(str) + ' | ' +\n",
    "    'BrandContext: ' + master_df['BrandName'] + ' ' + master_df['SubBrandName'] + ' ' + master_df['MasterBrandName']\n",
    ")\n",
    "\n",
    "# Load SBERT Model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode texts\n",
    "source_embeddings = model.encode(df['CombinedSourceText'].tolist(), convert_to_tensor=True)\n",
    "master_embeddings = model.encode(master_df['CombinedMasterText'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# Compute cosine similarities\n",
    "cosine_scores = util.cos_sim(source_embeddings, master_embeddings)\n",
    "\n",
    "# Find best match index & score for each row\n",
    "best_match_idx = cosine_scores.argmax(dim=1).cpu().numpy()\n",
    "best_scores = cosine_scores.max(dim=1).values.cpu().numpy()\n",
    "\n",
    "# Append best match results\n",
    "df['BestMatchProduct'] = master_df.iloc[best_match_idx]['ProductName'].values\n",
    "df['MatchedBrand'] = master_df.iloc[best_match_idx]['BrandName'].values\n",
    "df['MatchedSubBrand'] = master_df.iloc[best_match_idx]['SubBrandName'].values\n",
    "df['MatchedMasterBrand'] = master_df.iloc[best_match_idx]['MasterBrandName'].values\n",
    "df['SimilarityScore'] = best_scores\n",
    "\n",
    "# Confidence tagging\n",
    "threshold = 0.6\n",
    "df['Confidence'] = df['SimilarityScore'].apply(lambda x: 'High' if x >= threshold else 'Low')\n",
    "\n",
    "# Rule-based accuracy tagging based on brand or masterbrand\n",
    "def check_brand_accuracy(row):\n",
    "    source_brand = row['SourceBrand'].strip().lower()\n",
    "    matched_brand = row['MatchedBrand'].strip().lower()\n",
    "    source_master = row['SourceMasterBrand'].strip().lower()\n",
    "    matched_master = row['MatchedMasterBrand'].strip().lower()\n",
    "\n",
    "    if source_brand and matched_brand and source_brand == matched_brand:\n",
    "        return 'TRUE'\n",
    "    elif source_master and matched_master and source_master == matched_master:\n",
    "        return 'TRUE'\n",
    "    else:\n",
    "        return 'FALSE'\n",
    "\n",
    "df['Accuracy'] = df.apply(check_brand_accuracy, axis=1)\n",
    "\n",
    "# Save output\n",
    "df.to_csv('outputofmodel6.csv', index=False)\n",
    "\n",
    "# Preview\n",
    "print(\"✅ Matching complete. Output saved to 'outputofmodel6.csv'\")\n",
    "print(df[['SourceDescription', 'ProductName', 'BestMatchProduct', 'SimilarityScore', 'Confidence', 'Accuracy']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "818e0bd7-5fb7-430e-ba73-4ca143963031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55b1d4797764a36a2c9c1319d546e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b134d35e1e246409e4d8dddab40fe94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 1 complete. Output saved to 'outputofmodel7.csv'\n",
      "                   SourceDescription              ProductName  \\\n",
      "0             Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "1      Loreal Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "2               Curls 10 In 1 Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "3  SOIN MULTI BENEFICES CREME-MOUSSE  10 IN 1 CREAM IN MOUSSE   \n",
      "4     L'Oreal Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "\n",
      "          BestMatchProduct  SimilarityScore Confidence Accuracy  \n",
      "0  10 IN 1 CREAM IN MOUSSE         0.763084       High    FALSE  \n",
      "1  10 IN 1 CREAM IN MOUSSE         0.755397       High    FALSE  \n",
      "2        CURLS WHIP MOUSSE         0.776402       High    FALSE  \n",
      "3  10 IN 1 CREAM IN MOUSSE         0.762371       High    FALSE  \n",
      "4              CURL MOUSSE         0.738674       High    FALSE  \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# Drop rows with missing SourceDescription or ProductName\n",
    "df = df.dropna(subset=['SourceDescription', 'ProductName'])\n",
    "\n",
    "# Replace NaNs in brand-related columns with empty strings\n",
    "for col in ['SourceBrand', 'SourceSubBrand', 'SourceMasterBrand', 'BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# 🧠 STEP 1: Integrate brand fields into SourceDescription and ProductName\n",
    "def enrich_with_brand(desc, brand, subbrand, masterbrand):\n",
    "    parts = [brand, subbrand, masterbrand]\n",
    "    brand_str = ' '.join([p.strip() for p in parts if p.strip()])\n",
    "    return f\"{brand_str} {desc}\".strip()\n",
    "\n",
    "df['CombinedSourceText'] = df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['SourceDescription'], row['SourceBrand'], row['SourceSubBrand'], row['SourceMasterBrand'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Prepare Master Data with enriched descriptions\n",
    "master_df = df[['ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates().fillna('')\n",
    "master_df['CombinedMasterText'] = master_df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['ProductName'], row['BrandName'], row['SubBrandName'], row['MasterBrandName'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Load SBERT Model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode texts with progress bar\n",
    "source_embeddings = model.encode(df['CombinedSourceText'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "master_embeddings = model.encode(master_df['CombinedMasterText'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# Compute cosine similarities\n",
    "cosine_scores = util.cos_sim(source_embeddings, master_embeddings)\n",
    "\n",
    "# Find best match index & score for each row\n",
    "best_match_idx = cosine_scores.argmax(dim=1).cpu().numpy()\n",
    "best_scores = cosine_scores.max(dim=1).values.cpu().numpy()\n",
    "\n",
    "# Append best match results\n",
    "df['BestMatchProduct'] = master_df.iloc[best_match_idx]['ProductName'].values\n",
    "df['MatchedBrand'] = master_df.iloc[best_match_idx]['BrandName'].values\n",
    "df['MatchedSubBrand'] = master_df.iloc[best_match_idx]['SubBrandName'].values\n",
    "df['MatchedMasterBrand'] = master_df.iloc[best_match_idx]['MasterBrandName'].values\n",
    "df['SimilarityScore'] = best_scores\n",
    "\n",
    "# Confidence tagging\n",
    "threshold = 0.6\n",
    "df['Confidence'] = df['SimilarityScore'].apply(lambda x: 'High' if x >= threshold else 'Low')\n",
    "\n",
    "# Rule-based accuracy tagging based on brand or masterbrand\n",
    "def check_brand_accuracy(row):\n",
    "    source_brand = row['SourceBrand'].strip().lower()\n",
    "    matched_brand = row['MatchedBrand'].strip().lower()\n",
    "    source_master = row['SourceMasterBrand'].strip().lower()\n",
    "    matched_master = row['MatchedMasterBrand'].strip().lower()\n",
    "\n",
    "    if source_brand and matched_brand and source_brand == matched_brand:\n",
    "        return 'TRUE'\n",
    "    elif source_master and matched_master and source_master == matched_master:\n",
    "        return 'TRUE'\n",
    "    else:\n",
    "        return 'FALSE'\n",
    "\n",
    "df['Accuracy'] = df.apply(check_brand_accuracy, axis=1)\n",
    "\n",
    "# Save output\n",
    "df.to_csv('outputofmodel7.csv', index=False)\n",
    "\n",
    "# Preview\n",
    "print(\"✅ Step 1 complete. Output saved to 'outputofmodel7.csv'\")\n",
    "print(df[['SourceDescription', 'ProductName', 'BestMatchProduct', 'SimilarityScore', 'Confidence', 'Accuracy']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "118854f3-0a05-48fd-a57c-d449ddcfbfa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56d4729fa9904da18e2d4d5d0df31e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441e748ace504e9cabd03532948a1762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/319 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Step 1 complete. Output saved to 'outputofmodel8.csv'\n",
      "                   SourceDescription              ProductName  \\\n",
      "0             Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "1      Loreal Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "2               Curls 10 In 1 Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "3  SOIN MULTI BENEFICES CREME-MOUSSE  10 IN 1 CREAM IN MOUSSE   \n",
      "4     L'Oreal Curl Expression Mousse  10 IN 1 CREAM IN MOUSSE   \n",
      "\n",
      "          BestMatchProduct  SimilarityScore Confidence Accuracy  \n",
      "0  10 IN 1 CREAM IN MOUSSE         0.763084       High    FALSE  \n",
      "1  10 IN 1 CREAM IN MOUSSE         0.755397       High    FALSE  \n",
      "2        CURLS WHIP MOUSSE         0.776402       High    FALSE  \n",
      "3  10 IN 1 CREAM IN MOUSSE         0.762371       High    FALSE  \n",
      "4              CURL MOUSSE         0.738674       High    FALSE  \n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# Drop rows with missing SourceDescription or ProductName\n",
    "df = df.dropna(subset=['SourceDescription', 'ProductName'])\n",
    "\n",
    "# Replace NaNs in brand-related columns with empty strings\n",
    "for col in ['SourceBrand', 'SourceSubBrand', 'SourceMasterBrand', 'BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# 🧠 STEP 1: Integrate brand fields into SourceDescription and ProductName\n",
    "def enrich_with_brand(desc, brand, subbrand, masterbrand):\n",
    "    parts = [brand, subbrand, masterbrand]\n",
    "    brand_str = ' '.join([p.strip() for p in parts if p.strip()])\n",
    "    return f\"{brand_str} {desc}\".strip()\n",
    "\n",
    "df['CombinedSourceText'] = df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['SourceDescription'], row['SourceBrand'], row['SourceSubBrand'], row['SourceMasterBrand'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Prepare Master Data with enriched descriptions\n",
    "master_df = df[['ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates().fillna('')\n",
    "master_df['CombinedMasterText'] = master_df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['ProductName'], row['BrandName'], row['SubBrandName'], row['MasterBrandName'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Load SBERT Model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode texts with progress bar\n",
    "source_embeddings = model.encode(df['CombinedSourceText'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "master_embeddings = model.encode(master_df['CombinedMasterText'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# Compute cosine similarities\n",
    "cosine_scores = util.cos_sim(source_embeddings, master_embeddings)\n",
    "\n",
    "# Find best match index & score for each row\n",
    "best_match_idx = cosine_scores.argmax(dim=1).cpu().numpy()\n",
    "best_scores = cosine_scores.max(dim=1).values.cpu().numpy()\n",
    "\n",
    "# Append best match results\n",
    "df['BestMatchProduct'] = master_df.iloc[best_match_idx]['ProductName'].values\n",
    "df['MatchedBrand'] = master_df.iloc[best_match_idx]['BrandName'].values\n",
    "df['MatchedSubBrand'] = master_df.iloc[best_match_idx]['SubBrandName'].values\n",
    "df['MatchedMasterBrand'] = master_df.iloc[best_match_idx]['MasterBrandName'].values\n",
    "df['SimilarityScore'] = best_scores\n",
    "\n",
    "# Confidence tagging\n",
    "threshold = 0.7\n",
    "df['Confidence'] = df['SimilarityScore'].apply(lambda x: 'High' if x >= threshold else 'Low')\n",
    "\n",
    "# Save output\n",
    "df.to_csv('outputofmodel8.csv', index=False)\n",
    "\n",
    "# Preview\n",
    "print(\"✅ Step 1 complete. Output saved to 'outputofmodel8.csv'\")\n",
    "print(df[['SourceDescription', 'ProductName', 'BestMatchProduct', 'SimilarityScore', 'Confidence', 'Accuracy']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f172632-e085-4410-a8f6-2b5bdc89b5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\Temp\\ipykernel_3288\\2808660426.py:5: DtypeWarning: Columns (1,12,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('SampleFullData.csv')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85469a1f7a747ac8fd7417d6bb645ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10532 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9cf06fad3144a59aefc98e3f5503ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:116] data. DefaultCPUAllocator: not enough memory: you tried to allocate 48703384520 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m master_embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(master_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombinedMasterText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist(), convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Compute cosine similarities\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m cosine_scores \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mcos_sim(source_embeddings, master_embeddings)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Find best match index & score for each row\u001b[39;00m\n\u001b[0;32m     43\u001b[0m best_match_idx \u001b[38;5;241m=\u001b[39m cosine_scores\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sentence_transformers\\util.py:125\u001b[0m, in \u001b[0;36mcos_sim\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m    123\u001b[0m a_norm \u001b[38;5;241m=\u001b[39m normalize_embeddings(a)\n\u001b[0;32m    124\u001b[0m b_norm \u001b[38;5;241m=\u001b[39m normalize_embeddings(b)\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmm(a_norm, b_norm\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mto_dense()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:116] data. DefaultCPUAllocator: not enough memory: you tried to allocate 48703384520 bytes."
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# Drop rows with missing SourceDescription or ProductName\n",
    "df = df.dropna(subset=['SourceDescription', 'ProductName'])\n",
    "\n",
    "# Replace NaNs in brand-related columns with empty strings\n",
    "for col in ['SourceBrand', 'SourceSubBrand', 'SourceMasterBrand', 'BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# 🧠 STEP 1: Integrate brand fields into SourceDescription and ProductName\n",
    "def enrich_with_brand(desc, brand, subbrand, masterbrand):\n",
    "    parts = [brand, subbrand, masterbrand]\n",
    "    brand_str = ' '.join([p.strip() for p in parts if p.strip()])\n",
    "    return f\"{brand_str} {desc}\".strip()\n",
    "\n",
    "df['CombinedSourceText'] = df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['SourceDescription'], row['SourceBrand'], row['SourceSubBrand'], row['SourceMasterBrand'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Prepare Master Data with enriched descriptions\n",
    "master_df = df[['ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates().fillna('')\n",
    "master_df['CombinedMasterText'] = master_df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['ProductName'], row['BrandName'], row['SubBrandName'], row['MasterBrandName'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Load SBERT Model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode texts with progress bar\n",
    "source_embeddings = model.encode(df['CombinedSourceText'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "master_embeddings = model.encode(master_df['CombinedMasterText'].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# Compute cosine similarities\n",
    "cosine_scores = util.cos_sim(source_embeddings, master_embeddings)\n",
    "\n",
    "# Find best match index & score for each row\n",
    "best_match_idx = cosine_scores.argmax(dim=1).cpu().numpy()\n",
    "best_scores = cosine_scores.max(dim=1).values.cpu().numpy()\n",
    "\n",
    "# Append best match results\n",
    "df['BestMatchProduct'] = master_df.iloc[best_match_idx]['ProductName'].values\n",
    "df['MatchedBrand'] = master_df.iloc[best_match_idx]['BrandName'].values\n",
    "df['MatchedSubBrand'] = master_df.iloc[best_match_idx]['SubBrandName'].values\n",
    "df['MatchedMasterBrand'] = master_df.iloc[best_match_idx]['MasterBrandName'].values\n",
    "df['SimilarityScore'] = best_scores\n",
    "\n",
    "# Confidence tagging\n",
    "threshold = 0.7\n",
    "df['Confidence'] = df['SimilarityScore'].apply(lambda x: 'High' if x >= threshold else 'Low')\n",
    "\n",
    "# Save output\n",
    "df.to_csv('outputofmodel9.csv', index=False)\n",
    "\n",
    "# Preview\n",
    "print(\"✅ Step 1 complete. Output saved to 'outputofmodel9.csv'\")\n",
    "print(df[['SourceDescription', 'ProductName', 'BestMatchProduct', 'SimilarityScore', 'Confidence', 'Accuracy']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1f866b4-62b3-41dd-9361-51518b609f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\Temp\\ipykernel_3288\\3471367738.py:7: DtypeWarning: Columns (1,12,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('SampleFullData.csv')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfef619ae0c84bf484b654246f735546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Matching: 100%|██████████| 68/68 [27:14<00:00, 24.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batched matching complete! Results saved to 'output_batched_model9.csv'\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# Drop rows with missing SourceDescription or ProductName\n",
    "df = df.dropna(subset=['SourceDescription', 'ProductName'])\n",
    "\n",
    "# Fill NaNs in brand columns\n",
    "for col in ['SourceBrand', 'SourceSubBrand', 'SourceMasterBrand', 'BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# Enrich with brand info\n",
    "def enrich_with_brand(desc, brand, subbrand, masterbrand):\n",
    "    parts = [brand, subbrand, masterbrand]\n",
    "    brand_str = ' '.join([p.strip() for p in parts if p.strip()])\n",
    "    return f\"{brand_str} {desc}\".strip()\n",
    "\n",
    "df['CombinedSourceText'] = df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['SourceDescription'], row['SourceBrand'], row['SourceSubBrand'], row['SourceMasterBrand'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Master Data\n",
    "master_df = df[['ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates().fillna('')\n",
    "master_df['CombinedMasterText'] = master_df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['ProductName'], row['BrandName'], row['SubBrandName'], row['MasterBrandName'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Load SBERT Model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Encode master data once\n",
    "master_texts = master_df['CombinedMasterText'].tolist()\n",
    "master_embeddings = model.encode(master_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# Prepare for batching\n",
    "batch_size = 5000\n",
    "source_texts = df['CombinedSourceText'].tolist()\n",
    "\n",
    "best_indices = []\n",
    "best_scores = []\n",
    "\n",
    "# Batch-wise matching\n",
    "for i in tqdm(range(0, len(source_texts), batch_size), desc=\"Matching\"):\n",
    "    batch_texts = source_texts[i:i+batch_size]\n",
    "    batch_embeddings = model.encode(batch_texts, convert_to_tensor=True, show_progress_bar=False)\n",
    "    \n",
    "    scores = util.cos_sim(batch_embeddings, master_embeddings)  # shape [batch_size x M]\n",
    "    best_batch_scores, best_batch_indices = scores.max(dim=1)\n",
    "\n",
    "    best_scores.extend(best_batch_scores.cpu().numpy())\n",
    "    best_indices.extend(best_batch_indices.cpu().numpy())\n",
    "\n",
    "# Add match results\n",
    "df = df.iloc[:len(best_scores)].copy()  # ensure same length\n",
    "\n",
    "df['BestMatchProduct'] = [master_df.iloc[i]['ProductName'] for i in best_indices]\n",
    "df['MatchedBrand'] = [master_df.iloc[i]['BrandName'] for i in best_indices]\n",
    "df['MatchedSubBrand'] = [master_df.iloc[i]['SubBrandName'] for i in best_indices]\n",
    "df['MatchedMasterBrand'] = [master_df.iloc[i]['MasterBrandName'] for i in best_indices]\n",
    "df['SimilarityScore'] = best_scores\n",
    "\n",
    "# Confidence tagging\n",
    "threshold = 0.7\n",
    "df['Confidence'] = df['SimilarityScore'].apply(lambda x: 'High' if x >= threshold else 'Low')\n",
    "\n",
    "# Save results\n",
    "df.to_csv('output_batched_model9.csv', index=False)\n",
    "\n",
    "print(\"✅ Batched matching complete! Results saved to 'output_batched_model9.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c5b46540-780a-493f-bf03-5a72a4bb23c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8568abc10b6e4555ad08a252e1f379f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/26 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Matching: 100%|██████████| 1/1 [00:08<00:00,  8.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matching complete! Results saved to 'output_combined_model10.csv'\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# === Load CSV ===\n",
    "df = pd.read_csv('SampleLessData.csv')\n",
    "\n",
    "# === Drop rows with missing SourceDescription or ProductName ===\n",
    "df = df.dropna(subset=['SourceDescription', 'ProductName'])\n",
    "\n",
    "# === Fill NaNs in brand columns ===\n",
    "for col in ['SourceBrand', 'SourceSubBrand', 'SourceMasterBrand', 'BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# === Function to enrich with brand info ===\n",
    "def enrich_with_brand(desc, brand, subbrand, masterbrand):\n",
    "    parts = [brand, subbrand, masterbrand]\n",
    "    brand_str = ' '.join([p.strip() for p in parts if p.strip()])\n",
    "    return f\"{brand_str} {desc}\".strip()\n",
    "\n",
    "# === Combined Source Text ===\n",
    "df['CombinedSourceText'] = df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['SourceDescription'], row['SourceBrand'], row['SourceSubBrand'], row['SourceMasterBrand'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Prepare Master Data ===\n",
    "master_df = df[['ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates().fillna('')\n",
    "master_df['CombinedMasterText'] = master_df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['ProductName'], row['BrandName'], row['SubBrandName'], row['MasterBrandName'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Load SBERT Model ===\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# === Encode Master Texts ===\n",
    "master_texts = master_df['CombinedMasterText'].tolist()\n",
    "master_embeddings = model.encode(master_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# === Batched Matching ===\n",
    "batch_size = 5000\n",
    "source_texts = df['CombinedSourceText'].tolist()\n",
    "\n",
    "best_indices = []\n",
    "best_scores = []\n",
    "\n",
    "for i in tqdm(range(0, len(source_texts), batch_size), desc=\"Matching\"):\n",
    "    batch_texts = source_texts[i:i+batch_size]\n",
    "    batch_embeddings = model.encode(batch_texts, convert_to_tensor=True, show_progress_bar=False)\n",
    "    \n",
    "    scores = util.cos_sim(batch_embeddings, master_embeddings)  # [batch_size x M]\n",
    "    best_batch_scores, best_batch_indices = scores.max(dim=1)\n",
    "\n",
    "    best_scores.extend(best_batch_scores.cpu().numpy())\n",
    "    best_indices.extend(best_batch_indices.cpu().numpy())\n",
    "\n",
    "# === Add Matching Results ===\n",
    "df = df.iloc[:len(best_scores)].copy()  # ensure alignment\n",
    "\n",
    "df['BestMatchProduct'] = [master_df.iloc[i]['ProductName'] for i in best_indices]\n",
    "df['MatchedBrand'] = [master_df.iloc[i]['BrandName'] for i in best_indices]\n",
    "df['MatchedSubBrand'] = [master_df.iloc[i]['SubBrandName'] for i in best_indices]\n",
    "df['MatchedMasterBrand'] = [master_df.iloc[i]['MasterBrandName'] for i in best_indices]\n",
    "\n",
    "# === Score Calculation ===\n",
    "\n",
    "# SBERT score\n",
    "df['SBERTScore'] = best_scores\n",
    "\n",
    "# Fuzzy string match (normalized to 0-1)\n",
    "def fuzzy_overlap(a, b):\n",
    "    return fuzz.partial_ratio(str(a), str(b)) / 100.0\n",
    "\n",
    "df['ProductNameScore'] = df.apply(\n",
    "    lambda row: fuzzy_overlap(row['SourceDescription'], row['BestMatchProduct']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Combine scores\n",
    "alpha = 0.7  # SBERT weight\n",
    "df['SimilarityScore'] = df.apply(\n",
    "    lambda row: alpha * row['SBERTScore'] + (1 - alpha) * row['ProductNameScore'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Confidence tagging\n",
    "threshold = 0.7\n",
    "df['Confidence'] = df['SimilarityScore'].apply(lambda x: 'High' if x >= threshold else 'Low')\n",
    "\n",
    "# === Save Output ===\n",
    "df.to_csv('output_combined_model10.csv', index=False)\n",
    "\n",
    "print(\"✅ Matching complete! Results saved to 'output_combined_model10.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6642463-9017-478d-855f-7f50ba4cf978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rapidfuzz\n",
      "  Downloading rapidfuzz-3.13.0-cp313-cp313-win_amd64.whl.metadata (12 kB)\n",
      "Downloading rapidfuzz-3.13.0-cp313-cp313-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 0.8/1.6 MB 7.5 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.3/1.6 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.3/1.6 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.3/1.6 MB 6.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.3/1.6 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: rapidfuzz\n",
      "Successfully installed rapidfuzz-3.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "977d3745-2261-46ed-83ea-6f98a6849003",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\Temp\\ipykernel_3288\\1048795411.py:8: DtypeWarning: Columns (1,12,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('SampleFullData.csv')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbrand_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdesc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# === Combined Source Text ===\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombinedSourceText\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDescription: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menrich_with_brand(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSourceDescription\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;250m \u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSourceBrand\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;250m \u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSourceSubBrand\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;250m \u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSourceMasterBrand\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     26\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# === Prepare Master Data ===\u001b[39;00m\n\u001b[0;32m     30\u001b[0m master_df \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProductName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBrandName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubBrandName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMasterBrandName\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mdrop_duplicates()\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mapply()\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_generator()\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[41], line 25\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbrand_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdesc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# === Combined Source Text ===\u001b[39;00m\n\u001b[0;32m     24\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombinedSourceText\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDescription: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menrich_with_brand(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSourceDescription\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;250m \u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSourceBrand\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;250m \u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSourceSubBrand\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;250m \u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSourceMasterBrand\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     26\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# === Prepare Master Data ===\u001b[39;00m\n\u001b[0;32m     30\u001b[0m master_df \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProductName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBrandName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubBrandName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMasterBrandName\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mdrop_duplicates()\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1097\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m   1096\u001b[0m     check_dict_or_set_indexers(key)\n\u001b[1;32m-> 1097\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1099\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mEllipsis\u001b[39m:\n\u001b[0;32m   1100\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m using_copy_on_write() \u001b[38;5;129;01mor\u001b[39;00m warn_copy_on_write():\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\common.py:383\u001b[0m, in \u001b[0;36mapply_if_callable\u001b[1;34m(maybe_callable, obj, **kwargs)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_if_callable\u001b[39m(maybe_callable, obj, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    373\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    Evaluate possibly callable input using obj and kwargs if it is callable,\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m    otherwise return as it is.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m    **kwargs\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 383\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(maybe_callable):\n\u001b[0;32m    384\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m maybe_callable(obj, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m maybe_callable\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# === Load CSV ===\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# === Drop rows with missing SourceDescription or ProductName ===\n",
    "df = df.dropna(subset=['SourceDescription', 'ProductName'])\n",
    "\n",
    "# === Fill NaNs in brand columns ===\n",
    "for col in ['SourceBrand', 'SourceSubBrand', 'SourceMasterBrand', 'BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# === Function to enrich with brand info ===\n",
    "def enrich_with_brand(desc, brand, subbrand, masterbrand):\n",
    "    parts = [brand, subbrand, masterbrand]\n",
    "    brand_str = ' '.join([p.strip() for p in parts if p.strip()])\n",
    "    return f\"{brand_str} {desc}\".strip()\n",
    "\n",
    "# === Combined Source Text ===\n",
    "df['CombinedSourceText'] = df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['SourceDescription'], row['SourceBrand'], row['SourceSubBrand'], row['SourceMasterBrand'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Prepare Master Data ===\n",
    "master_df = df[['ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates().fillna('')\n",
    "master_df['CombinedMasterText'] = master_df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['ProductName'], row['BrandName'], row['SubBrandName'], row['MasterBrandName'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Load SBERT Model ===\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# === Encode Master Texts ===\n",
    "master_texts = master_df['CombinedMasterText'].tolist()\n",
    "master_embeddings = model.encode(master_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# === Batched Matching ===\n",
    "batch_size = 5000\n",
    "source_texts = df['CombinedSourceText'].tolist()\n",
    "\n",
    "best_indices = []\n",
    "best_scores = []\n",
    "\n",
    "for i in tqdm(range(0, len(source_texts), batch_size), desc=\"Matching\"):\n",
    "    batch_texts = source_texts[i:i+batch_size]\n",
    "    batch_embeddings = model.encode(batch_texts, convert_to_tensor=True, show_progress_bar=False)\n",
    "    \n",
    "    scores = util.cos_sim(batch_embeddings, master_embeddings)  # [batch_size x M]\n",
    "    best_batch_scores, best_batch_indices = scores.max(dim=1)\n",
    "\n",
    "    best_scores.extend(best_batch_scores.cpu().numpy())\n",
    "    best_indices.extend(best_batch_indices.cpu().numpy())\n",
    "\n",
    "# === Add Matching Results ===\n",
    "df = df.iloc[:len(best_scores)].copy()  # ensure alignment\n",
    "\n",
    "df['BestMatchProduct'] = [master_df.iloc[i]['ProductName'] for i in best_indices]\n",
    "df['MatchedBrand'] = [master_df.iloc[i]['BrandName'] for i in best_indices]\n",
    "df['MatchedSubBrand'] = [master_df.iloc[i]['SubBrandName'] for i in best_indices]\n",
    "df['MatchedMasterBrand'] = [master_df.iloc[i]['MasterBrandName'] for i in best_indices]\n",
    "\n",
    "# === Score Calculation ===\n",
    "\n",
    "# SBERT score\n",
    "df['SBERTScore'] = best_scores\n",
    "\n",
    "# Fuzzy string match (normalized to 0-1)\n",
    "def fuzzy_overlap(a, b):\n",
    "    return fuzz.partial_ratio(str(a), str(b)) / 100.0\n",
    "\n",
    "df['ProductNameScore'] = df.apply(\n",
    "    lambda row: fuzzy_overlap(row['SourceDescription'], row['BestMatchProduct']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Combine scores\n",
    "alpha = 0.7  # SBERT weight\n",
    "df['SimilarityScore'] = df.apply(\n",
    "    lambda row: alpha * row['SBERTScore'] + (1 - alpha) * row['ProductNameScore'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Confidence tagging\n",
    "threshold = 0.7\n",
    "df['Confidence'] = df['SimilarityScore'].apply(lambda x: 'High' if x >= threshold else 'Low')\n",
    "\n",
    "# === Save Output ===\n",
    "df.to_csv('output_combined_model10.csv', index=False)\n",
    "\n",
    "print(\"✅ Matching complete! Results saved to 'output_combined_model10.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28a32f51-cbc3-4179-96ca-01a3dcbb0361",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\Temp\\ipykernel_3288\\3427055514.py:8: DtypeWarning: Columns (1,12,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('SampleFullData.csv')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7448025328d842d7bd6df75c6b17f452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Matching: 100%|██████████| 68/68 [27:53<00:00, 24.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matching complete! Results saved to 'output_combined_model11.csv'\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# === Load CSV ===\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# === Drop rows with missing SourceDescription or ProductName ===\n",
    "df = df.dropna(subset=['SourceDescription', 'ProductName'])\n",
    "\n",
    "# === Fill NaNs in brand columns ===\n",
    "for col in ['SourceBrand', 'SourceSubBrand', 'SourceMasterBrand', 'BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# === Function to enrich with brand info ===\n",
    "def enrich_with_brand(desc, brand, subbrand, masterbrand):\n",
    "    parts = [brand, subbrand, masterbrand]\n",
    "    brand_str = ' '.join([p.strip() for p in parts if p.strip()])\n",
    "    return f\"{brand_str} {desc}\".strip()\n",
    "\n",
    "# === Combined Source Text ===\n",
    "df['CombinedSourceText'] = df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['SourceDescription'], row['SourceBrand'], row['SourceSubBrand'], row['SourceMasterBrand'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Prepare Master Data ===\n",
    "master_df = df[['ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates().fillna('')\n",
    "master_df['CombinedMasterText'] = master_df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['ProductName'], row['BrandName'], row['SubBrandName'], row['MasterBrandName'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Load SBERT Model ===\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# === Encode Master Texts ===\n",
    "master_texts = master_df['CombinedMasterText'].tolist()\n",
    "master_embeddings = model.encode(master_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# === Batched Matching ===\n",
    "batch_size = 5000\n",
    "source_texts = df['CombinedSourceText'].tolist()\n",
    "\n",
    "best_indices = []\n",
    "best_scores = []\n",
    "\n",
    "for i in tqdm(range(0, len(source_texts), batch_size), desc=\"Matching\"):\n",
    "    batch_texts = source_texts[i:i+batch_size]\n",
    "    batch_embeddings = model.encode(batch_texts, convert_to_tensor=True, show_progress_bar=False)\n",
    "    \n",
    "    scores = util.cos_sim(batch_embeddings, master_embeddings)  # [batch_size x M]\n",
    "    best_batch_scores, best_batch_indices = scores.max(dim=1)\n",
    "\n",
    "    best_scores.extend(best_batch_scores.cpu().numpy())\n",
    "    best_indices.extend(best_batch_indices.cpu().numpy())\n",
    "\n",
    "# === Add Matching Results ===\n",
    "df = df.iloc[:len(best_scores)].copy()  # ensure alignment\n",
    "\n",
    "df['BestMatchProduct'] = [master_df.iloc[i]['ProductName'] for i in best_indices]\n",
    "df['MatchedBrand'] = [master_df.iloc[i]['BrandName'] for i in best_indices]\n",
    "df['MatchedSubBrand'] = [master_df.iloc[i]['SubBrandName'] for i in best_indices]\n",
    "df['MatchedMasterBrand'] = [master_df.iloc[i]['MasterBrandName'] for i in best_indices]\n",
    "\n",
    "# === Score Calculation ===\n",
    "\n",
    "# SBERT score\n",
    "df['SBERTScore'] = best_scores\n",
    "\n",
    "# Fuzzy string match (normalized to 0-1)\n",
    "def fuzzy_overlap(a, b):\n",
    "    return fuzz.partial_ratio(str(a), str(b)) / 100.0\n",
    "\n",
    "df['ProductNameScore'] = df.apply(\n",
    "    lambda row: fuzzy_overlap(row['SourceDescription'], row['BestMatchProduct']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Combine scores\n",
    "alpha = 0.7  # SBERT weight\n",
    "df['SimilarityScore'] = df.apply(\n",
    "    lambda row: alpha * row['SBERTScore'] + (1 - alpha) * row['ProductNameScore'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Confidence tagging\n",
    "threshold = 0.7\n",
    "df['Confidence'] = df['SimilarityScore'].apply(lambda x: 'High' if x >= threshold else 'Low')\n",
    "\n",
    "# === Save Output ===\n",
    "df.to_csv('output_combined_model11.csv', index=False)\n",
    "\n",
    "print(\"✅ Matching complete! Results saved to 'output_combined_model11.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de833916-f9d3-430d-9e7c-a888f5698aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\Temp\\ipykernel_3288\\2030125212.py:8: DtypeWarning: Columns (1,12,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('SampleFullData.csv')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b529052a844539bd244e9aecdfc6aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Matching: 100%|██████████| 68/68 [28:19<00:00, 25.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matching complete! Results saved to 'output_combined_model14.csv'\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz.fuzz import token_set_ratio\n",
    "\n",
    "# === Load CSV ===\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# === Drop rows with missing SourceDescription or ProductName ===\n",
    "df = df.dropna(subset=['SourceDescription', 'ProductName'])\n",
    "\n",
    "# === Fill NaNs in brand columns ===\n",
    "for col in ['SourceBrand', 'SourceSubBrand', 'SourceMasterBrand', 'BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# === Function to enrich with brand info ===\n",
    "def enrich_with_brand(desc, brand, subbrand, masterbrand):\n",
    "    parts = [brand, subbrand, masterbrand]\n",
    "    brand_str = ' '.join([p.strip() for p in parts if p.strip()])\n",
    "    return f\"{brand_str} {desc}\".strip()\n",
    "\n",
    "# === Combined Source Text ===\n",
    "df['CombinedSourceText'] = df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['SourceDescription'], row['SourceBrand'], row['SourceSubBrand'], row['SourceMasterBrand'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Prepare Master Data ===\n",
    "master_df = df[['ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates().fillna('')\n",
    "master_df['CombinedMasterText'] = master_df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['ProductName'], row['BrandName'], row['SubBrandName'], row['MasterBrandName'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Load SBERT Model ===\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# === Encode Master Texts ===\n",
    "master_texts = master_df['CombinedMasterText'].tolist()\n",
    "master_embeddings = model.encode(master_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# === Batched Matching ===\n",
    "batch_size = 5000\n",
    "source_texts = df['CombinedSourceText'].tolist()\n",
    "\n",
    "best_indices = []\n",
    "best_scores = []\n",
    "\n",
    "for i in tqdm(range(0, len(source_texts), batch_size), desc=\"Matching\"):\n",
    "    batch_texts = source_texts[i:i+batch_size]\n",
    "    batch_embeddings = model.encode(batch_texts, convert_to_tensor=True, show_progress_bar=False)\n",
    "    \n",
    "    scores = util.cos_sim(batch_embeddings, master_embeddings)  # [batch_size x M]\n",
    "    best_batch_scores, best_batch_indices = scores.max(dim=1)\n",
    "\n",
    "    best_scores.extend(best_batch_scores.cpu().numpy())\n",
    "    best_indices.extend(best_batch_indices.cpu().numpy())\n",
    "\n",
    "# === Add Matching Results ===\n",
    "df = df.iloc[:len(best_scores)].copy()  # ensure alignment\n",
    "\n",
    "df['BestMatchProduct'] = [master_df.iloc[i]['ProductName'] for i in best_indices]\n",
    "df['MatchedBrand'] = [master_df.iloc[i]['BrandName'] for i in best_indices]\n",
    "df['MatchedSubBrand'] = [master_df.iloc[i]['SubBrandName'] for i in best_indices]\n",
    "df['MatchedMasterBrand'] = [master_df.iloc[i]['MasterBrandName'] for i in best_indices]\n",
    "\n",
    "# === Score Calculation ===\n",
    "\n",
    "# SBERT score\n",
    "df['SBERTScore'] = best_scores\n",
    "\n",
    "# Improved fuzzy matching using token_set_ratio\n",
    "def fuzzy_overlap(a, b):\n",
    "    return token_set_ratio(str(a), str(b)) / 100.0\n",
    "\n",
    "df['ProductNameScore'] = df.apply(\n",
    "    lambda row: fuzzy_overlap(row['SourceDescription'], row['BestMatchProduct']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Combine scores: adjust alpha\n",
    "alpha = 0.8  # More weight to SBERT for precision\n",
    "df['SimilarityScore'] = df.apply(\n",
    "    lambda row: alpha * row['SBERTScore'] + (1 - alpha) * row['ProductNameScore'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Confidence tagging\n",
    "threshold = 0.65\n",
    "df['Confidence'] = df['SimilarityScore'].apply(lambda x: 'High' if x >= threshold else 'Low')\n",
    "\n",
    "# === Save Output ===\n",
    "df.to_csv('output_combined_model14.csv', index=False)\n",
    "\n",
    "print(\"✅ Matching complete! Results saved to 'output_combined_model14.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e337944-9c5b-4878-811d-935a4b714e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\Temp\\ipykernel_3288\\801549580.py:8: DtypeWarning: Columns (1,12,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('SampleFullData.csv')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef31bce9c1394d60b2e7f7151ef4ed83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Matching: 100%|██████████| 68/68 [32:36<00:00, 28.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matching complete! Results saved to 'output_combined_model17.csv'\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz.fuzz import token_set_ratio\n",
    "\n",
    "# === Load CSV ===\n",
    "df = pd.read_csv('SampleLessData.csv')\n",
    "\n",
    "# === Drop rows with missing SourceDescription or ProductName ===\n",
    "df = df.dropna(subset=['SourceDescription', 'ProductName'])\n",
    "\n",
    "# === Fill NaNs in brand columns ===\n",
    "for col in ['SourceBrand', 'SourceSubBrand', 'SourceMasterBrand', 'BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# === Function to enrich with brand info ===\n",
    "def enrich_with_brand(desc, brand, subbrand, masterbrand):\n",
    "    parts = [brand, subbrand, masterbrand]\n",
    "    brand_str = ' '.join([p.strip() for p in parts if p.strip()])\n",
    "    return f\"{brand_str} {desc}\".strip()\n",
    "\n",
    "# === Combined Source Text ===\n",
    "df['CombinedSourceText'] = df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['SourceDescription'], row['SourceBrand'], row['SourceSubBrand'], row['SourceMasterBrand'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Prepare Master Data ===\n",
    "master_df = df[['ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates().fillna('')\n",
    "master_df['CombinedMasterText'] = master_df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['ProductName'], row['BrandName'], row['SubBrandName'], row['MasterBrandName'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Load SBERT Model ===\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# === Encode Master Texts ===\n",
    "master_texts = master_df['CombinedMasterText'].tolist()\n",
    "master_embeddings = model.encode(master_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# === Batched Matching ===\n",
    "batch_size = 5000\n",
    "source_texts = df['CombinedSourceText'].tolist()\n",
    "\n",
    "best_indices = []\n",
    "best_scores = []\n",
    "\n",
    "for i in tqdm(range(0, len(source_texts), batch_size), desc=\"Matching\"):\n",
    "    batch_texts = source_texts[i:i+batch_size]\n",
    "    batch_embeddings = model.encode(batch_texts, convert_to_tensor=True, show_progress_bar=False)\n",
    "    \n",
    "    scores = util.cos_sim(batch_embeddings, master_embeddings)  # [batch_size x M]\n",
    "    best_batch_scores, best_batch_indices = scores.max(dim=1)\n",
    "\n",
    "    best_scores.extend(best_batch_scores.cpu().numpy())\n",
    "    best_indices.extend(best_batch_indices.cpu().numpy())\n",
    "\n",
    "# === Add Matching Results ===\n",
    "df = df.iloc[:len(best_scores)].copy()  # ensure alignment\n",
    "\n",
    "df['BestMatchProduct'] = [master_df.iloc[i]['ProductName'] for i in best_indices]\n",
    "df['MatchedBrand'] = [master_df.iloc[i]['BrandName'] for i in best_indices]\n",
    "df['MatchedSubBrand'] = [master_df.iloc[i]['SubBrandName'] for i in best_indices]\n",
    "df['MatchedMasterBrand'] = [master_df.iloc[i]['MasterBrandName'] for i in best_indices]\n",
    "\n",
    "# === Score Calculation ===\n",
    "\n",
    "# SBERT score\n",
    "df['SBERTScore'] = best_scores\n",
    "\n",
    "# Fuzzy match (ProductNameScore)\n",
    "def fuzzy_overlap(a, b):\n",
    "    return token_set_ratio(str(a), str(b)) / 100.0\n",
    "\n",
    "df['ProductNameScore'] = df.apply(\n",
    "    lambda row: fuzzy_overlap(row['SourceDescription'], row['BestMatchProduct']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Combine scores using alpha weight\n",
    "alpha = 0.8  # More weight to SBERT for precision\n",
    "df['SimilarityScore'] = df.apply(\n",
    "    lambda row: alpha * row['SBERTScore'] + (1 - alpha) * row['ProductNameScore'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Adjust similarity based on fuzzy score ===\n",
    "def adjust_similarity(row):\n",
    "    sim = row['SimilarityScore']\n",
    "    fuzzy = row['ProductNameScore']\n",
    "    \n",
    "    if fuzzy >= 0.90:\n",
    "        return min(sim + 0.10, 1.0)\n",
    "    elif fuzzy <= 0.30:\n",
    "        return max(sim - 0.08, 0.0)\n",
    "    else:\n",
    "        return sim\n",
    "\n",
    "df['AdjustedSimilarityScore'] = df.apply(adjust_similarity, axis=1)\n",
    "\n",
    "# === Confidence tagging based on adjusted score ===\n",
    "threshold = 0.65\n",
    "df['Confidence'] = df['AdjustedSimilarityScore'].apply(lambda x: 'High' if x >= threshold else 'Low')\n",
    "\n",
    "# === Save Output ===\n",
    "df.to_csv('output_combined_model17.csv', index=False)\n",
    "\n",
    "print(\"✅ Matching complete! Results saved to 'output_combined_model1.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd31d807-bc5a-4203-acc7-ff47cb5844e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\Temp\\ipykernel_3288\\2921469059.py:8: DtypeWarning: Columns (1,12,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('SampleFullData.csv')\n",
      "Exception ignored in: 'zmq.backend.cython._zmq.Frame.__del__'\n",
      "Traceback (most recent call last):\n",
      "  File \"_zmq.py\", line 160, in zmq.backend.cython._zmq._check_rc\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbrand_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdesc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# === Combined Source Text ===\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombinedSourceText\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDescription: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menrich_with_brand(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSourceDescription\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;250m \u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSourceBrand\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;250m \u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSourceSubBrand\u001b[39m\u001b[38;5;124m'\u001b[39m],\u001b[38;5;250m \u001b[39mrow[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSourceMasterBrand\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     26\u001b[0m     axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# === Prepare Master Data ===\u001b[39;00m\n\u001b[0;32m     30\u001b[0m master_df \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProductName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBrandName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubBrandName\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMasterBrandName\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mdrop_duplicates()\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mapply()\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_generator()\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1079\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1076\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m-> 1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m   1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(v, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1271\u001b[0m, in \u001b[0;36mFrameColumnApply.series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arr, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex):\n\u001b[0;32m   1269\u001b[0m     \u001b[38;5;66;03m# GH#35462 re-pin mgr in case setitem changed it\u001b[39;00m\n\u001b[0;32m   1270\u001b[0m     ser\u001b[38;5;241m.\u001b[39m_mgr \u001b[38;5;241m=\u001b[39m mgr\n\u001b[1;32m-> 1271\u001b[0m     mgr\u001b[38;5;241m.\u001b[39mset_values(arr)\n\u001b[0;32m   1272\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(ser, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, name)\n\u001b[0;32m   1273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_view:\n\u001b[0;32m   1274\u001b[0m         \u001b[38;5;66;03m# In apply_series_generator we store the a shallow copy of the\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m         \u001b[38;5;66;03m# result, which potentially increases the ref count of this reused\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1278\u001b[0m         \u001b[38;5;66;03m# the refs here to avoid triggering a unnecessary CoW inside the\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m         \u001b[38;5;66;03m# applied function (https://github.com/pandas-dev/pandas/pull/56212)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:2076\u001b[0m, in \u001b[0;36mSingleBlockManager.set_values\u001b[1;34m(self, values)\u001b[0m\n\u001b[0;32m   2073\u001b[0m \u001b[38;5;66;03m# NOTE(CoW) Currently this is only used for FrameColumnApply.series_generator\u001b[39;00m\n\u001b[0;32m   2074\u001b[0m \u001b[38;5;66;03m# which handles CoW by setting the refs manually if necessary\u001b[39;00m\n\u001b[0;32m   2075\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;241m=\u001b[39m values\n\u001b[1;32m-> 2076\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_mgr_locs \u001b[38;5;241m=\u001b[39m BlockPlacement(\u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28mlen\u001b[39m(values)))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz.fuzz import token_set_ratio\n",
    "\n",
    "# === Load CSV ===\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# === Drop rows with missing SourceDescription or ProductName ===\n",
    "df = df.dropna(subset=['SourceDescription', 'ProductName'])\n",
    "\n",
    "# === Fill NaNs in brand columns ===\n",
    "for col in ['SourceBrand', 'SourceSubBrand', 'SourceMasterBrand', 'BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# === Function to enrich with brand info ===\n",
    "def enrich_with_brand(desc, brand, subbrand, masterbrand):\n",
    "    parts = [brand, subbrand, masterbrand]\n",
    "    brand_str = ' '.join([p.strip() for p in parts if p.strip()])\n",
    "    return f\"{brand_str} {desc}\".strip()\n",
    "\n",
    "# === Combined Source Text ===\n",
    "df['CombinedSourceText'] = df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['SourceDescription'], row['SourceBrand'], row['SourceSubBrand'], row['SourceMasterBrand'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Prepare Master Data ===\n",
    "master_df = df[['ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates().fillna('')\n",
    "master_df['CombinedMasterText'] = master_df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['ProductName'], row['BrandName'], row['SubBrandName'], row['MasterBrandName'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Load SBERT Model ===\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# === Encode Master Texts ===\n",
    "master_texts = master_df['CombinedMasterText'].tolist()\n",
    "master_embeddings = model.encode(master_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# === Batched Matching ===\n",
    "batch_size = 5000\n",
    "source_texts = df['CombinedSourceText'].tolist()\n",
    "\n",
    "best_indices = []\n",
    "best_scores = []\n",
    "\n",
    "for i in tqdm(range(0, len(source_texts), batch_size), desc=\"Matching\"):\n",
    "    batch_texts = source_texts[i:i+batch_size]\n",
    "    batch_embeddings = model.encode(batch_texts, convert_to_tensor=True, show_progress_bar=False)\n",
    "    \n",
    "    scores = util.cos_sim(batch_embeddings, master_embeddings)  # [batch_size x M]\n",
    "    best_batch_scores, best_batch_indices = scores.max(dim=1)\n",
    "\n",
    "    best_scores.extend(best_batch_scores.cpu().numpy())\n",
    "    best_indices.extend(best_batch_indices.cpu().numpy())\n",
    "\n",
    "# === Add Matching Results ===\n",
    "df = df.iloc[:len(best_scores)].copy()  # ensure alignment\n",
    "\n",
    "df['BestMatchProduct'] = [master_df.iloc[i]['ProductName'] for i in best_indices]\n",
    "df['MatchedBrand'] = [master_df.iloc[i]['BrandName'] for i in best_indices]\n",
    "df['MatchedSubBrand'] = [master_df.iloc[i]['SubBrandName'] for i in best_indices]\n",
    "df['MatchedMasterBrand'] = [master_df.iloc[i]['MasterBrandName'] for i in best_indices]\n",
    "\n",
    "# === Score Calculation ===\n",
    "\n",
    "# SBERT score\n",
    "df['SBERTScore'] = best_scores\n",
    "\n",
    "# Fuzzy match (ProductNameScore)\n",
    "def fuzzy_overlap(a, b):\n",
    "    return token_set_ratio(str(a), str(b)) / 100.0\n",
    "\n",
    "df['ProductNameScore'] = df.apply(\n",
    "    lambda row: fuzzy_overlap(row['SourceDescription'], row['BestMatchProduct']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Combine scores using alpha weight\n",
    "alpha = 0.8  # More weight to SBERT for precision\n",
    "df['SimilarityScore'] = df.apply(\n",
    "    lambda row: alpha * row['SBERTScore'] + (1 - alpha) * row['ProductNameScore'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Adjust similarity based on fuzzy score ===\n",
    "def adjust_similarity(row):\n",
    "    sim = row['SimilarityScore']\n",
    "    fuzzy = row['ProductNameScore']\n",
    "    \n",
    "    if fuzzy >= 0.90:\n",
    "        return min(sim + 0.50, 1.0)\n",
    "    elif fuzzy <= 0.30:\n",
    "        return max(sim - 0.03, 0.0)\n",
    "    else:\n",
    "        return sim\n",
    "\n",
    "df['AdjustedSimilarityScore'] = df.apply(adjust_similarity, axis=1)\n",
    "\n",
    "# === Confidence tagging based on adjusted score ===\n",
    "threshold = 0.65\n",
    "df['Confidence'] = df['AdjustedSimilarityScore'].apply(lambda x: 'High' if x >= threshold else 'Low')\n",
    "\n",
    "# === Save Output ===\n",
    "df.to_csv('output_combined_model18.csv', index=False)\n",
    "\n",
    "print(\"✅ Matching complete! Results saved to 'output_combined_model18.csv'\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b2d8cdec-9db2-43e9-b45c-21dd8093a629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\Temp\\ipykernel_3288\\3896107411.py:8: DtypeWarning: Columns (1,12,13,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('SampleFullData.csv')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdb7977d7aa5493aa89ef9d5fb4c9cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1130 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Matching: 100%|██████████| 68/68 [29:09<00:00, 25.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matching complete! Results saved to 'output_combined_model19.csv'\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz.fuzz import token_set_ratio\n",
    "\n",
    "# === Load CSV ===\n",
    "df = pd.read_csv('SampleFullData.csv')\n",
    "\n",
    "# === Drop rows with missing SourceDescription or ProductName ===\n",
    "df = df.dropna(subset=['SourceDescription', 'ProductName'])\n",
    "\n",
    "# === Fill NaNs in brand columns ===\n",
    "for col in ['SourceBrand', 'SourceSubBrand', 'SourceMasterBrand', 'BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# === Function to enrich with brand info ===\n",
    "def enrich_with_brand(desc, brand, subbrand, masterbrand):\n",
    "    parts = [brand, subbrand, masterbrand]\n",
    "    brand_str = ' '.join([p.strip() for p in parts if p.strip()])\n",
    "    return f\"{brand_str} {desc}\".strip()\n",
    "\n",
    "# === Combined Source Text ===\n",
    "df['CombinedSourceText'] = df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['SourceDescription'], row['SourceBrand'], row['SourceSubBrand'], row['SourceMasterBrand'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Prepare Master Data ===\n",
    "master_df = df[['ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates().fillna('')\n",
    "master_df['CombinedMasterText'] = master_df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['ProductName'], row['BrandName'], row['SubBrandName'], row['MasterBrandName'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Load SBERT Model ===\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# === Encode Master Texts ===\n",
    "master_texts = master_df['CombinedMasterText'].tolist()\n",
    "master_embeddings = model.encode(master_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# === Batched Matching ===\n",
    "batch_size = 5000\n",
    "source_texts = df['CombinedSourceText'].tolist()\n",
    "\n",
    "best_indices = []\n",
    "best_scores = []\n",
    "\n",
    "for i in tqdm(range(0, len(source_texts), batch_size), desc=\"Matching\"):\n",
    "    batch_texts = source_texts[i:i+batch_size]\n",
    "    batch_embeddings = model.encode(batch_texts, convert_to_tensor=True, show_progress_bar=False)\n",
    "    \n",
    "    scores = util.cos_sim(batch_embeddings, master_embeddings)  # [batch_size x M]\n",
    "    best_batch_scores, best_batch_indices = scores.max(dim=1)\n",
    "\n",
    "    best_scores.extend(best_batch_scores.cpu().numpy())\n",
    "    best_indices.extend(best_batch_indices.cpu().numpy())\n",
    "\n",
    "# === Add Matching Results ===\n",
    "df = df.iloc[:len(best_scores)].copy()  # ensure alignment\n",
    "\n",
    "df['BestMatchProduct'] = [master_df.iloc[i]['ProductName'] for i in best_indices]\n",
    "df['MatchedBrand'] = [master_df.iloc[i]['BrandName'] for i in best_indices]\n",
    "df['MatchedSubBrand'] = [master_df.iloc[i]['SubBrandName'] for i in best_indices]\n",
    "df['MatchedMasterBrand'] = [master_df.iloc[i]['MasterBrandName'] for i in best_indices]\n",
    "\n",
    "# === Score Calculation ===\n",
    "\n",
    "# SBERT score\n",
    "df['SBERTScore'] = best_scores\n",
    "\n",
    "# Fuzzy match (ProductNameScore)\n",
    "def fuzzy_overlap(a, b):\n",
    "    return token_set_ratio(str(a), str(b)) / 100.0\n",
    "\n",
    "df['ProductNameScore'] = df.apply(\n",
    "    lambda row: fuzzy_overlap(row['SourceDescription'], row['BestMatchProduct']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Combine scores using alpha weight\n",
    "alpha = 0.8  # More weight to SBERT for precision\n",
    "df['SimilarityScore'] = df.apply(\n",
    "    lambda row: alpha * row['SBERTScore'] + (1 - alpha) * row['ProductNameScore'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Adjust similarity based on fuzzy score ===\n",
    "def adjust_similarity(row):\n",
    "    sim = row['SimilarityScore']\n",
    "    fuzzy = row['ProductNameScore']\n",
    "    \n",
    "    if fuzzy >= 0.90:\n",
    "        return min(sim + 0.50, 1.0)\n",
    "    elif fuzzy <= 0.30:\n",
    "        return max(sim - 0.03, 0.0)\n",
    "    else:\n",
    "        return sim\n",
    "\n",
    "df['AdjustedSimilarityScore'] = df.apply(adjust_similarity, axis=1)\n",
    "\n",
    "# === Confidence tagging based on adjusted score ===\n",
    "threshold = 0.65\n",
    "df['Confidence'] = df['AdjustedSimilarityScore'].apply(lambda x: 'High' if x >= threshold else 'Low')\n",
    "\n",
    "# === Save Output ===\n",
    "df.to_csv('output_combined_model19.csv', index=False)\n",
    "\n",
    "print(\"✅ Matching complete! Results saved to 'output_combined_model19.csv'\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7b84bfcf-71e3-4e06-9ce5-790140a99f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c08adbddf444567a92992ecd6816f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# === Encode Master Texts ===\u001b[39;00m\n\u001b[0;32m     40\u001b[0m master_texts \u001b[38;5;241m=\u001b[39m master_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombinedMasterText\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m---> 41\u001b[0m master_embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(master_texts, convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# === Batched Matching ===\u001b[39;00m\n\u001b[0;32m     44\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1052\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m-> 1052\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1054\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1133\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[1;34m(self, input, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m             module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[0;32m   1128\u001b[0m         module_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1129\u001b[0m             key: value\n\u001b[0;32m   1130\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1131\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mforward_kwargs)\n\u001b[0;32m   1132\u001b[0m         }\n\u001b[1;32m-> 1133\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs)\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:437\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features, **kwargs)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[0;32m    431\u001b[0m trans_features \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    432\u001b[0m     key: value\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    435\u001b[0m }\n\u001b[1;32m--> 437\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    438\u001b[0m token_embeddings \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    439\u001b[0m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token_embeddings\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1028\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m-> 1028\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1029\u001b[0m     embedding_output,\n\u001b[0;32m   1030\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   1031\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1032\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1033\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1034\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1035\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1036\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1037\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1038\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1039\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m   1040\u001b[0m )\n\u001b[0;32m   1041\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1042\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:675\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    671\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m    673\u001b[0m layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    676\u001b[0m     hidden_states,\n\u001b[0;32m    677\u001b[0m     attention_mask,\n\u001b[0;32m    678\u001b[0m     layer_head_mask,\n\u001b[0;32m    679\u001b[0m     encoder_hidden_states,  \u001b[38;5;66;03m# as a positional argument for gradient checkpointing\u001b[39;00m\n\u001b[0;32m    680\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    681\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m    682\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    683\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    684\u001b[0m )\n\u001b[0;32m    686\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:584\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    574\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    575\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    582\u001b[0m     cache_position: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    583\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 584\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m    585\u001b[0m         hidden_states,\n\u001b[0;32m    586\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    587\u001b[0m         head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m    588\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    589\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    590\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    591\u001b[0m     )\n\u001b[0;32m    592\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    593\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:524\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_attention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.55.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    503\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    504\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    512\u001b[0m     cache_position: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    513\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    514\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    515\u001b[0m         hidden_states,\n\u001b[0;32m    516\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    522\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[0;32m    523\u001b[0m     )\n\u001b[1;32m--> 524\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    525\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n\u001b[0;32m    526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:461\u001b[0m, in \u001b[0;36mBertSelfOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m--> 461\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m    462\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    463\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz.fuzz import token_set_ratio\n",
    "\n",
    "# === Load CSV ===\n",
    "df = pd.read_csv('HackWeekProductsData.csv')\n",
    "\n",
    "# === Drop rows with missing SourceDescription or ProductName ===\n",
    "df = df.dropna(subset=['SourceDescription', 'ProductName'])\n",
    "\n",
    "# === Fill NaNs in brand columns ===\n",
    "for col in ['SourceBrand', 'SourceSubBrand', 'SourceMasterBrand', 'BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# === Function to enrich with brand info ===\n",
    "def enrich_with_brand(desc, brand, subbrand, masterbrand):\n",
    "    parts = [brand, subbrand, masterbrand]\n",
    "    brand_str = ' '.join([p.strip() for p in parts if p.strip()])\n",
    "    return f\"{brand_str} {desc}\".strip()\n",
    "\n",
    "# === Combined Source Text ===\n",
    "df['CombinedSourceText'] = df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['SourceDescription'], row['SourceBrand'], row['SourceSubBrand'], row['SourceMasterBrand'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Prepare Master Data ===\n",
    "master_df = df[['ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates().fillna('')\n",
    "master_df['CombinedMasterText'] = master_df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['ProductName'], row['BrandName'], row['SubBrandName'], row['MasterBrandName'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Load SBERT Model ===\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# === Encode Master Texts ===\n",
    "master_texts = master_df['CombinedMasterText'].tolist()\n",
    "master_embeddings = model.encode(master_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# === Batched Matching ===\n",
    "batch_size = 5000\n",
    "source_texts = df['CombinedSourceText'].tolist()\n",
    "\n",
    "best_indices = []\n",
    "best_scores = []\n",
    "\n",
    "for i in tqdm(range(0, len(source_texts), batch_size), desc=\"Matching\"):\n",
    "    batch_texts = source_texts[i:i+batch_size]\n",
    "    batch_embeddings = model.encode(batch_texts, convert_to_tensor=True, show_progress_bar=False)\n",
    "    \n",
    "    scores = util.cos_sim(batch_embeddings, master_embeddings)  # [batch_size x M]\n",
    "    best_batch_scores, best_batch_indices = scores.max(dim=1)\n",
    "\n",
    "    best_scores.extend(best_batch_scores.cpu().numpy())\n",
    "    best_indices.extend(best_batch_indices.cpu().numpy())\n",
    "\n",
    "# === Add Matching Results ===\n",
    "df = df.iloc[:len(best_scores)].copy()  # ensure alignment\n",
    "\n",
    "df['BestMatchProduct'] = [master_df.iloc[i]['ProductName'] for i in best_indices]\n",
    "df['MatchedBrand'] = [master_df.iloc[i]['BrandName'] for i in best_indices]\n",
    "df['MatchedSubBrand'] = [master_df.iloc[i]['SubBrandName'] for i in best_indices]\n",
    "df['MatchedMasterBrand'] = [master_df.iloc[i]['MasterBrandName'] for i in best_indices]\n",
    "\n",
    "# === Score Calculation ===\n",
    "\n",
    "# SBERT score\n",
    "df['SBERTScore'] = best_scores\n",
    "\n",
    "# Fuzzy match (ProductNameScore)\n",
    "def fuzzy_overlap(a, b):\n",
    "    return token_set_ratio(str(a), str(b)) / 100.0\n",
    "\n",
    "df['ProductNameScore'] = df.apply(\n",
    "    lambda row: fuzzy_overlap(row['SourceDescription'], row['BestMatchProduct']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Combine scores using alpha weight\n",
    "alpha = 0.8  # More weight to SBERT for precision\n",
    "df['SimilarityScore'] = df.apply(\n",
    "    lambda row: alpha * row['SBERTScore'] + (1 - alpha) * row['ProductNameScore'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Adjust similarity based on fuzzy score ===\n",
    "def adjust_similarity(row):\n",
    "    sim = row['SimilarityScore']\n",
    "    fuzzy = row['ProductNameScore']\n",
    "    \n",
    "    if fuzzy >= 0.90:\n",
    "        return min(sim + 0.50, 1.0)\n",
    "    elif fuzzy <= 0.30:\n",
    "        return max(sim - 0.03, 0.0)\n",
    "    else:\n",
    "        return sim\n",
    "\n",
    "df['AdjustedSimilarityScore'] = df.apply(adjust_similarity, axis=1)\n",
    "\n",
    "# === Confidence tagging based on adjusted score ===\n",
    "threshold = 0.65\n",
    "df['Confidence'] = df['AdjustedSimilarityScore'].apply(lambda x: 'High' if x >= threshold else 'Low')\n",
    "\n",
    "# === Save Output ===\n",
    "df.to_csv('output_combined_model20.csv', index=False)\n",
    "\n",
    "print(\"✅ Matching complete! Results saved to 'output_combined_model20.csv'\")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cdbf2f66-95b2-4f6e-81f1-70391a4da06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f29851313aa415ea8d67443dba523bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Matching: 100%|██████████| 151/151 [1:06:27<00:00, 26.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matching complete! Results saved to 'output_combined_model20.csv'\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz.fuzz import token_set_ratio\n",
    "\n",
    "# === Load CSV ===\n",
    "df = pd.read_csv('FullData.csv')\n",
    "\n",
    "# === Drop rows with missing SourceDescription or ProductName ===\n",
    "df = df.dropna(subset=['SourceDescription', 'ProductName'])\n",
    "\n",
    "# === Fill NaNs in brand columns ===\n",
    "for col in ['SourceBrand', 'SourceSubBrand', 'SourceMasterBrand', 'BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# === Function to enrich with brand info ===\n",
    "def enrich_with_brand(desc, brand, subbrand, masterbrand):\n",
    "    parts = [brand, subbrand, masterbrand]\n",
    "    brand_str = ' '.join([p.strip() for p in parts if p.strip()])\n",
    "    return f\"{brand_str} {desc}\".strip()\n",
    "\n",
    "# === Combined Source Text ===\n",
    "df['CombinedSourceText'] = df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['SourceDescription'], row['SourceBrand'], row['SourceSubBrand'], row['SourceMasterBrand'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Prepare Master Data ===\n",
    "master_df = df[['ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates().fillna('')\n",
    "master_df['CombinedMasterText'] = master_df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['ProductName'], row['BrandName'], row['SubBrandName'], row['MasterBrandName'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Load SBERT Model ===\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# === Encode Master Texts ===\n",
    "master_texts = master_df['CombinedMasterText'].tolist()\n",
    "master_embeddings = model.encode(master_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# === Batched Matching ===\n",
    "batch_size = 5000\n",
    "source_texts = df['CombinedSourceText'].tolist()\n",
    "\n",
    "best_indices = []\n",
    "best_scores = []\n",
    "\n",
    "for i in tqdm(range(0, len(source_texts), batch_size), desc=\"Matching\"):\n",
    "    batch_texts = source_texts[i:i+batch_size]\n",
    "    batch_embeddings = model.encode(batch_texts, convert_to_tensor=True, show_progress_bar=False)\n",
    "    \n",
    "    scores = util.cos_sim(batch_embeddings, master_embeddings)  # [batch_size x M]\n",
    "    best_batch_scores, best_batch_indices = scores.max(dim=1)\n",
    "\n",
    "    best_scores.extend(best_batch_scores.cpu().numpy())\n",
    "    best_indices.extend(best_batch_indices.cpu().numpy())\n",
    "\n",
    "# === Add Matching Results ===\n",
    "df = df.iloc[:len(best_scores)].copy()  # ensure alignment\n",
    "\n",
    "df['BestMatchProduct'] = [master_df.iloc[i]['ProductName'] for i in best_indices]\n",
    "df['MatchedBrand'] = [master_df.iloc[i]['BrandName'] for i in best_indices]\n",
    "df['MatchedSubBrand'] = [master_df.iloc[i]['SubBrandName'] for i in best_indices]\n",
    "df['MatchedMasterBrand'] = [master_df.iloc[i]['MasterBrandName'] for i in best_indices]\n",
    "\n",
    "# === Score Calculation ===\n",
    "\n",
    "# SBERT score\n",
    "df['SBERTScore'] = best_scores\n",
    "\n",
    "# Fuzzy match (ProductNameScore)\n",
    "def fuzzy_overlap(a, b):\n",
    "    return token_set_ratio(str(a), str(b)) / 100.0\n",
    "\n",
    "df['ProductNameScore'] = df.apply(\n",
    "    lambda row: fuzzy_overlap(row['SourceDescription'], row['BestMatchProduct']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Combine scores using alpha weight\n",
    "alpha = 0.8  # More weight to SBERT for precision\n",
    "df['SimilarityScore'] = df.apply(\n",
    "    lambda row: alpha * row['SBERTScore'] + (1 - alpha) * row['ProductNameScore'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Adjust similarity based on fuzzy score ===\n",
    "def adjust_similarity(row):\n",
    "    sim = row['SimilarityScore']\n",
    "    fuzzy = row['ProductNameScore']\n",
    "    \n",
    "    if fuzzy >= 0.90:\n",
    "        return min(sim + 0.50, 1.0)\n",
    "    elif fuzzy <= 0.30:\n",
    "        return max(sim - 0.03, 0.0)\n",
    "    else:\n",
    "        return sim\n",
    "\n",
    "df['AdjustedSimilarityScore'] = df.apply(adjust_similarity, axis=1)\n",
    "\n",
    "# === Confidence tagging based on adjusted score ===\n",
    "threshold = 0.65\n",
    "df['Confidence'] = df['AdjustedSimilarityScore'].apply(lambda x: 'High' if x >= threshold else 'Low')\n",
    "\n",
    "# === Save Output ===\n",
    "df.to_csv('output_combined_model20.csv', index=False)\n",
    "\n",
    "print(\"✅ Matching complete! Results saved to 'output_combined_model20.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a7ac7115-0516-46c5-a599-bad9056c8cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eeb53a725c74b64bc61122cc3ed1284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Matching: 100%|██████████| 145/145 [1:03:24<00:00, 26.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matching complete! Filtered results saved to 'output_combined_model21.csv'\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz.fuzz import token_set_ratio\n",
    "\n",
    "# === Load CSV ===\n",
    "df = pd.read_csv('FullDataTest.csv')\n",
    "\n",
    "# === Drop rows with missing SourceDescription or ProductName ===\n",
    "df = df.dropna(subset=['SourceDescription', 'ProductName'])\n",
    "\n",
    "# === Fill NaNs in brand columns ===\n",
    "for col in ['SourceBrand', 'SourceSubBrand', 'SourceMasterBrand', 'BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# === Function to enrich with brand info ===\n",
    "def enrich_with_brand(desc, brand, subbrand, masterbrand):\n",
    "    parts = [brand, subbrand, masterbrand]\n",
    "    brand_str = ' '.join([p.strip() for p in parts if p.strip()])\n",
    "    return f\"{brand_str} {desc}\".strip()\n",
    "\n",
    "# === Combined Source Text ===\n",
    "df['CombinedSourceText'] = df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['SourceDescription'], row['SourceBrand'], row['SourceSubBrand'], row['SourceMasterBrand'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Prepare Master Data ===\n",
    "master_df = df[['ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates().fillna('')\n",
    "master_df['CombinedMasterText'] = master_df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['ProductName'], row['BrandName'], row['SubBrandName'], row['MasterBrandName'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Load SBERT Model ===\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# === Encode Master Texts ===\n",
    "master_texts = master_df['CombinedMasterText'].tolist()\n",
    "master_embeddings = model.encode(master_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# === Batched Matching ===\n",
    "batch_size = 5000\n",
    "source_texts = df['CombinedSourceText'].tolist()\n",
    "\n",
    "best_indices = []\n",
    "best_scores = []\n",
    "\n",
    "for i in tqdm(range(0, len(source_texts), batch_size), desc=\"Matching\"):\n",
    "    batch_texts = source_texts[i:i+batch_size]\n",
    "    batch_embeddings = model.encode(batch_texts, convert_to_tensor=True, show_progress_bar=False)\n",
    "    \n",
    "    scores = util.cos_sim(batch_embeddings, master_embeddings)  # [batch_size x M]\n",
    "    best_batch_scores, best_batch_indices = scores.max(dim=1)\n",
    "\n",
    "    best_scores.extend(best_batch_scores.cpu().numpy())\n",
    "    best_indices.extend(best_batch_indices.cpu().numpy())\n",
    "\n",
    "# === Add Matching Results ===\n",
    "df = df.iloc[:len(best_scores)].copy()  # ensure alignment\n",
    "\n",
    "df['BestMatchProduct'] = [master_df.iloc[i]['ProductName'] for i in best_indices]\n",
    "df['MatchedBrand'] = [master_df.iloc[i]['BrandName'] for i in best_indices]\n",
    "df['MatchedSubBrand'] = [master_df.iloc[i]['SubBrandName'] for i in best_indices]\n",
    "df['MatchedMasterBrand'] = [master_df.iloc[i]['MasterBrandName'] for i in best_indices]\n",
    "\n",
    "# === Score Calculation ===\n",
    "\n",
    "# SBERT score\n",
    "df['SBERTScore'] = best_scores\n",
    "\n",
    "# Fuzzy match (ProductNameScore)\n",
    "def fuzzy_overlap(a, b):\n",
    "    return token_set_ratio(str(a), str(b)) / 100.0\n",
    "\n",
    "df['ProductNameScore'] = df.apply(\n",
    "    lambda row: fuzzy_overlap(row['SourceDescription'], row['BestMatchProduct']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Combine scores using alpha weight\n",
    "alpha = 0.8  # More weight to SBERT for precision\n",
    "df['SimilarityScore'] = df.apply(\n",
    "    lambda row: alpha * row['SBERTScore'] + (1 - alpha) * row['ProductNameScore'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Adjust similarity based on fuzzy score ===\n",
    "def adjust_similarity(row):\n",
    "    sim = row['SimilarityScore']\n",
    "    fuzzy = row['ProductNameScore']\n",
    "    \n",
    "    if fuzzy >= 0.90:\n",
    "        return min(sim + 0.50, 1.0)\n",
    "    elif fuzzy <= 0.30:\n",
    "        return max(sim - 0.03, 0.0)\n",
    "    else:\n",
    "        return sim\n",
    "\n",
    "df['AdjustedSimilarityScore'] = df.apply(adjust_similarity, axis=1)\n",
    "\n",
    "# === Confidence tagging based on adjusted score ===\n",
    "threshold = 0.65\n",
    "df['Confidence'] = df['AdjustedSimilarityScore'].apply(lambda x: 'High' if x >= threshold else 'Low')\n",
    "\n",
    "# === Save Output ===\n",
    "# Select only the desired output columns\n",
    "output_df = df[['ServiceAndProductMappingId', 'ProductMasterId', 'ProductName', 'BestMatchProduct', 'AdjustedSimilarityScore', 'Confidence']]\n",
    "\n",
    "# Save to CSV\n",
    "output_df.to_csv('output_combined_model21.csv', index=False)\n",
    "\n",
    "print(\"✅ Matching complete! Filtered results saved to 'output_combined_model21.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "52575b61-dfd2-4228-a48a-a291b5ab3c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8088060bdbd4146b104c2ff2d53eebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sarthak.Prakash\\AppData\\Local\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "Matching: 100%|██████████| 145/145 [1:02:34<00:00, 25.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matching complete! Filtered results saved to 'FinalOutput.csv'\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from rapidfuzz.fuzz import token_set_ratio\n",
    "\n",
    "# === Load CSV ===\n",
    "df = pd.read_csv('FinalData.csv')\n",
    "\n",
    "# === Drop rows with missing SourceDescription or ProductName ===\n",
    "df = df.dropna(subset=['SourceDescription', 'ProductName'])\n",
    "\n",
    "# === Fill NaNs in brand columns ===\n",
    "for col in ['SourceBrand', 'SourceSubBrand', 'SourceMasterBrand', 'BrandName', 'SubBrandName', 'MasterBrandName']:\n",
    "    df[col] = df[col].fillna('')\n",
    "\n",
    "# === Function to enrich with brand info ===\n",
    "def enrich_with_brand(desc, brand, subbrand, masterbrand):\n",
    "    parts = [brand, subbrand, masterbrand]\n",
    "    brand_str = ' '.join([p.strip() for p in parts if p.strip()])\n",
    "    return f\"{brand_str} {desc}\".strip()\n",
    "\n",
    "# === Combined Source Text ===\n",
    "df['CombinedSourceText'] = df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['SourceDescription'], row['SourceBrand'], row['SourceSubBrand'], row['SourceMasterBrand'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Prepare Master Data ===\n",
    "master_df = df[['ProductName', 'BrandName', 'SubBrandName', 'MasterBrandName']].drop_duplicates().fillna('')\n",
    "master_df['CombinedMasterText'] = master_df.apply(\n",
    "    lambda row: f\"Description: {enrich_with_brand(row['ProductName'], row['BrandName'], row['SubBrandName'], row['MasterBrandName'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Load SBERT Model ===\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# === Encode Master Texts ===\n",
    "master_texts = master_df['CombinedMasterText'].tolist()\n",
    "master_embeddings = model.encode(master_texts, convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# === Batched Matching ===\n",
    "batch_size = 5000\n",
    "source_texts = df['CombinedSourceText'].tolist()\n",
    "\n",
    "best_indices = []\n",
    "best_scores = []\n",
    "\n",
    "for i in tqdm(range(0, len(source_texts), batch_size), desc=\"Matching\"):\n",
    "    batch_texts = source_texts[i:i+batch_size]\n",
    "    batch_embeddings = model.encode(batch_texts, convert_to_tensor=True, show_progress_bar=False)\n",
    "    \n",
    "    scores = util.cos_sim(batch_embeddings, master_embeddings)  # [batch_size x M]\n",
    "    best_batch_scores, best_batch_indices = scores.max(dim=1)\n",
    "\n",
    "    best_scores.extend(best_batch_scores.cpu().numpy())\n",
    "    best_indices.extend(best_batch_indices.cpu().numpy())\n",
    "\n",
    "# === Add Matching Results ===\n",
    "df = df.iloc[:len(best_scores)].copy()  # ensure alignment\n",
    "\n",
    "df['BestMatchProduct'] = [master_df.iloc[i]['ProductName'] for i in best_indices]\n",
    "df['MatchedBrand'] = [master_df.iloc[i]['BrandName'] for i in best_indices]\n",
    "df['MatchedSubBrand'] = [master_df.iloc[i]['SubBrandName'] for i in best_indices]\n",
    "df['MatchedMasterBrand'] = [master_df.iloc[i]['MasterBrandName'] for i in best_indices]\n",
    "\n",
    "# === Score Calculation ===\n",
    "\n",
    "# SBERT score\n",
    "df['SBERTScore'] = best_scores\n",
    "\n",
    "# Fuzzy match (ProductNameScore)\n",
    "def fuzzy_overlap(a, b):\n",
    "    return token_set_ratio(str(a), str(b)) / 100.0\n",
    "\n",
    "df['ProductNameScore'] = df.apply(\n",
    "    lambda row: fuzzy_overlap(row['SourceDescription'], row['BestMatchProduct']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Combine scores using alpha weight\n",
    "alpha = 0.8  # More weight to SBERT for precision\n",
    "df['SimilarityScore'] = df.apply(\n",
    "    lambda row: alpha * row['SBERTScore'] + (1 - alpha) * row['ProductNameScore'],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# === Adjust similarity based on fuzzy score ===\n",
    "def adjust_similarity(row):\n",
    "    sim = row['SimilarityScore']\n",
    "    fuzzy = row['ProductNameScore']\n",
    "    \n",
    "    if fuzzy >= 0.90:\n",
    "        return min(sim + 0.50, 1.0)\n",
    "    elif fuzzy <= 0.30:\n",
    "        return max(sim - 0.03, 0.0)\n",
    "    else:\n",
    "        return sim\n",
    "\n",
    "df['AdjustedSimilarityScore'] = df.apply(adjust_similarity, axis=1)\n",
    "\n",
    "# === Confidence tagging based on adjusted score ===\n",
    "threshold = 0.65\n",
    "df['Confidence'] = df['AdjustedSimilarityScore'].apply(lambda x: 'High' if x >= threshold else 'Low')\n",
    "\n",
    "# === Save Output ===\n",
    "# Select only the desired output columns\n",
    "output_df = df[['ServiceAndProductMappingId', 'ProductMasterId', 'ProductName', 'BestMatchProduct', 'AdjustedSimilarityScore', 'Confidence']]\n",
    "\n",
    "# Save to CSV\n",
    "output_df.to_csv('FinalOutput.csv', index=False)\n",
    "\n",
    "print(\"✅ Matching complete! Filtered results saved to 'FinalOutput.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eaaee4-62ac-4019-af48-0fb63830e6e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
